"","title","author","subject","abstract","meta"
"1","Towards Understanding of Medical Randomized Controlled Trials by Conclusion Generation","Alexander Te-Wei Shieh, Yung-Sung Chuang, Shang-Yu Su, Yun-Nung Chen","Computation and Language (cs.CL)","Randomized controlled trials (RCTs) represent the paramount evidence of clinical medicine. Using machines to interpret the massive amount of RCTs has the potential of aiding clinical decision-making. We propose a RCT conclusion generation task from the PubMed 200k RCT sentence classification dataset to examine the effectiveness of sequence-to-sequence models on understanding RCTs. We first build a pointer-generator baseline model for conclusion generation. Then we fine-tune the state-of-the-art GPT-2 language model, which is pre-trained with general domain data, for this new medical domain task. Both automatic and human evaluation show that our GPT-2 fine-tuned models achieve improved quality and correctness in the generated conclusions compared to the baseline pointer-generator model. Further inspection points out the limitations of this current approach and future directions to explore.","Thu, 3 Oct 2019 13:35:00 UTC (711 KB)"
"2","W-RNN: News text classification based on a Weighted RNN","Dan Wang, Jibing Gong, Yaxi Song","Information Retrieval (cs.IR)","Most of the information is stored as text, so text mining is regarded as having high commercial potential. Aiming at the semantic constraint problem of classification methods based on sparse representation, we propose a weighted recurrent neural network (W-RNN), which can fully extract text serialization semantic information. For the problem that the feature high dimensionality and unclear semantic relationship in text data representation, we first utilize the word vector to represent the vocabulary in the text and use Recurrent Neural Network (RNN) to extract features of the serialized text data. The word vector is then automatically weighted and summed using the intermediate output of the word vector to form the text representation vector. Finally, the neural network is used for classification. W-RNN is verified on the news dataset and proves that W-RNN is superior to other four baseline methods in Precision, Recall, F1 and loss values, which is suitable for text classification.","Sat, 28 Sep 2019 11:54:15 UTC (856 KB)"
"3","Stock Market Forecasting Based on Text Mining Technology: A Support Vector Machine Method","Yancong Xie, Hongxun Jiang","Machine Learning (cs.LG)","News items have a significant impact on stock markets but the ways are obscure. Many previous works have aimed at finding accurate stock market forecasting models. In this paper, we use text mining and sentiment analysis on Chinese online financial news, to predict Chinese stock tendency and stock prices based on support vector machine (SVM). Firstly, we collect 2,302,692 news items, which date from 1/1/2008 to 1/1/2015. Secondly, based on this dataset, a specific domain stop-word dictionary and a precise sentiment dictionary are formed. Thirdly, we propose a forecasting model using SVM. On the algorithm of SVM implementation, we also propose two-parameter optimization algorithms to search for the best initial parameter setting. The result shows that parameter G has the main effect, while parameter C's effect is not obvious. Furthermore, support vector regression (SVR) models for different Chinese stocks are similar whereas in support vector classification (SVC) models best parameters are quite differential. Series of contrast experiments show that: a) News has significant influence on stock market; b) Expansion input vector for additional situations when that day has no news data is better than normal input in SVR, yet is worse in SVC; c) SVR shows a fantastic degree of fitting in predicting stock fluctuation while such result has some time lag; d) News effect time lag for stock market is less than two days; e) In SVC, historic stock data has a most efficient time lag which is about 10 days, whereas in SVR this effect is not obvious. Besides, based on the special structure of the input vector, we also design a method to calculate the financial source impact factor. Result suggests that the news quality and audience number both have a significant effect on the source impact factor. Besides, for Chinese investors, traditional media has more influence than digital media.","Fri, 27 Sep 2019 16:52:27 UTC (1,869 KB)"
"4","Deep Learning and Random Forest-Based Augmentation of sRNA Expression Profiles","Jelena Fiosina, Maksims Fiosins, Stefan Bonn","Genomics (q-bio.GN)","The lack of well-structured annotations in a growing amount of RNA expression data complicates data interoperability and reusability. Commonly - used text mining methods extract annotations from existing unstructured data descriptions and often provide inaccurate output that requires manual curation. Automatic data-based augmentation (generation of annotations on the base of expression data) can considerably improve the annotation quality and has not been well-studied. We formulate an automatic augmentation of small RNA-seq expression data as a classification problem and investigate deep learning (DL) and random forest (RF) approaches to solve it. We generate tissue and sex annotations from small RNA-seq expression data for tissues and cell lines of homo sapiens. We validate our approach on 4243 annotated small RNA-seq samples from the Small RNA Expression Atlas (SEA) database. The average prediction accuracy for tissue groups is 98% (DL), for tissues - 96.5% (DL), and for sex - 77% (DL). The ""one dataset out"" average accuracy for tissue group prediction is 83% (DL) and 59% (RF). On average, DL provides better results as compared to RF, and considerably improves classification performance for 'unseen' datasets.","Thu, 26 Sep 2019 07:10:03 UTC (1,178 KB)"
"5","Deep Text Mining of Instagram Data Without Strong Supervision","Kim Hammar, Shatha Jaradat, Nima Dokoohaki, Mihhail Matskin","Computation and Language (cs.CL)","With the advent of social media, our online feeds increasingly consist of short, informal, and unstructured text. This textual data can be analyzed for the purpose of improving user recommendations and detecting trends. Instagram is one of the largest social media platforms, containing both text and images. However, most of the prior research on text processing in social media is focused on analyzing Twitter data, and little attention has been paid to text mining of Instagram data. Moreover, many text mining methods rely on annotated training data, which in practice is both difficult and expensive to obtain. In this paper, we present methods for unsupervised mining of fashion attributes from Instagram text, which can enable a new kind of user recommendation in the fashion domain. In this context, we analyze a corpora of Instagram posts from the fashion domain, introduce a system for extracting fashion attributes from Instagram, and train a deep clothing classifier with weak supervision to classify Instagram posts based on the associated text. With our experiments, we confirm that word embeddings are a useful asset for information extraction. Experimental results show that information extraction using word embeddings outperforms a baseline that uses Levenshtein distance. The results also show the benefit of combining weak supervision signals using generative models instead of majority voting. Using weak supervision and generative modeling, an F1 score of 0.61 is achieved on the task of classifying the image contents of Instagram posts based solely on the associated text, which is on level with human performance. Finally, our empirical study provides one of the few available studies on Instagram text and shows that the text is noisy, that the text distribution exhibits the long-tail phenomenon, and that comment sections on Instagram are multi-lingual.","Tue, 24 Sep 2019 11:04:02 UTC (1,978 KB)"
"6","Biomedical Mention Disambiguation using a Deep Learning Approach","Chih-Hsuan Wei, Kyubum Lee, Robert Leaman, Zhiyong Lu","Computation and Language (cs.CL)","Automatically locating named entities in natural language text - named entity recognition - is an important task in the biomedical domain. Many named entity mentions are ambiguous between several bioconcept types, however, causing text spans to be annotated as more than one type when simultaneously recognizing multiple entity types. The straightforward solution is a rule-based approach applying a priority order based on the precision of each entity tagger (from highest to lowest). While this method is straightforward and useful, imprecise disambiguation remains a significant source of error. We address this issue by generating a partially labeled corpus of ambiguous concept mentions. We first collect named entity mentions from multiple human-curated databases (e.g. CTDbase, gene2pubmed), then correlate them with the text mined span from PubTator to provide the context where the mention appears. Our corpus contains more than 3 million concept mentions that ambiguous between one or more concept types in PubTator (about 3% of all mentions). We approached this task as a classification problem and developed a deep learning-based method which uses the semantics of the span being classified and the surrounding words to identify the most likely bioconcept type. More specifically, we develop a convolutional neural network (CNN) and along short-term memory (LSTM) network to respectively handle the semantic syntax features, then concatenate these within a fully connected layer for final classification. The priority ordering rule-based approach demonstrated F1-scores of 71.29% (micro-averaged) and 41.19% (macro-averaged), while the new disambiguation method demonstrated F1-scores of 91.94% (micro-averaged) and 85.42% (macro-averaged), a very substantial increase.","Mon, 23 Sep 2019 15:14:56 UTC (413 KB)"
"7","Learning Dynamic Author Representations with Temporal Language Models","Edouard Delasalles, Sylvain Lamprier, Ludovic Denoyer","Computation and Language (cs.CL)","Language models are at the heart of numerous works, notably in the text mining and information retrieval communities. These statistical models aim at extracting word distributions, from simple unigram models to recurrent approaches with latent variables that capture subtle dependencies in texts. However, those models are learned from word sequences only, and authors' identities, as well as publication dates, are seldom considered. We propose a neural model, based on recurrent language modeling, which aims at capturing language diffusion tendencies in author communities through time. By conditioning language models with author and temporal vector states, we are able to leverage the latent dependencies between the text contexts. This allows us to beat several temporal and non-temporal language baselines on two real-world corpora, and to learn meaningful author representations that vary through time.","Wed, 11 Sep 2019 11:51:43 UTC (1,680 KB)"
"8","Global Locality in Event Extraction","Elaheh ShafieiBavani, Antonio Jimeno Yepes, Xu Zhong","Computation and Language (cs.CL)","Due to the exponential growth of biomedical literature, event and relation extraction are important tasks in biomedical text mining. Most work in relation extraction detect a single entity pair mention on a short span of text, which is not ideal due to long sentences that appear in biomedical contexts. We propose an approach to both event and relation extraction, for simultaneously predicting relationships between all mention pairs in a text. Our model includes a set of multi-head attentions and convolutions, an adaptation of the transformer architecture, which offers self-attention the ability to strengthen dependencies among related elements, and models the interaction between features extracted by multiple attention heads. Experiment results demonstrate that our approach outperforms the state-of-the-art on a set of benchmark biomedical corpora including BioNLP 2009, 2011, 2013 and BioCreative 2017 shared tasks.","Wed, 11 Sep 2019 02:20:57 UTC (441 KB)"
"9","Finding Personal Difference of Interpretation about Future in Assessment of Future Economic Conditions: Application of Weakly Supervised Learning and Text Mining","Masahiro Kato","Econometrics (econ.EM)","We reveal the different interpretations of the future in their judgments of future economic conditions by applying weakly supervised learning and text mining. In the Economy Watcher Survey, which is a market survey published by the Japanese government, there are \emph{assessments of current and future economic conditions} by people from various fields. Although this survey provides insights regarding an economic policy for policymakers in Japan, there is no clear definition of the future, in future economic conditions. Hence, in the survey, respondents make their assessments based on their interpretations of the future. In our research, we separate the assessments of future economic conditions into near and distant future economic conditions using learning from positive and unlabeled data (PU learning), which is weakly supervised learning. The dataset is composed of several periods, and we develop a PU learning algorithm for efficient training, using the dataset with the time series. Through empirical analysis, we interpret the classification results from the viewpoint of economics.","Sat, 7 Sep 2019 23:13:46 UTC (676 KB)[v2] Tue, 1 Oct 2019 15:43:16 UTC (1,544 KB)"
"10","Deep learning with sentence embeddings pre-trained on biomedical corpora improves the performance of finding similar sentences in electronic medical records","Qingyu Chen, Jingcheng Du, Sun Kim, W. John Wilbur, Zhiyong Lu","Computation and Language (cs.CL)","Capturing sentence semantics plays a vital role in a range of text mining applications. Despite continuous efforts on the development of related datasets and models in the general domain, both datasets and models are limited in biomedical and clinical domains. The BioCreative/OHNLP organizers have made the first attempt to annotate 1,068 sentence pairs from clinical notes and have called for a community effort to tackle the Semantic Textual Similarity (BioCreative/OHNLP STS) challenge. We developed models using traditional machine learning and deep learning approaches. For the post challenge, we focus on two models: the Random Forest and the Encoder Network. We applied sentence embeddings pre-trained on PubMed abstracts and MIMIC-III clinical notes and updated the Random Forest and the Encoder Network accordingly. The official results demonstrated our best submission was the ensemble of eight models. It achieved a Person correlation coefficient of 0.8328, the highest performance among 13 submissions from 4 teams. For the post challenge, the performance of both Random Forest and the Encoder Network was improved; in particular, the correlation of the Encoder Network was improved by ~13%. During the challenge task, no end-to-end deep learning models had better performance than machine learning models that take manually-crafted features. In contrast, with the sentence embeddings pre-trained on biomedical corpora, the Encoder Network now achieves a correlation of ~0.84, which is higher than the original best model. The ensembled model taking the improved versions of the Random Forest and Encoder Network as inputs further increased performance to 0.8528. Deep learning models with sentence embeddings pre-trained on biomedical corpora achieve the highest performance on the test set.","Fri, 6 Sep 2019 17:56:01 UTC (562 KB)"
"11","CT Data Curation for Liver Patients: Phase Recognition in Dynamic Contrast-Enhanced CT","Bo Zhou, Adam P. Harrison, Jiawen Yao, Chi-Tung Cheng, Jing Xiao, Chien-Hung Liao, Le Lu","Image and Video Processing (eess.IV)","As the demand for more descriptive machine learning models grows within medical imaging, bottlenecks due to data paucity will exacerbate. Thus, collecting enough large-scale data will require automated tools to harvest data/label pairs from messy and real-world datasets, such as hospital PACS. This is the focus of our work, where we present a principled data curation tool to extract multi-phase CT liver studies and identify each scan's phase from a real-world and heterogenous hospital PACS dataset. Emulating a typical deployment scenario, we first obtain a set of noisy labels from our institutional partners that are text mined using simple rules from DICOM tags. We train a deep learning system, using a customized and streamlined 3D SE architecture, to identify non-contrast, arterial, venous, and delay phase dynamic CT liver scans, filtering out anything else, including other types of liver contrast studies. To exploit as much training data as possible, we also introduce an aggregated cross entropy loss that can learn from scans only identified as ""contrast"". Extensive experiments on a dataset of 43K scans of 7680 patient imaging studies demonstrate that our 3DSE architecture, armed with our aggregated loss, can achieve a mean F1 of 0.977 and can correctly harvest up to 92.7% of studies, which significantly outperforms the text-mined and standard-loss approach, and also outperforms other, and more complex, model architectures.","Thu, 5 Sep 2019 16:31:40 UTC (1,355 KB)[v2] Fri, 27 Sep 2019 21:48:31 UTC (1,354 KB)"
"12","Ordered Sets for Data Analysis","Sergei O. Kuznetsov","Logic in Computer Science (cs.LO)","This book dwells on mathematical and algorithmic issues of data analysis based on generality order of descriptions and respective precision. To speak of these topics correctly, we have to go some way getting acquainted with the important notions of relation and order theory. On the one hand, data often have a complex structure with natural order on it. On the other hand, many symbolic methods of data analysis and machine learning allow to compare the obtained classifiers w.r.t. their generality, which is also an order relation. Efficient algorithms are very important in data analysis, especially when one deals with big data, so scalability is a real issue. That is why we analyze the computational complexity of algorithms and problems of data analysis. We start from the basic definitions and facts of algorithmic complexity theory and analyze the complexity of various tools of data analysis we consider. The tools and methods of data analysis, like computing taxonomies, groups of similar objects (concepts and n-clusters), dependencies in data, classification, etc., are illustrated with applications in particular subject domains, from chemoinformatics to text mining and natural language processing.","Tue, 27 Aug 2019 18:01:13 UTC (689 KB)"
"13","Training Optimus Prime, M.D.: Generating Medical Certification Items by Fine-Tuning OpenAI's gpt2 Transformer Model","Matthias von Davier","Computation and Language (cs.CL)","This article describes new results of an application using transformer-based language models to automated item generation (AIG), an area of ongoing interest in the domain of certification testing as well as in educational measurement and psychological testing. OpenAI's gpt2 pre-trained 345M parameter language model was retrained using the public domain text mining set of PubMed articles and subsequently used to generate item stems (case vignettes) as well as distractor proposals for multiple-choice items. This case study shows promise and produces draft text that can be used by human item writers as input for authoring. Future experiments with more recent transformer models (such as Grover, TransformerXL) using existing item pools are expected to improve results further and to facilitate the development of assessment materials.","Fri, 23 Aug 2019 00:58:21 UTC (3,167 KB)[v2] Mon, 26 Aug 2019 11:07:04 UTC (3,167 KB)[v3] Thu, 29 Aug 2019 23:11:08 UTC (3,169 KB)"
"14","Parsimonious Morpheme Segmentation with an Application to Enriching Word Embeddings","Ahmed El-Kishky, Frank Xu, Aston Zhang, Jiawei Han","Computation and Language (cs.CL)","Traditionally, many text-mining tasks treat individual word-tokens as the finest meaningful semantic granularity. However, in many languages and specialized corpora, words are composed by concatenating semantically meaningful subword structures. Word-level analysis cannot leverage the semantic information present in such subword structures. With regard to word embedding techniques, this leads to not only poor embeddings for infrequent words in long-tailed text corpora but also weak capabilities for handling out-of-vocabulary words. In this paper we propose MorphMine for unsupervised morpheme segmentation. MorphMine applies a parsimony criterion to hierarchically segment words into the fewest number of morphemes at each level of the hierarchy. This leads to longer shared morphemes at each level of segmentation. Experiments show that MorphMine segments words in a variety of languages into human-verified morphemes. Additionally, we experimentally demonstrate that utilizing MorphMine morphemes to enrich word embeddings consistently improves embedding quality on a variety of of embedding evaluations and a downstream language modeling task.","Sun, 18 Aug 2019 00:45:16 UTC (835 KB)"
"15","Generating an Overview Report over Many Documents","Jingwen Wang, Hao Zhang, Cheng Zhang, Wenjing Yang, Liqun Shao, Jie Wang","Computation and Language (cs.CL)","How to efficiently generate an accurate, well-structured overview report (ORPT) over thousands of related documents is challenging. A well-structured ORPT consists of sections of multiple levels (e.g., sections and subsections). None of the existing multi-document summarization (MDS) algorithms is directed toward this task. To overcome this obstacle, we present NDORGS (Numerous Documents' Overview Report Generation Scheme) that integrates text filtering, keyword scoring, single-document summarization (SDS), topic modeling, MDS, and title generation to generate a coherent, well-structured ORPT. We then devise a multi-criteria evaluation method using techniques of text mining and multi-attribute decision making on a combination of human judgments, running time, information coverage, and topic diversity. We evaluate ORPTs generated by NDORGS on two large corpora of documents, where one is classified and the other unclassified. We show that, using Saaty's pairwise comparison 9-point scale and under TOPSIS, the ORPTs generated on SDS's with the length of 20% of the original documents are the best overall on both datasets.","Sat, 17 Aug 2019 01:11:04 UTC (1,218 KB)"
"16","BERT-based Ranking for Biomedical Entity Normalization","Zongcheng Ji, Qiang Wei, Hua Xu","Information Retrieval (cs.IR)","Developing high-performance entity normalization algorithms that can alleviate the term variation problem is of great interest to the biomedical community. Although deep learning-based methods have been successfully applied to biomedical entity normalization, they often depend on traditional context-independent word embeddings. Bidirectional Encoder Representations from Transformers (BERT), BERT for Biomedical Text Mining (BioBERT) and BERT for Clinical Text Mining (ClinicalBERT) were recently introduced to pre-train contextualized word representation models using bidirectional Transformers, advancing the state-of-the-art for many natural language processing tasks. In this study, we proposed an entity normalization architecture by fine-tuning the pre-trained BERT / BioBERT / ClinicalBERT models and conducted extensive experiments to evaluate the effectiveness of the pre-trained models for biomedical entity normalization using three different types of datasets. Our experimental results show that the best fine-tuned models consistently outperformed previous methods and advanced the state-of-the-art for biomedical entity normalization, with up to 1.17% increase in accuracy.","Fri, 9 Aug 2019 17:19:43 UTC (363 KB)"
"17","Text mining policy: Classifying forest and landscape restoration policy agenda with neural information retrieval","John Brandt","Information Retrieval (cs.IR)","Dozens of countries have committed to restoring the ecological functionality of 350 million hectares of land by 2030. In order to achieve such wide-scale implementation of restoration, the values and priorities of multi-sectoral stakeholders must be aligned and integrated with national level commitments and other development agenda. Although misalignment across scales of policy and between stakeholders are well known barriers to implementing restoration, fast-paced policy making in multi-stakeholder environments complicates the monitoring and analysis of governance and policy. In this work, we assess the potential of machine learning to identify restoration policy agenda across diverse policy documents. An unsupervised neural information retrieval architecture is introduced that leverages transfer learning and word embeddings to create high-dimensional representations of paragraphs. Policy agenda labels are recast as information retrieval queries in order to classify policies with a cosine similarity threshold between paragraphs and query embeddings. This approach achieves a 0.83 F1-score measured across 14 policy agenda in 31 policy documents in Malawi, Kenya, and Rwanda, indicating that automated text mining can provide reliable, generalizable, and efficient analyses of restoration policy.","Wed, 7 Aug 2019 02:58:24 UTC (1,371 KB)"
"18","Exhaustive Exact String Matching: The Analysis of the Full Human Genome","Konstantinos F. Xylogiannopoulos","Data Structures and Algorithms (cs.DS)","Exact string matching has been a fundamental problem in computer science for decades because of many practical applications. Some are related to common procedures, such as searching in files and text editors, or, more recently, to more advanced problems such as pattern detection in Artificial Intelligence and Bioinformatics. Tens of algorithms and methodologies have been developed for pattern matching and several programming languages, packages, applications and online systems exist that can perform exact string matching in biological sequences. These techniques, however, are limited to searching for specific and predefined strings in a sequence. In this paper a novel methodology (called Ex2SM) is presented, which is a pipeline of execution of advanced data structures and algorithms, explicitly designed for text mining, that can detect every possible repeated string in multivariate biological sequences. In contrast to known algorithms in literature, the methodology presented here is string agnostic, i.e., it does not require an input string to search for it, rather it can detect every string that exists at least twice, regardless of its attributes such as length, frequency, alphabet, overlapping etc. The complexity of the problem solved and the potential of the proposed methodology is demonstrated with the experimental analysis performed on the entire human genome. More specifically, all repeated strings with a length of up to 50 characters have been detected, an achievement which is practically impossible using other algorithms due to the exponential number of possible permutations of such long strings.","Wed, 24 Jul 2019 20:50:16 UTC (604 KB)"
"19","TEAGS: Time-aware Text Embedding Approach to Generate Subgraphs","Saeid Hosseini, Saeed Najafipour, Ngai-Man Cheung, Hongzhi Yin, Mohammad Reza Kangavari, Xiaofang Zhou","Information Retrieval (cs.IR)","Contagions (e.g. virus, gossip) spread over the nodes in propagation graphs. We can use the temporal and textual data of the nodes to compute the edge weights and then generate subgraphs with highly relevant nodes. This is beneficial to many applications. Yet, challenges abound. First, the propagation pattern between each pair of nodes may change by time. Second, not always the same contagion propagates. Hence, the state-of-the-art text mining approaches including topic-modeling cannot effectively compute the edge weights. Third, since the propagation is affected by time, the word-word co-occurrence patterns may differ in various temporal dimensions, that can decrease the effectiveness of word embedding approaches. We argue that multi-aspect temporal dimensions (hour, day, etc) should be considered to better calculate the correlation weights between the nodes. In this work, we devise a novel framework that on the one hand, integrates a neural network based time-aware word embedding component to construct the word vectors through multiple temporal facets, and on the other hand, uses a temporal generative model to compute the weights. Subsequently, we propose a Max-Heap Graph cutting algorithm to generate subgraphs. We validate our model through comprehensive experiments on real-world datasets. The results show that our model can retrieve the subgraphs more effective than other rivals and the temporal dynamics should be noticed both in word embedding and propagation processes.","Sat, 6 Jul 2019 21:26:22 UTC (6,983 KB)[v2] Wed, 21 Aug 2019 13:28:23 UTC (6,983 KB)[v3] Sat, 24 Aug 2019 11:40:41 UTC (6,983 KB)"
"20","Analyses of Multi-collection Corpora via Compound Topic Modeling","Clint P. George, Wei Xia, George Michailidis","Information Retrieval (cs.IR)","As electronically stored data grow in daily life, obtaining novel and relevant information becomes challenging in text mining. Thus people have sought statistical methods based on term frequency, matrix algebra, or topic modeling for text mining. Popular topic models have centered on one single text collection, which is deficient for comparative text analyses. We consider a setting where one can partition the corpus into subcollections. Each subcollection shares a common set of topics, but there exists relative variation in topic proportions among collections. Including any prior knowledge about the corpus (e.g. organization structure), we propose the compound latent Dirichlet allocation (cLDA) model, improving on previous work, encouraging generalizability, and depending less on user-input parameters. To identify the parameters of interest in cLDA, we study Markov chain Monte Carlo (MCMC) and variational inference approaches extensively, and suggest an efficient MCMC method. We evaluate cLDA qualitatively and quantitatively using both synthetic and real-world corpora. The usability study on some real-world corpora illustrates the superiority of cLDA to explore the underlying topics automatically but also model their connections and variations across multiple collections.","Mon, 17 Jun 2019 06:59:25 UTC (380 KB)"
"21","Hidden in Plain Sight For Too Long: Using Text Mining Techniques to Shine a Light on Workplace Sexism and Sexual Harassment","Amir Karami, Suzanne C. Swan, Cynthia Nicole White, Kayla Ford","Computers and Society (cs.CY)","Objective: The goal of this study is to understand how people experience sexism and sexual harassment in the workplace by discovering themes in 2,362 experiences posted on the Everyday Sexism Project's website this http URL. Method: This study used both quantitative and qualitative methods. The quantitative method was a computational framework to collect and analyze a large number of workplace sexual harassment experiences. The qualitative method was the analysis of the topics generated by a text mining method. Results: Twenty-three topics were coded and then grouped into three overarching themes from the sex discrimination and sexual harassment literature. The Sex Discrimination theme included experiences in which women were treated unfavorably due to their sex, such as being passed over for promotion, denied opportunities, paid less than men, and ignored or talked over in meetings. The Sex Discrimination and Gender harassment theme included stories about sex discrimination and gender harassment, such as sexist hostility behaviors ranging from insults and jokes invoking misogynistic stereotypes to bullying behavior. The last theme, Unwanted Sexual Attention, contained stories describing sexual comments and behaviors used to degrade women. Unwanted touching was the highest weighted topic, indicating how common it was for website users to endure being touched, hugged or kissed, groped, and grabbed. Conclusions: This study illustrates how researchers can use automatic processes to go beyond the limits of traditional research methods and investigate naturally occurring large scale datasets on the internet to achieve a better understanding of everyday workplace sexism experiences.","Mon, 1 Jul 2019 01:48:49 UTC (1,079 KB)"
"22","Explainable Fact Checking with Probabilistic Answer Set Programming","Naser Ahmadi, Joohyung Lee, Paolo Papotti, Mohammed Saeed","Databases (cs.DB)","One challenge in fact checking is the ability to improve the transparency of the decision. We present a fact checking method that uses reference information in knowledge graphs (KGs) to assess claims and explain its decisions. KGs contain a formal representation of knowledge with semantic descriptions of entities and their relationships. We exploit such rich semantics to produce interpretable explanations for the fact checking output. As information in a KG is inevitably incomplete, we rely on logical rule discovery and on Web text mining to gather the evidence to assess a given claim. Uncertain rules and facts are turned into logical programs and the checking task is modeled as an inference problem in a probabilistic extension of answer set programs. Experiments show that the probabilistic inference enables the efficient labeling of claims with interpretable explanations, and the quality of the results is higher than state of the art baselines.","Fri, 21 Jun 2019 15:35:03 UTC (104 KB)"
"23","Meta-learning of textual representations","Jorge Madrid, Hugo Jair Escalante, Eduardo Morales","Machine Learning (cs.LG)","Recent progress in AutoML has lead to state-of-the-art methods (e.g., AutoSKLearn) that can be readily used by non-experts to approach any supervised learning problem. Whereas these methods are quite effective, they are still limited in the sense that they work for tabular (matrix formatted) data only. This paper describes one step forward in trying to automate the design of supervised learning methods in the context of text mining. We introduce a meta learning methodology for automatically obtaining a representation for text mining tasks starting from raw text. We report experiments considering 60 different textual representations and more than 80 text mining datasets associated to a wide variety of tasks. Experimental results show the proposed methodology is a promising solution to obtain highly effective off the shell text classification pipelines.","Fri, 21 Jun 2019 03:39:46 UTC (210 KB)[v2] Fri, 19 Jul 2019 21:19:49 UTC (196 KB)"
"24","Low-resource Deep Entity Resolution with Transfer and Active Learning","Jungo Kasai, Kun Qian, Sairam Gurajada, Yunyao Li, Lucian Popa","Databases (cs.DB)","Entity resolution (ER) is the task of identifying different representations of the same real-world entities across databases. It is a key step for knowledge base creation and text mining. Recent adaptation of deep learning methods for ER mitigates the need for dataset-specific feature engineering by constructing distributed representations of entity records. While these methods achieve state-of-the-art performance over benchmark data, they require large amounts of labeled data, which are typically unavailable in realistic ER applications. In this paper, we develop a deep learning-based method that targets low-resource settings for ER through a novel combination of transfer learning and active learning. We design an architecture that allows us to learn a transferable model from a high-resource setting to a low-resource one. To further adapt to the target dataset, we incorporate active learning that carefully selects a few informative examples to fine-tune the transferred model. Empirical evaluation demonstrates that our method achieves comparable, if not better, performance compared to state-of-the-art learning-based methods while using an order of magnitude fewer labels.","Mon, 17 Jun 2019 20:33:24 UTC (85 KB)"
"25","A Simple Text Mining Approach for Ranking Pairwise Associations in Biomedical Applications","Finn Kuusisto, John Steill, Zhaobin Kuang, James Thomson, David Page, Ron Stewart","Information Retrieval (cs.IR)","We present a simple text mining method that is easy to implement, requires minimal data collection and preparation, and is easy to use for proposing ranked associations between a list of target terms and a key phrase. We call this method KinderMiner, and apply it to two biomedical applications. The first application is to identify relevant transcription factors for cell reprogramming, and the second is to identify potential drugs for investigation in drug repositioning. We compare the results from our algorithm to existing data and state-of-the-art algorithms, demonstrating compelling results for both application areas. While we apply the algorithm here for biomedical applications, we argue that the method is generalizable to any available corpus of sufficient size.","Wed, 12 Jun 2019 17:28:21 UTC (137 KB)"
"26","Innovating HR Using an Expert System for Recruiting IT Specialists -- ESRIT","Ciprian-Octavian Truic<U+0103>, Adriana Barnoschi","Information Retrieval (cs.IR)","One of the most rapidly evolving and dynamic business sector is the IT domain, where there is a problem finding experienced, skilled and qualified employees. Specialists are essential for developing and implementing new ideas into products. Human resources (HR) department plays a major role in the recruitment of qualified employees by assessing their skills, using different HR metrics, and selecting the best candidates for a specific job. Most recruiters are not qualified to evaluate IT specialists. In order to decrease the gap between the HR department and IT specialists, we designed, implemented and tested an Expert System for Recruiting IT specialist - ESRIT. The expert system uses text mining, natural language processing, and classification algorithms to extract relevant information from resumes by using a knowledge base that stores the relevant key skills and phrases. The recruiter is looking for the same abilities and certificates, trying to place the best applicant into a specific position. The article presents a developing picture of the top major IT skills that will be required in 2014 and it argues for the choice of the IT abilities domain.","Tue, 11 Jun 2019 13:15:01 UTC (561 KB)"
"27","Hierarchical Taxonomy-Aware and Attentional Graph Capsule RCNNs for Large-Scale Multi-Label Text Classification","Hao Peng, Jianxin Li, Qiran Gong, Senzhang Wang, Lifang He, Bo Li, Lihong Wang, Philip S. Yu","Information Retrieval (cs.IR)","CNNs, RNNs, GCNs, and CapsNets have shown significant insights in representation learning and are widely used in various text mining tasks such as large-scale multi-label text classification. However, most existing deep models for multi-label text classification consider either the non-consecutive and long-distance semantics or the sequential semantics, but how to consider them both coherently is less studied. In addition, most existing methods treat output labels as independent methods, but ignore the hierarchical relations among them, leading to useful semantic information loss. In this paper, we propose a novel hierarchical taxonomy-aware and attentional graph capsule recurrent CNNs framework for large-scale multi-label text classification. Specifically, we first propose to model each document as a word order preserved graph-of-words and normalize it as a corresponding words-matrix representation which preserves both the non-consecutive, long-distance and local sequential semantics. Then the words-matrix is input to the proposed attentional graph capsule recurrent CNNs for more effectively learning the semantic features. To leverage the hierarchical relations among the class labels, we propose a hierarchical taxonomy embedding method to learn their representations, and define a novel weighted margin loss by incorporating the label representation similarity. Extensive evaluations on three datasets show that our model significantly improves the performance of large-scale multi-label text classification by comparing with state-of-the-art approaches.","Sun, 9 Jun 2019 07:23:45 UTC (4,462 KB)"
"28","Exploring Diseases and Syndromes in Neurology Case Reports from 1955 to 2017 with Text Mining","Amir Karami, Mehdi Ghasemi, Souvik Sen, Marcos Moraes, Vishal Shah","Quantitative Methods (q-bio.QM)","Background: A large number of neurology case reports have been published, but it is a challenging task for human medical experts to explore all of these publications. Text mining offers a computational approach to investigate neurology literature and capture meaningful patterns. The overarching goal of this study is to provide a new perspective on case reports of neurological disease and syndrome analysis over the last six decades using text mining. Methods: We extracted diseases and syndromes (DsSs) from more than 65,000 neurology case reports from 66 journals in PubMed over the last six decades from 1955 to 2017. Text mining was applied to reports on the detected DsSs to investigate high-frequency DsSs, categorize them, and explore the linear trends over the 63-year time frame. Results: The text mining methods explored high-frequency neurologic DsSs and their trends and the relationships between them from 1955 to 2017. We detected more than 18,000 unique DsSs and found 10 categories of neurologic DsSs. While the trend analysis showed the increasing trends in the case reports for top-10 high-frequency DsSs, the categories had mixed trends. Conclusion: Our study provided new insights into the application of text mining methods to investigate DsSs in a large number of medical case reports that occur over several decades. The proposed approach can be used to provide a macro level analysis of medical literature by discovering interesting patterns and tracking them over several years to help physicians explore these case reports more efficiently.","Thu, 23 May 2019 17:38:06 UTC (2,052 KB)"
"29","Generalized Separable Nonnegative Matrix Factorization","Junjun Pan, Nicolas Gillis","Machine Learning (cs.LG)","Nonnegative matrix factorization (NMF) is a linear dimensionality technique for nonnegative data with applications such as image analysis, text mining, audio source separation and hyperspectral unmixing. Given a data matrix $M$ and a factorization rank $r$, NMF looks for a nonnegative matrix $W$ with $r$ columns and a nonnegative matrix $H$ with $r$ rows such that $M \approx WH$. NMF is NP-hard to solve in general. However, it can be computed efficiently under the separability assumption which requires that the basis vectors appear as data points, that is, that there exists an index set $\mathcal{K}$ such that $W = M(:,\mathcal{K})$. In this paper, we generalize the separability assumption: We only require that for each rank-one factor $W(:,k)H(k,:)$ for $k=1,2,\dots,r$, either $W(:,k) = M(:,j)$ for some $j$ or $H(k,:) = M(i,:)$ for some $i$. We refer to the corresponding problem as generalized separable NMF (GS-NMF). We discuss some properties of GS-NMF and propose a convex optimization model which we solve using a fast gradient method. We also propose a heuristic algorithm inspired by the successive projection algorithm. To verify the effectiveness of our methods, we compare them with several state-of-the-art separable NMF algorithms on synthetic, document and image data sets.","Thu, 30 May 2019 12:18:25 UTC (788 KB)"
"30","From web crawled text to project descriptions: automatic summarizing of social innovation projects","Nikola Milosevic, Dimitar Marinov, Abdullah Gok, Goran Nenadic","Computation and Language (cs.CL)","In the past decade, social innovation projects have gained the attention of policy makers, as they address important social issues in an innovative manner. A database of social innovation is an important source of information that can expand collaboration between social innovators, drive policy and serve as an important resource for research. Such a database needs to have projects described and summarized. In this paper, we propose and compare several methods (e.g. SVM-based, recurrent neural network based, ensambled) for describing projects based on the text that is available on project websites. We also address and propose a new metric for automated evaluation of summaries based on topic modelling.","Wed, 22 May 2019 11:49:37 UTC (72 KB)"
"31","A joint text mining-rank size investigation of the rhetoric structures of the US Presidents' speeches","Valerio Ficcadenti, Roy Cerqueti, Marcel Ausloos","Computation and Language (cs.CL)","This work presents a text mining context and its use for a deep analysis of the messages delivered by the politicians. Specifically, we deal with an expert systems-based exploration of the rhetoric dynamics of a large collection of US Presidents' speeches, ranging from Washington to Trump. In particular, speeches are viewed as complex expert systems whose structures can be effectively analyzed through rank-size laws. The methodological contribution of the paper is twofold. First, we develop a text mining-based procedure for the construction of the dataset by using a web scraping routine on the Miller Center website -- the repository collecting the speeches. Second, we explore the implicit structure of the discourse data by implementing a rank-size procedure over the individual speeches, being the words of each speech ranked in terms of their frequencies. The scientific significance of the proposed combination of text-mining and rank-size approaches can be found in its flexibility and generality, which let it be reproducible to a wide set of expert systems and text mining contexts. The usefulness of the proposed method and the speech subsequent analysis is demonstrated by the findings themselves. Indeed, in terms of impact, it is worth noting that interesting conclusions of social, political and linguistic nature on how 45 United States Presidents, from April 30, 1789 till February 28, 2017 delivered political messages can be carried out. Indeed, the proposed analysis shows some remarkable regularities, not only inside a given speech, but also among different speeches. Moreover, under a purely methodological perspective, the presented contribution suggests possible ways of generating a linguistic decision-making algorithm.","Thu, 9 May 2019 14:17:15 UTC (2,549 KB)"
"32","Metadata Management for Textual Documents in Data Lakes","Pegdwende Sawadogo (ERIC), Tokio Kibata, Jerome Darmont (ERIC)","Databases (cs.DB)","Data lakes have emerged as an alternative to data warehouses for the storage, exploration and analysis of big data. In a data lake, data are stored in a raw state and bear no explicit schema. Thence, an efficient metadata system is essential to avoid the data lake turning to a so-called data swamp. Existing works about managing data lake metadata mostly focus on structured and semi-structured data, with little research on unstructured data. Thus, we propose in this paper a methodological approach to build and manage a metadata system that is specific to textual documents in data lakes. First, we make an inventory of usual and meaningful metadata to extract. Then, we apply some specific techniques from the text mining and information retrieval domains to extract, store and reuse these metadata within the COREL research project, in order to validate our proposals.","Fri, 10 May 2019 09:46:01 UTC (486 KB)"
"33","Where does active travel fit within local community narratives of mobility space and place?","Alec Biehl, Ying Chen, Karla Sanabria-Veaz, David Uttal, Amanda Stathopoulos","General Economics (econ.GN)","Encouraging sustainable mobility patterns is at the forefront of policymaking at all scales of governance as the collective consciousness surrounding climate change continues to expand. Not every community, however, possesses the necessary economic or socio-cultural capital to encourage modal shifts away from private motorized vehicles towards active modes. The current literature on `soft' policy emphasizes the importance of tailoring behavior change campaigns to individual or geographic context. Yet, there is a lack of insight and appropriate tools to promote active mobility and overcome transport disadvantage from the local community perspective. The current study investigates the promotion of walking and cycling adoption using a series of focus groups with local residents in two geographic communities, namely Chicago's (1) Humboldt Park neighborhood and (2) suburb of Evanston. The research approach combines traditional qualitative discourse analysis with quantitative text-mining tools, namely topic modeling and sentiment analysis. The analysis uncovers the local mobility culture, embedded norms and values associated with acceptance of active travel modes in different communities. We observe that underserved populations within diverse communities view active mobility simultaneously as a necessity and as a symbol of privilege that is sometimes at odds with the local culture. The mixed methods approach to analyzing community member discourses is translated into policy findings that are either tailored to local context or broadly applicable to curbing automobile dominance. Overall, residents of both Humboldt Park and Evanston envision a society in which multimodalism replaces car-centrism, but differences in the local physical and social environments would and should influence the manner in which overarching policy objectives are met.","Tue, 7 May 2019 16:25:12 UTC (491 KB)"
"34","Analysis of Chinese Tourists in Japan by Text Mining of a Hotel Portal Site","Elisa Claire Aleman Carreon, Hirofumi Nonaka, Toru Hiraoka","Information Retrieval (cs.IR)","With an increasingly large number of Chinese tourists in Japan, the hotel industry is in need of an affordable market research tool that does not rely on expensive and time-consuming surveys or interviews. Because this problem is real and relevant to the hotel industry in Japan, and otherwise completely unexplored in other studies, we have extracted a list of potential keywords from Chinese reviews of Japanese hotels in the hotel portal site Ctrip1 using a mathematical model to then use them in a sentiment analysis with a machine learning classifier. While most studies that use information collected from the internet use pre-existing data analysis tools, in our study, we designed the mathematical model to have the highest possible performing results in classification, while also exploring on the potential business implications these may have.","Wed, 24 Apr 2019 15:33:28 UTC (206 KB)[v2] Wed, 1 May 2019 04:59:25 UTC (206 KB)"
"35","Exploring the Daschle Collection using Text Mining","Damon Bayer, Semhar Michael","Information Retrieval (cs.IR)","A U.S. Senator from South Dakota donated documents that were accumulated during his service as a house representative and senator to be housed at the Bridges library at South Dakota State University. This project investigated the utility of quantitative statistical methods to explore some portions of this vast document collection. The available scanned documents and emails from constituents are analyzed using natural language processing methods including the Latent Dirichlet Allocation (LDA) model. This model identified major topics being discussed in a given collection of documents. Important events and popular issues from the Senator Daschles career are reflected in the changing topics from the model. These quantitative statistical methods provide a summary of the massive amount of text without requiring significant human effort or time and can be applied to similar collections.","Tue, 23 Apr 2019 16:31:20 UTC (3,013 KB)"
"36","#Cyberbullying in the Digital Age: Exploring People's Opinions with Text Mining","Iman Tahamtan, Li-Min Huang","Computers and Society (cs.CY)","This study used text mining to investigate people's insights about cyberbullying. English-language tweets were collected and analyzed by R software. Our analysis demonstrated three major themes: (a) the major actions that needed to be taken into consideration (e.g. guiding parents and teachers to cyberbullying prevention, funding schools to fight cyberbullying), (b) certain events that were important to people (e.g. the Michigan cyberbullying law), and (c) people's major concerns in this regard (e.g. mental health issues among students). Parents and teachers have an important role in educating, informing, warning, preventing, and protecting against cyberbullying behaviors. The frequency of negative sentiments was almost 2.45 times more than positive sentiments.","Mon, 15 Apr 2019 21:41:46 UTC (787 KB)[v2] Fri, 14 Jun 2019 19:09:35 UTC (866 KB)[v3] Tue, 23 Jul 2019 23:28:42 UTC (574 KB)"
"37","Holistic and Comprehensive Annotation of Clinically Significant Findings on Diverse CT Images: Learning from Radiology Reports and Label Ontology","Ke Yan, Yifan Peng, Veit Sandfort, Mohammadhadi Bagheri, Zhiyong Lu, Ronald M. Summers","Computer Vision and Pattern Recognition (cs.CV)","In radiologists' routine work, one major task is to read a medical image, e.g., a CT scan, find significant lesions, and describe them in the radiology report. In this paper, we study the lesion description or annotation problem. Given a lesion image, our aim is to predict a comprehensive set of relevant labels, such as the lesion's body part, type, and attributes, which may assist downstream fine-grained diagnosis. To address this task, we first design a deep learning module to extract relevant semantic labels from the radiology reports associated with the lesion images. With the images and text-mined labels, we propose a lesion annotation network (LesaNet) based on a multilabel convolutional neural network (CNN) to learn all labels holistically. Hierarchical relations and mutually exclusive relations between the labels are leveraged to improve the label prediction accuracy. The relations are utilized in a label expansion strategy and a relational hard example mining algorithm. We also attach a simple score propagation layer on LesaNet to enhance recall and explore implicit relation between labels. Multilabel metric learning is combined with classification to enable interpretable prediction. We evaluated LesaNet on the public DeepLesion dataset, which contains over 32K diverse lesion images. Experiments show that LesaNet can precisely annotate the lesions using an ontology of 171 fine-grained labels with an average AUC of 0.9344.","Tue, 9 Apr 2019 13:34:31 UTC (1,815 KB)[v2] Sat, 27 Apr 2019 00:45:27 UTC (1,815 KB)"
"38","On Attribution of Recurrent Neural Network Predictions via Additive Decomposition","Mengnan Du, Ninghao Liu, Fan Yang, Shuiwang Ji, Xia Hu","Computation and Language (cs.CL)","RNN models have achieved the state-of-the-art performance in a wide range of text mining tasks. However, these models are often regarded as black-boxes and are criticized due to the lack of interpretability. In this paper, we enhance the interpretability of RNNs by providing interpretable rationales for RNN predictions. Nevertheless, interpreting RNNs is a challenging problem. Firstly, unlike existing methods that rely on local approximation, we aim to provide rationales that are more faithful to the decision making process of RNN models. Secondly, a flexible interpretation method should be able to assign contribution scores to text segments of varying lengths, instead of only to individual words. To tackle these challenges, we propose a novel attribution method, called REAT, to provide interpretations to RNN predictions. REAT decomposes the final prediction of a RNN into additive contribution of each word in the input text. This additive decomposition enables REAT to further obtain phrase-level attribution scores. In addition, REAT is generally applicable to various RNN architectures, including GRU, LSTM and their bidirectional versions. Experimental results demonstrate the faithfulness and interpretability of the proposed attribution method. Comprehensive analysis shows that our attribution method could unveil the useful linguistic knowledge captured by RNNs. Some analysis further demonstrates our method could be utilized as a debugging tool to examine the vulnerability and failure reasons of RNNs, which may lead to several promising future directions to promote generalization ability of RNNs.","Wed, 27 Mar 2019 04:25:57 UTC (672 KB)"
"39","git2net - Mining Time-Stamped Co-Editing Networks from Large git Repositories","Christoph Gote, Ingo Scholtes, Frank Schweitzer","Software Engineering (cs.SE)","Data from software repositories have become an important foundation for the empirical study of software engineering processes. A recurring theme in the repository mining literature is the inference of developer networks capturing e.g. collaboration, coordination, or communication from the commit history of projects. Most of the studied networks are based on the co-authorship of software artefacts defined at the level of files, modules, or packages. While this approach has led to insights into the social aspects of software development, it neglects detailed information on code changes and code ownership, e.g. which exact lines of code have been authored by which developers, that is contained in the commit log of software projects. Addressing this issue, we introduce git2net, a scalable python software that facilitates the extraction of fine-grained co-editing networks in large git repositories. It uses text mining techniques to analyse the detailed history of textual modifications within files. This information allows us to construct directed, weighted, and time-stamped networks, where a link signifies that one developer has edited a block of source code originally written by another developer. Our tool is applied in case studies of an Open Source and a commercial software project. We argue that it opens up a massive new source of high-resolution data on human collaboration patterns.","Mon, 25 Mar 2019 08:49:26 UTC (1,346 KB)"
"40","Redditors in Recovery: Text Mining Reddit to Investigate Transitions into Drug Addiction","John Lu, Sumati Sridhar, Ritika Pandey, Mohammad Al Hasan, George Mohler","Social and Information Networks (cs.SI)","Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing data mining techniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtain data from Reddit, an online collection of forums, to gather insight into drug use/misuse using text data from users themselves. Specifically, using user posts, we trained 1) a binary classifier which predicts transitions from casual drug discussion forums to drug recovery forums and 2) a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.","Mon, 11 Mar 2019 00:10:29 UTC (1,571 KB)"
"41","Twitter Speaks: A Case of National Disaster Situational Awareness","Amir Karami, Vishal Shah, Reza Vaezi, Amit Bansal","Social and Information Networks (cs.SI)","In recent years, we have been faced with a series of natural disasters causing a tremendous amount of financial, environmental, and human losses. The unpredictable nature of natural disasters' behavior makes it hard to have a comprehensive situational awareness (SA) to support disaster management. Using opinion surveys is a traditional approach to analyze public concerns during natural disasters; however, this approach is limited, expensive, and time-consuming. Luckily the advent of social media has provided scholars with an alternative means of analyzing public concerns. Social media enable users (people) to freely communicate their opinions and disperse information regarding current events including natural disasters. This research emphasizes the value of social media analysis and proposes an analytical framework: Twitter Situational Awareness (TwiSA). This framework uses text mining methods including sentiment analysis and topic modeling to create a better SA for disaster preparedness, response, and recovery. TwiSA has also effectively deployed on a large number of tweets and tracks the negative concerns of people during the 2015 South Carolina flood.","Thu, 7 Mar 2019 03:02:00 UTC (249 KB)"
"42","Deep Sentiment Analysis using a Graph-based Text Representation","Kayvan Bijari, Hadi Zare, Hadi Veisi, Emad Kebriaei","Computation and Language (cs.CL)","Social media brings about new ways of communication among people and is influencing trading strategies in the market. The popularity of social networks produces a large collection of unstructured data such as text and image in a variety of disciplines like business and health. The main element of social media arises as text which provokes a set of challenges for traditional information retrieval and natural language processing tools. Informal language, spelling errors, abbreviations, and special characters are typical in social media posts. These features lead to a prohibitively large vocabulary size for text mining methods. Another problem with traditional social text mining techniques is that they fail to take semantic relations into account, which is essential in a domain of applications such as event detection, opinion mining, and news recommendation. This paper set out to employ a network-based viewpoint on text documents and investigate the usefulness of graph representation to exploit word relations and semantics of the textual data. Moreover, the proposed approach makes use of a random walker to extract deep features of a graph to facilitate the task of document classification. The experimental results indicate that the proposed approach defeats the earlier sentiment analysis methods based on several benchmark datasets, and it generalizes well on different datasets without dependency for pre-trained word embeddings.","Sat, 23 Feb 2019 16:38:35 UTC (77 KB)[v2] Thu, 29 Aug 2019 12:25:14 UTC (352 KB)"
"43","A framework for information extraction from tables in biomedical literature","Nikola Milosevic, Cassie Gregson, Robert Hernandez, Goran Nenadic","Computation and Language (cs.CL)","The scientific literature is growing exponentially, and professionals are no more able to cope with the current amount of publications. Text mining provided in the past methods to retrieve and extract information from text; however, most of these approaches ignored tables and figures. The research done in mining table data still does not have an integrated approach for mining that would consider all complexities and challenges of a table. Our research is examining the methods for extracting numerical (number of patients, age, gender distribution) and textual (adverse reactions) information from tables in the clinical literature. We present a requirement analysis template and an integral methodology for information extraction from tables in clinical domain that contains 7 steps: (1) table detection, (2) functional processing, (3) structural processing, (4) semantic tagging, (5) pragmatic processing, (6) cell selection and (7) syntactic processing and extraction. Our approach performed with the F-measure ranged between 82 and 92%, depending on the variable, task and its complexity.","Tue, 26 Feb 2019 16:22:15 UTC (1,970 KB)"
"44","Comprehensive review of models and methods for inferences in bio-chemical reaction networks","Pavel Loskot, Komlan Atitey, Lyudmila Mihaylova","Quantitative Methods (q-bio.QM)","Key processes in biological and chemical systems are described by networks of chemical reactions. From molecular biology to biotechnology applications, computational models of reaction networks are used extensively to elucidate their non-linear dynamics. Model dynamics are crucially dependent on parameter values which are often estimated from observations. Over past decade, the interest in parameter and state estimation in models of (bio-)chemical reaction networks (BRNs) grew considerably. Statistical inference problems are also encountered in many other tasks including model calibration, discrimination, identifiability and checking as well as optimum experiment design, sensitivity analysis, bifurcation analysis and other. The aim of this review paper is to explore developments of past decade to understand what BRN models are commonly used in literature, and for what inference tasks and inference methods. Initial collection of about 700 publications excluding books in computational biology and chemistry were screened to select over 260 research papers and 20 graduate theses concerning estimation problems in BRNs. The paper selection was performed as text mining using scripts to automate search for relevant keywords and terms. The outcome are tables revealing the level of interest in different inference tasks and methods for given models in literature as well as recent trends. In addition, a brief survey of general estimation strategies is provided to facilitate understanding of estimation methods which are used for BRNs. Our findings indicate that many combinations of models, tasks and methods are still relatively sparse representing new research opportunities to explore those that have not been considered - perhaps for a good reason. The paper concludes by discussing future research directions including research problems which cannot be directly deduced from presented tables.","Fri, 15 Feb 2019 14:52:35 UTC (1,023 KB)"
"45","A new simple and effective measure for bag-of-word inter-document similarity measurement","Sunil Aryal, Kai Ming Ting, Takashi Washio, Gholamreza Haffari","Information Retrieval (cs.IR)","To measure the similarity of two documents in the bag-of-words (BoW) vector representation, different term weighting schemes are used to improve the performance of cosine similarity---the most widely used inter-document similarity measure in text mining. In this paper, we identify the shortcomings of the underlying assumptions of term weighting in the inter-document similarity measurement task; and provide a more fit-to-the-purpose alternative. Based on this new assumption, we introduce a new simple but effective similarity measure which does not require explicit term weighting. The proposed measure employs a more nuanced probabilistic approach than those used in term weighting to measure the similarity of two documents w.r.t each term occurring in the two documents. Our empirical comparison with the existing similarity measures using different term weighting schemes shows that the new measure produces (i) better results in the binary BoW representation; and (ii) competitive and more consistent results in the term-frequency-based BoW representation.","Sat, 9 Feb 2019 10:32:45 UTC (1,261 KB)"
"46","Multi-task Learning for Target-dependent Sentiment Classification","Divam Gupta, Kushagra Singh, Soumen Chakrabarti, Tanmoy Chakraborty","Machine Learning (cs.LG)","Detecting and aggregating sentiments toward people, organizations, and events expressed in unstructured social media have become critical text mining operations. Early systems detected sentiments over whole passages, whereas more recently, target-specific sentiments have been of greater interest. In this paper, we present MTTDSC, a multi-task target-dependent sentiment classification system that is informed by feature representation learnt for the related auxiliary task of passage-level sentiment classification. The auxiliary task uses a gated recurrent unit (GRU) and pools GRU states, followed by an auxiliary fully-connected layer that outputs passage-level predictions. In the main task, these GRUs contribute auxiliary per-token representations over and above word embeddings. The main task has its own, separate GRUs. The auxiliary and main GRUs send their states to a different fully connected layer, trained for the main task. Extensive experiments using two auxiliary datasets and three benchmark datasets (of which one is new, introduced by us) for the main task demonstrate that MTTDSC outperforms state-of-the-art baselines. Using word-level sensitivity analysis, we present anecdotal evidence that prior systems can make incorrect target-specific predictions because they miss sentiments expressed by words independent of target.","Fri, 8 Feb 2019 03:58:09 UTC (348 KB)"
"47","How to ""DODGE"" Complex Software Analytics?","Amritanshu Agrawal, Wei Fu, Di Chen, Xipeng Shen, Tim Menzies","Software Engineering (cs.SE)","AI software is still software. Software engineers need better tools to make better use of AI software. For example, for software defect prediction and software text mining, the default tunings for software analytics tools can be improved with ""hyperparameter optimization"" tools that decide (e.g.,) how many trees are needed in a random forest. Hyperparameter optimization is unnecessarily slow when optimizers waste time exploring redundant options (i.e., pairs of tunings with indistinguishably different results). By ignoring redundant tunings, the Dodge(E) hyperparameter optimization tool can run orders of magnitude faster, yet still find better tunings than prior state-of-the-art algorithms (for software defect prediction and software text mining).","Tue, 5 Feb 2019 18:16:56 UTC (1,074 KB)"
"48","A Multi-Resolution Word Embedding for Document Retrieval from Large Unstructured Knowledge Bases","Tolgahan Cakaloglu, Xiaowei Xu","Information Retrieval (cs.IR)","Deep language models learning a hierarchical representation proved to be a powerful tool for natural language processing, text mining and information retrieval. However, representations that perform well for retrieval must capture semantic meaning at different levels of abstraction or context-scopes. In this paper, we propose a new method to generate multi-resolution word embeddings that represent documents at multiple resolutions in terms of context-scopes. In order to investigate its performance,we use the Stanford Question Answering Dataset (SQuAD) and the Question Answering by Search And Reading (QUASAR) in an open-domain question-answering setting, where the first task is to find documents useful for answering a given question. To this end, we first compare the quality of various text-embedding methods for retrieval performance and give an extensive empirical comparison with the performance of various non-augmented base embeddings with and without multi-resolution representation. We argue that multi-resolution word embeddings are consistently superior to the original counterparts and deep residual neural models specifically trained for retrieval purposes can yield further significant gains when they are used for augmenting those embeddings.","Sat, 2 Feb 2019 07:44:41 UTC (341 KB)[v2] Tue, 19 Feb 2019 18:01:07 UTC (341 KB)[v3] Thu, 21 Feb 2019 19:22:45 UTC (341 KB)[v4] Thu, 2 May 2019 17:47:54 UTC (415 KB)[v5] Thu, 9 May 2019 06:46:00 UTC (415 KB)[v6] Fri, 10 May 2019 20:25:14 UTC (411 KB)[v7] Wed, 22 May 2019 23:03:24 UTC (902 KB)"
"49","Revised JNLPBA Corpus: A Revised Version of Biomedical NER Corpus for Relation Extraction Task","Ming-Siang Huang, Po-Ting Lai, Richard Tzong-Han Tsai, Wen-Lian Hsu","Information Retrieval (cs.IR)","The advancement of biomedical named entity recognition (BNER) and biomedical relation extraction (BRE) researches promotes the development of text mining in biological domains. As a cornerstone of BRE, robust BNER system is required to identify the mentioned NEs in plain texts for further relation extraction stage. However, the current BNER corpora, which play important roles in these tasks, paid less attention to achieve the criteria for BRE task. In this study, we present Revised JNLPBA corpus, the revision of JNLPBA corpus, to broaden the applicability of a NER corpus from BNER to BRE task. We preserve the original entity types including protein, DNA, RNA, cell line and cell type while all the abstracts in JNLPBA corpus are manually curated by domain experts again basis on the new annotation guideline focusing on the specific NEs instead of general terms. Simultaneously, several imperfection issues in JNLPBA are pointed out and made up in the new corpus. To compare the adaptability of different NER systems in Revised JNLPBA and JNLPBA corpora, the F1-measure was measured in three open sources NER systems including BANNER, Gimli and NERSuite. In the same circumstance, all the systems perform average 10% better in Revised JNLPBA than in JNLPBA. Moreover, the cross-validation test is carried out which we train the NER systems on JNLPBA/Revised JNLPBA corpora and access the performance in both protein-protein interaction extraction (PPIE) and biomedical event extraction (BEE) corpora to confirm that the newly refined Revised JNLPBA is a competent NER corpus in biomedical relation application. The revised JNLPBA corpus is freely available at this http URL.","Tue, 29 Jan 2019 11:12:58 UTC (379 KB)"
"50","BioBERT: a pre-trained biomedical language representation model for biomedical text mining","Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang","Computation and Language (cs.CL)","Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in machine learning, extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, as deep learning models require a large amount of training data, applying deep learning to biomedical text mining is often unsuccessful due to the lack of training data in biomedical fields. Recent researches on training contextualized language representation models on text corpora shed light on the possibility of leveraging a large number of unannotated biomedical text corpora. We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain specific language representation model pre-trained on large-scale biomedical corpora. Based on the BERT architecture, BioBERT effectively transfers the knowledge from a large amount of biomedical texts to biomedical text mining models with minimal task-specific architecture modifications. While BERT shows competitive performances with previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.51% absolute improvement), biomedical relation extraction (3.49% absolute improvement), and biomedical question answering (9.61% absolute improvement). We make the pre-trained weights of BioBERT freely available at this https URL, and the source code for fine-tuning BioBERT available at this https URL.","Fri, 25 Jan 2019 05:57:24 UTC (1,090 KB)[v2] Tue, 29 Jan 2019 06:43:51 UTC (1,025 KB)[v3] Sun, 3 Feb 2019 09:06:53 UTC (1,028 KB)"
"51","Text Mining Customer Reviews For Aspect-based Restaurant Rating","Jovelyn C. Cuizon, Jesserine Lopez, Danica Rose Jones","Computation and Language (cs.CL)","This study applies text mining to analyze customer reviews and automatically assign a collective restaurant star rating based on five predetermined aspects: ambiance, cost, food, hygiene, and service. The application provides a web and mobile crowd sourcing platform where users share dining experiences and get insights about the strengths and weaknesses of a restaurant through user contributed feedback. Text reviews are tokenized into sentences. Noun-adjective pairs are extracted from each sentence using Stanford Core NLP library and are associated to aspects based on the bag of associated words fed into the system. The sentiment weight of the adjectives is determined through AFINN library. An overall restaurant star rating is computed based on the individual aspect rating. Further, a word cloud is generated to provide visual display of the most frequently occurring terms in the reviews. The more feedbacks are added the more reflective the sentiment score to the restaurants' performance.","Mon, 7 Jan 2019 01:57:21 UTC (455 KB)"
"52","Sentiment Classification of Customer Reviews about Automobiles in Roman Urdu","Moin Khan, Kamran Malik","Computation and Language (cs.CL)","Text mining is a broad field having sentiment mining as its important constituent in which we try to deduce the behavior of people towards a specific item, merchandise, politics, sports, social media comments, review sites etc. Out of many issues in sentiment mining, analysis and classification, one major issue is that the reviews and comments can be in different languages like English, Arabic, Urdu etc. Handling each language according to its rules is a difficult task. A lot of research work has been done in English Language for sentiment analysis and classification but limited sentiment analysis work is being carried out on other regional languages like Arabic, Urdu and Hindi. In this paper, Waikato Environment for Knowledge Analysis (WEKA) is used as a platform to execute different classification models for text classification of Roman Urdu text. Reviews dataset has been scrapped from different automobiles sites. These extracted Roman Urdu reviews, containing 1000 positive and 1000 negative reviews, are then saved in WEKA attribute-relation file format (arff) as labeled examples. Training is done on 80% of this data and rest of it is used for testing purpose which is done using different models and results are analyzed in each case. The results show that Multinomial Naive Bayes outperformed Bagging, Deep Neural Network, Decision Tree, Random Forest, AdaBoost, k-NN and SVM Classifiers in terms of more accuracy, precision, recall and F-measure.","Sun, 30 Dec 2018 18:50:35 UTC (348 KB)"
"53","Applying Text Mining to Protest Stories as Voice against Media Censorship","Tahsin Mayeesha, Zareen Tasneem, Jasmine Jones, Nova Ahmed","Social and Information Networks (cs.SI)","Data driven activism attempts to collect, analyze and visualize data to foster social change. However, during media censorship it is often impossible to collect such data. Here we demonstrate that data from personal stories can also help us to gain insights about protests and activism which can work as a voice for the activists. We analyze protest story data by extracting location network from the stories and perform emotion mining to get insight about the protest.","Sat, 29 Dec 2018 19:50:30 UTC (576 KB)"
"54","Asynchronous Training of Word Embeddings for Large Text Corpora","Avishek Anand, Megha Khosla, Jaspreet Singh, Jan-Hendrik Zab, Zijian Zhang","Machine Learning (cs.LG)","Word embeddings are a powerful approach for analyzing language and have been widely popular in numerous tasks in information retrieval and text mining. Training embeddings over huge corpora is computationally expensive because the input is typically sequentially processed and parameters are synchronously updated. Distributed architectures for asynchronous training that have been proposed either focus on scaling vocabulary sizes and dimensionality or suffer from expensive synchronization latencies. In this paper, we propose a scalable approach to train word embeddings by partitioning the input space instead in order to scale to massive text corpora while not sacrificing the performance of the embeddings. Our training procedure does not involve any parameter synchronization except a final sub-model merge phase that typically executes in a few minutes. Our distributed training scales seamlessly to large corpus sizes and we get comparable and sometimes even up to 45% performance improvement in a variety of NLP benchmarks using models trained by our distributed procedure which requires $1/10$ of the time taken by the baseline approach. Finally we also show that we are robust to missing words in sub-models and are able to effectively reconstruct word representations.","Fri, 7 Dec 2018 11:34:33 UTC (3,612 KB)"
"55","A PMU-based Multivariate Model for Classifying Power System Events","Rui Ma, Sagnik Basumallik, Sara Eftekharnejad","Signal Processing (eess.SP)","Real-time transient event identification is essential for power system situational awareness and protection. The increased penetration of Phasor Measurement Units (PMUs) enhance power system visualization and real time monitoring and control. However, a malicious false data injection attack on PMUs can provide wrong data that might prompt the operator to take incorrect actions which can eventually jeopardize system reliability. In this paper, a multivariate method based on text mining is applied to detect false data and identify transient events by analyzing the attributes of each individual PMU time series and their relationship. It is shown that the proposed approach is efficient in detecting false data and identifying each transient event regardless of the system topology and loading condition as well as the coverage rate and placement of PMUs. The proposed method is tested on IEEE 30-bus system and the classification results are provided.","Sat, 1 Dec 2018 19:18:02 UTC (948 KB)"
"56","Naive Dictionary On Musical Corpora: From Knowledge Representation To Pattern Recognition","Qiuyi Wu, Ernest Fokoue","Information Retrieval (cs.IR)","In this paper, we propose and develop the novel idea of treating musical sheets as literary documents in the traditional text analytics parlance, to fully benefit from the vast amount of research already existing in statistical text mining and topic modelling. We specifically introduce the idea of representing any given piece of music as a collection of ""musical words"" that we codenamed ""muselets"", which are essentially musical words of various lengths. Given the novelty and therefore the extremely difficulty of properly forming a complete version of a dictionary of muselets, the present paper focuses on a simpler albeit naive version of the ultimate dictionary, which we refer to as a Naive Dictionary because of the fact that all the words are of the same length. We specifically herein construct a naive dictionary featuring a corpus made up of African American, Chinese, Japanese and Arabic music, on which we perform both topic modelling and pattern recognition. Although some of the results based on the Naive Dictionary are reasonably good, we anticipate phenomenal predictive performances once we get around to actually building a full scale complete version of our intended dictionary of muselets.","Thu, 29 Nov 2018 02:10:57 UTC (4,994 KB)"
"57","Scalable graph-based individual named entity identification","Sammy Khalife, Michalis Vazirgiannis","Information Retrieval (cs.IR)","Named entity discovery (NED) is an important information retrieval problem that can be decomposed into two sub-problems. The first sub-problem, named entity recognition (NER), aims to tag pre-defined sets of words in a vocabulary (called ""named entities"": names, places, locations, ...) when they appear in natural language. The second subproblem, named entity linking/identification (NEL), considers these entity mentions as queries to be identified in a pre-existing database. In this paper, we consider the NEL problem, and assume a set of queries (or mentions) that have to be identified within a knowledge base. This knowledge base is represented by a text database paired with a semantic graph. We present state-of-the-art methods in NEL, and propose a 2-step method for individual identification of named entities. Our approach is well-motivated by the limitations brought by recent deep learning approaches that lack interpratability, and require lots of parameter tuning along with large volume of annotated data. First of all, we propose a filtering algorithm designed with information retrieval and text mining techniques, aiming to maximize precision at K (typically for 5 <= K <=20). Then, we introduce two graph-based methods for named entity identification to maximize precision at 1 by re-ranking the remaining top entity candidates. The first identification method is using parametrized graph mining, and the second similarity with graph kernels. Our approach capitalizes on a fine-grained classification of entities from annotated web data. We present our algorithms in details, and show experimentally on standard datasets (NIST TAC-KBP, CONLL/AIDA) their performance in terms of precision are better than any graph-based method reported, and competitive with state-of-the-art systems. Finally, we conclude on the advantages of our graph-based approach compared to recent deep learning methods.","Mon, 26 Nov 2018 17:50:40 UTC (30 KB)"
"58","Creating a contemporary corpus of similes in Serbian by using natural language processing","Nikola Milosevic, Goran Nenadic","Computation and Language (cs.CL)","Simile is a figure of speech that compares two things through the use of connection words, but where comparison is not intended to be taken literally. They are often used in everyday communication, but they are also a part of linguistic cultural heritage. In this paper we present a methodology for semi-automated collection of similes from the World Wide Web using text mining and machine learning techniques. We expanded an existing corpus by collecting 442 similes from the internet and adding them to the existing corpus collected by Vuk Stefanovic Karadzic that contained 333 similes. We, also, introduce crowdsourcing to the collection of figures of speech, which helped us to build corpus containing 787 unique similes.","Thu, 22 Nov 2018 12:55:40 UTC (542 KB)"
"59","A Rule-based Kurdish Text Transliteration System","Sina Ahmadi","Computation and Language (cs.CL)","In this article, we present a rule-based approach for transliterating two mostly used orthographies in Sorani Kurdish. Our work consists of detecting a character in a word by removing the possible ambiguities and mapping it into the target orthography. We describe different challenges in Kurdish text mining and propose novel ideas concerning the transliteration task for Sorani Kurdish. Our transliteration system, named Wergor, achieves 82.79% overall precision and more than 99% in detecting the double-usage characters. We also present a manually transliterated corpus for Kurdish.","Mon, 26 Nov 2018 10:37:05 UTC (539 KB)"
"60","A Big data analytical framework for portfolio optimization","Dhanya Jothimani, Ravi Shankar, Surendra S. Yadav","General Finance (q-fin.GN)","With the advent of Web 2.0, various types of data are being produced every day. This has led to the revolution of big data. Huge amount of structured and unstructured data are produced in financial markets. Processing these data could help an investor to make an informed investment decision. In this paper, a framework has been developed to incorporate both structured and unstructured data for portfolio optimization. Portfolio optimization consists of three processes: Asset selection, Asset weighting and Asset management. This framework proposes to achieve the first two processes using a 5-stage methodology. The stages include shortlisting stocks using Data Envelopment Analysis (DEA), incorporation of the qualitative factors using text mining, stock clustering, stock ranking and optimizing the portfolio using heuristics. This framework would help the investors to select appropriate assets to make portfolio, invest in them to minimize the risk and maximize the return and monitor their performance.","Sat, 17 Nov 2018 17:07:56 UTC (377 KB)[v2] Sat, 24 Nov 2018 16:42:27 UTC (378 KB)"
"61","Unsupervised Identification of Study Descriptors in Toxicology Research: An Experimental Study","Drahomira Herrmannova, Steven R. Young, Robert M. Patton, Christopher G. Stahl, Nicole C. Kleinstreuer, Mary S. Wolfe","Computation and Language (cs.CL)","Identifying and extracting data elements such as study descriptors in publication full texts is a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. Specifically, provided a set of criteria describing specific study parameters, such as species, route of administration, and dosing regimen, we develop an unsupervised approach to identify text segments (sentences) relevant to the criteria. A binary classifier trained to identify publications that met the criteria performs better when trained on the candidate sentences than when trained on sentences randomly picked from the text, supporting the intuition that our method is able to accurately identify study descriptors.","Sat, 3 Nov 2018 09:29:36 UTC (1,173 KB)"
"62","Biomedical Document Clustering and Visualization based on the Concepts of Diseases","Setu Shah, Xiao Luo","Computation and Language (cs.CL)","Document clustering is a text mining technique used to provide better document search and browsing in digital libraries or online corpora. A lot of research has been done on biomedical document clustering that is based on using existing ontology. But, associations and co-occurrences of the medical concepts are not well represented by using ontology. In this research, a vector representation of concepts of diseases and similarity measurement between concepts are proposed. They identify the closest concepts of diseases in the context of a corpus. Each document is represented by using the vector space model. A weight scheme is proposed to consider both local content and associations between concepts. A Self-Organizing Map is used as document clustering algorithm. The vector projection and visualization features of SOM enable visualization and analysis of the clusters distributions and relationships on the two dimensional space. The experimental results show that the proposed document clustering framework generates meaningful clusters and facilitate visualization of the clusters based on the concepts of diseases.","Mon, 22 Oct 2018 23:37:31 UTC (864 KB)"
"63","BioSentVec: creating sentence embeddings for biomedical texts","Qingyu Chen, Yifan Peng, Zhiyong Lu","Computation and Language (cs.CL)","Sentence embeddings have become an essential part of today's natural language processing (NLP) systems, especially together advanced deep learning methods. Although pre-trained sentence encoders are available in the general domain, none exists for biomedical texts to date. In this work, we introduce BioSentVec: the first open set of sentence embeddings trained with over 30 million documents from both scholarly articles in PubMed and clinical notes in the MIMIC-III Clinical Database. We evaluate BioSentVec embeddings in two sentence pair similarity tasks in different text genres. Our benchmarking results demonstrate that the BioSentVec embeddings can better capture sentence semantics compared to the other competitive alternatives and achieve state-of-the-art performance in both tasks. We expect BioSentVec to facilitate the research and development in biomedical text mining and to complement the existing resources in biomedical word embeddings. BioSentVec is publicly available at this https URL","Mon, 22 Oct 2018 14:10:01 UTC (7 KB)[v2] Fri, 26 Oct 2018 15:04:19 UTC (7 KB)[v3] Wed, 15 May 2019 04:41:06 UTC (262 KB)[v4] Tue, 18 Jun 2019 13:46:32 UTC (261 KB)[v5] Wed, 19 Jun 2019 00:33:32 UTC (261 KB)"
"64","Responsible team players wanted: an analysis of soft skill requirements in job advertisements","Federica Calanca, Luiza Sayfullina, Lara Minkus, Claudia Wagner, Eric Malmi","Computers and Society (cs.CY)","During the past decades the importance of soft skills for labour market outcomes has grown substantially. This carries implications for labour market inequality, since previous research shows that soft skills are not valued equally across race and gender. This work explores the role of soft skills in job advertisements by drawing on methods from computational science as well as on theoretical and empirical insights from economics, sociology and psychology. We present a semi-automatic approach based on crowdsourcing and text mining for extracting a list of soft skills. We find that soft skills are a crucial component of job ads, especially of low-paid jobs and jobs in female-dominated professions. Our work shows that soft skills can serve as partial predictors of the gender composition in job categories and that not all soft skills receive equal wage returns at the labour market. Especially ""female"" skills are frequently associated with wage penalties. Our results expand the growing literature on the association of soft skills on wage inequality and highlight their importance for occupational gender segregation at labour markets.","Sat, 13 Oct 2018 10:28:45 UTC (142 KB)[v2] Mon, 1 Apr 2019 18:28:18 UTC (347 KB)"
"65","Architecture of Text Mining Application in Analyzing Public Sentiments of West Java Governor Election using Naive Bayes Classification","Suryanto Nugroho, Prihandoko","Computers and Society (cs.CY)","The selection of West Java governor is one event that seizes the attention of the public is no exception to social media users. Public opinion on a prospective regional leader can help predict electability and tendency of voters. Data that can be used by the opinion mining process can be obtained from Twitter. Because the data is very varied form and very unstructured, it must be managed and uninformed using data pre-processing techniques into semi-structured data. This semi-structured information is followed by a classification stage to categorize the opinion into negative or positive opinions. The research methodology uses a literature study where the research will examine previous research on a similar topic. The purpose of this study is to find the right architecture to develop it into the application of twitter opinion mining to know public sentiments toward the election of the governor of west java. The result of this research is that Twitter opinion mining is part of text mining where opinions in Twitter if they want to be classified, must go through the preprocessing text stage first. The preprocessing step required from twitter data is cleansing, case folding, POS Tagging and stemming. The resulting text mining architecture is an architecture that can be used for text mining research with different topics.","Thu, 20 Sep 2018 07:14:10 UTC (510 KB)"
"66","Text-based Sentiment Analysis and Music Emotion Recognition","Erion Cano","Computation and Language (cs.CL)","Sentiment polarity of tweets, blog posts or product reviews has become highly attractive and is utilized in recommender systems, market predictions, business intelligence and more. Deep learning techniques are becoming top performers on analyzing such texts. There are however several problems that need to be solved for efficient use of deep neural networks on text mining and text polarity analysis. First, deep neural networks need to be fed with data sets that are big in size as well as properly labeled. Second, there are various uncertainties regarding the use of word embedding vectors: should they be generated from the same data set that is used to train the model or it is better to source them from big and popular collections? Third, to simplify model creation it is convenient to have generic neural network architectures that are effective and can adapt to various texts, encapsulating much of design complexity. This thesis addresses the above problems to provide methodological and practical insights for utilizing neural networks on sentiment analysis of texts and achieving state of the art results. Regarding the first problem, the effectiveness of various crowdsourcing alternatives is explored and two medium-sized and emotion-labeled song data sets are created utilizing social tags. To address the second problem, a series of experiments with large text collections of various contents and domains were conducted, trying word embeddings of various parameters. Regarding the third problem, a series of experiments involving convolution and max-pooling neural layers were conducted. Combining convolutions of words, bigrams, and trigrams with regional max-pooling layers in a couple of stacks produced the best results. The derived architecture achieves competitive performance on sentiment polarity analysis of movie, business and product reviews.","Sat, 6 Oct 2018 17:42:19 UTC (4,131 KB)"
"67","Sifaka: Text Mining Above a Search API","Cameron VandenBerg, Jamie Callan","Information Retrieval (cs.IR)","Text mining and analytics software has become popular, but little attention has been paid to the software architectures of such systems. Often they are built from scratch using special-purpose software and data structures, which increases their cost and complexity. This demo paper describes Sifaka, a new open-source text mining application constructed above a standard search engine index using existing application programmer interface (API) calls. Indexing integrates popular annotation software libraries to augment the full-text index with noun phrase and named-entities; n-grams are also provided. Sifaka enables a person to quickly explore and analyze large text collections using search, frequency analysis, and co-occurrence analysis; and import existing document labels or interactively construct document sets that are positive or negative examples of new concepts, perform feature selection, and export feature vectors compatible with popular machine learning software. Sifaka demonstrates that search engines are good platforms for text mining applications while also making common IR text mining capabilities accessible to researchers in disciplines where programming skills are less common.","Fri, 5 Oct 2018 23:24:19 UTC (205 KB)"
"68","Clust-LDA: Joint Model for Text Mining and Author Group Inference","Shaoyang Ning, Xi Qu, Victor Cai, Nathan Sanders","Information Retrieval (cs.IR)","Social media corpora pose unique challenges and opportunities, including typically short document lengths and rich meta-data such as author characteristics and relationships. This creates great potential for systematic analysis of the enormous body of the users and thus provides implications for industrial strategies such as targeted marketing. Here we propose a novel and statistically principled method, clust-LDA, which incorporates authorship structure into the topical modeling, thus accomplishing the task of the topical inferences across documents on the basis of authorship and, simultaneously, the identification of groupings between authors. We develop an inference procedure for clust-LDA and demonstrate its performance on simulated data, showing that clust-LDA out-performs the ""vanilla"" LDA on the topic identification task where authors exhibit distinctive topical preference. We also showcase the empirical performance of clust-LDA based on a real-world social media dataset from Reddit.","Fri, 5 Oct 2018 14:33:40 UTC (276 KB)"
"69","CollaboNet: collaboration of deep neural networks for biomedical named entity recognition","Wonjin Yoon, Chan Ho So, Jinhyuk Lee, Jaewoo Kang","Computation and Language (cs.CL)","Background: Finding biomedical named entities is one of the most essential tasks in biomedical text mining. Recently, deep learning-based approaches have been applied to biomedical named entity recognition (BioNER) and showed promising results. However, as deep learning approaches need an abundant amount of training data, a lack of data can hinder performance. BioNER datasets are scarce resources and each dataset covers only a small subset of entity types. Furthermore, many bio entities are polysemous, which is one of the major obstacles in named entity recognition. Results: To address the lack of data and the entity type misclassification problem, we propose CollaboNet which utilizes a combination of multiple NER models. In CollaboNet, models trained on a different dataset are connected to each other so that a target model obtains information from other collaborator models to reduce false positives. Every model is an expert on their target entity type and takes turns serving as a target and a collaborator model during training time. The experimental results show that CollaboNet can be used to greatly reduce the number of false positives and misclassified entities including polysemous words. CollaboNet achieved state-of-the-art performance in terms of precision, recall and F1 score. Conclusions: We demonstrated the benefits of combining multiple models for BioNER. Our model has successfully reduced the number of misclassified entities and improved the performance by leveraging multiple datasets annotated for different entity types. Given the state-of-the-art performance of our model, we believe that CollaboNet can improve the accuracy of downstream biomedical text mining applications such as bio-entity relation extraction.","Fri, 21 Sep 2018 05:48:54 UTC (261 KB)[v2] Wed, 29 May 2019 16:34:50 UTC (1,218 KB)"
"70","Supervised Machine Learning for Extractive Query Based Summarisation of Biomedical Data","Mandeep Kaur, Diego Molla","Computation and Language (cs.CL)","The automation of text summarisation of biomedical publications is a pressing need due to the plethora of information available on-line. This paper explores the impact of several supervised machine learning approaches for extracting multi-document summaries for given queries. In particular, we compare classification and regression approaches for query-based extractive summarisation using data provided by the BioASQ Challenge. We tackled the problem of annotating sentences for training classification systems and show that a simple annotation approach outperforms regression-based summarisation.","Fri, 14 Sep 2018 06:27:38 UTC (109 KB)[v2] Thu, 6 Dec 2018 06:38:57 UTC (109 KB)"
"71","Learning to Summarize Radiology Findings","Yuhao Zhang, Daisy Yi Ding, Tianpei Qian, Christopher D. Manning, Curtis P. Langlotz","Computation and Language (cs.CL)","The Impression section of a radiology report summarizes crucial radiology findings in natural language and plays a central role in communicating these findings to physicians. However, the process of generating impressions by summarizing findings is time-consuming for radiologists and prone to errors. We propose to automate the generation of radiology impressions with neural sequence-to-sequence learning. We further propose a customized neural model for this task which learns to encode the study background information and use this information to guide the decoding process. On a large dataset of radiology reports collected from actual hospital studies, our model outperforms existing non-neural and neural baselines under the ROUGE metrics. In a blind experiment, a board-certified radiologist indicated that 67% of sampled system summaries are at least as good as the corresponding human-written summaries, suggesting significant clinical validity. To our knowledge our work represents the first attempt in this direction.","Wed, 12 Sep 2018 22:41:47 UTC (709 KB)[v2] Mon, 8 Oct 2018 20:30:20 UTC (709 KB)"
"72","Deep learning for language understanding of mental health concepts derived from Cognitive Behavioural Therapy","Lina Rojas-Barahona, Bo-Hsiang Tseng, Yinpei Dai, Clare Mansfield, Osman Ramadan, Stefan Ultes, Michael Crawford, Milica Gasic","Computation and Language (cs.CL)","In recent years, we have seen deep learning and distributed representations of words and sentences make impact on a number of natural language processing tasks, such as similarity, entailment and sentiment analysis. Here we introduce a new task: understanding of mental health concepts derived from Cognitive Behavioural Therapy (CBT). We define a mental health ontology based on the CBT principles, annotate a large corpus where this phenomena is exhibited and perform understanding using deep learning and distributed representations. Our results show that the performance of deep learning models combined with word embeddings or sentence embeddings significantly outperform non-deep-learning models in this difficult task. This understanding module will be an essential component of a statistical dialogue system delivering therapy.","Mon, 3 Sep 2018 16:17:11 UTC (1,118 KB)"
"73","Total Recall, Language Processing, and Software Engineering","Zhe Yu, Tim Menzies","Software Engineering (cs.SE)","A broad class of software engineering problems can be generalized as the ""total recall problem"". This short paper claims that identifying and exploring total recall language processing problems in software engineering is an important task with wide applicability. To make that case, we show that by applying and adapting the state of the art active learning and text mining, solutions of the total recall problem, can help solve two important software engineering tasks: (a) supporting large literature reviews and (b) identifying software security vulnerabilities. Furthermore, we conjecture that (c) test case prioritization and (d) static warning identification can also be categorized as the total recall problem. The widespread applicability of ""total recall"" to software engineering suggests that there exists some underlying framework that encompasses not just natural language processing, but a wide range of important software engineering tasks.","Fri, 31 Aug 2018 19:37:20 UTC (201 KB)"
"74","Big Data Privacy Context: Literature Effects On Secure Informational Assets","Celina Rebello, Elaine Tavares","Computers and Society (cs.CY)","This article's objective is the identification of research opportunities in the current big data privacy domain, evaluating literature effects on secure informational assets. Until now, no study has analyzed such relation. Its results can foster science, technologies and businesses. To achieve these objectives, a big data privacy Systematic Literature Review (SLR) is performed on the main scientific peer reviewed journals in Scopus database. Bibliometrics and text mining analysis complement the SLR. This study provides support to big data privacy researchers on: most and least researched themes, research novelty, most cited works and authors, themes evolution through time and many others. In addition, TOPSIS and VIKOR ranks were developed to evaluate literature effects versus informational assets indicators. Secure Internet Servers (SIS) was chosen as decision criteria. Results show that big data privacy literature is strongly focused on computational aspects. However, individuals, societies, organizations and governments face a technological change that has just started to be investigated, with growing concerns on law and regulation aspects. TOPSIS and VIKOR Ranks differed in several positions and the only consistent country between literature and SIS adoption is the United States. Countries in the lowest ranking positions represent future research opportunities.","Sun, 26 Aug 2018 11:56:37 UTC (1,943 KB)"
"75","Comparing CNN and LSTM character-level embeddings in BiLSTM-CRF models for chemical and disease named entity recognition","Zenan Zhai, Dat Quoc Nguyen, Karin Verspoor","Computation and Language (cs.CL)","We compare the use of LSTM-based and CNN-based character-level word embeddings in BiLSTM-CRF models to approach chemical and disease named entity recognition (NER) tasks. Empirical results over the BioCreative V CDR corpus show that the use of either type of character-level word embeddings in conjunction with the BiLSTM-CRF models leads to comparable state-of-the-art performance. However, the models using CNN-based character-level word embeddings have a computational performance advantage, increasing training time over word-based models by 25% while the LSTM-based character-level word embeddings more than double the required training time.","Sat, 25 Aug 2018 17:02:29 UTC (106 KB)"
"76","Measuring LDA Topic Stability from Clusters of Replicated Runs","Mika Mantyla, Maelick Claes, Umar Farooq","Computation and Language (cs.CL)","Background: Unstructured and textual data is increasing rapidly and Latent Dirichlet Allocation (LDA) topic modeling is a popular data analysis methods for it. Past work suggests that instability of LDA topics may lead to systematic errors. Aim: We propose a method that relies on replicated LDA runs, clustering, and providing a stability metric for the topics. Method: We generate k LDA topics and replicate this process n times resulting in n*k topics. Then we use K-medioids to cluster the n*k topics to k clusters. The k clusters now represent the original LDA topics and we present them like normal LDA topics showing the ten most probable words. For the clusters, we try multiple stability metrics, out of which we recommend Rank-Biased Overlap, showing the stability of the topics inside the clusters. Results: We provide an initial validation where our method is used for 270,000 Mozilla Firefox commit messages with k=20 and n=20. We show how our topic stability metrics are related to the contents of the topics. Conclusions: Advances in text mining enable us to analyze large masses of text in software engineering but non-deterministic algorithms, such as LDA, may lead to unreplicable conclusions. Our approach makes LDA stability transparent and is also complementary rather than alternative to many prior works that focus on LDA parameter tuning.","Fri, 24 Aug 2018 11:37:40 UTC (475 KB)"
"77","Methodology for identifying study sites in scientific corpus","Eric Kergosien (GERIICO), Marie-Noelle Bessagnet (LIUPPA), Maguelonne Teisseire (UMR TETIS), Joachim Schopfel (GERIICO), Mohammad Amin Farvardin (LAMSADE), Stephane Chaudiron (GERIICO), Bernard Jacquemin (GERIICO), Annig Le Parc-Lacayrelle (LIUPPA), Mathieu Roche (UMR TETIS), Christian Sallaberry (LIUPPA), Jean-Philippe Tonneau (UMR TETIS), Marie-Noelle Bessagnet (LIUPPA), Amin Farvardin (UMR TETIS), Annig Lacayrelle","Information Retrieval (cs.IR)","The TERRE-ISTEX project aims at identifying the evolution of research working relation to study areas, disciplinary crossings and concrete research methods based on the heterogeneous digital content available in scientific corpora. The project is divided into three main actions: (1) to identify the periods and places which have been the subject of empirical studies, and which reflect the publications resulting from the corpus analyzed, (2) to identify the thematics addressed in these works and (3) to develop a web-based geographical information retrieval tool (GIR). The first two actions involve approaches combining Natural languages processing patterns with text mining methods. By crossing the three dimensions (spatial, thematic and temporal) in a GIR engine, it will be possible to understand what research has been carried out on which territories and at what time. In the project, the experiments are carried out on a heterogeneous corpus including electronic thesis and scientific articles from the ISTEX digital libraries and the CIRAD research center.","Mon, 13 Aug 2018 09:46:36 UTC (967 KB)"
"78","Evaluating Wikipedia as a source of information for disease understanding","Eduardo P. Garcia del Valle, Gerardo Lagunes Garcia, Lucia Prieto Santamaria, Massimiliano Zanin, Alejandro Rodriguez-Gonzalez, Ernestina Menasalvas Ruiz","Information Retrieval (cs.IR)","The increasing availability of biological data is improving our understanding of diseases and providing new insight into their underlying relationships. Thanks to the improvements on both text mining techniques and computational capacity, the combination of biological data with semantic information obtained from medical publications has proven to be a very promising path. However, the limitations in the access to these data and their lack of structure pose challenges to this approach. In this document we propose the use of Wikipedia - the free online encyclopedia - as a source of accessible textual information for disease understanding research. To check its validity, we compare its performance in the determination of relationships between diseases with that of PubMed, one of the most consulted data sources of medical texts. The obtained results suggest that the information extracted from Wikipedia is as relevant as that obtained from PubMed abstracts (i.e. the free access portion of its articles), although further research is proposed to verify its reliability for medical studies.","Sat, 4 Aug 2018 09:38:31 UTC (346 KB)"
"79","How Does Tweet Difficulty Affect Labeling Performance of Annotators?","Stefan Rabiger (1), Yucel Saygn (1), Myra Spiliopoulou (2) ((1) Sabanc University (Istanbul, Turkey), (2) Otto-von-Guericke University (Magdeburg, Germany))","Human-Computer Interaction (cs.HC)","Crowdsourcing is a popular means to obtain labeled data at moderate costs, for example for tweets, which can then be used in text mining tasks. To alleviate the problem of low-quality labels in this context, multiple human factors have been analyzed to identify and deal with workers who provide such labels. However, one aspect that has been rarely considered is the inherent difficulty of tweets to be labeled and how this affects the reliability of the labels that annotators assign to such tweets. Therefore, we investigate in this preliminary study this connection using a hierarchical sentiment labeling task on Twitter. We find that there is indeed a relationship between both factors, assuming that annotators have labeled some tweets before: labels assigned to easy tweets are more reliable than those assigned to difficult tweets. Therefore, training predictors on easy tweets enhances the performance by up to 6% in our experiment. This implies potential improvements for active learning techniques and crowdsourcing.","Wed, 1 Aug 2018 15:55:07 UTC (337 KB)"
"80","Clustering Prominent People and Organizations in Topic-Specific Text Corpora","Abdulkareem Alsudais, Hovig Tchalian","Computation and Language (cs.CL)","Named entities in text documents are the names of people, organization, location or other types of objects in the documents that exist in the real world. A persisting research challenge is to use computational techniques to identify such entities in text documents. Once identified, several text mining tools and algorithms can be utilized to leverage these discovered named entities and improve NLP applications. In this paper, a method that clusters prominent names of people and organizations based on their semantic similarity in a text corpus is proposed. The method relies on common named entity recognition techniques and on recent word embeddings models. The semantic similarity scores generated using the word embeddings models for the named entities are used to cluster similar entities of the people and organizations types. Two human judges evaluated ten variations of the method after it was run on a corpus that consists of 4,821 articles on a specific topic. The performance of the method was measured using three quantitative measures. The results of these three metrics demonstrate that the method is effective in clustering semantically similar named entities.","Fri, 27 Jul 2018 19:00:01 UTC (1,114 KB)[v2] Sun, 7 Jul 2019 11:43:55 UTC (380 KB)"
"81","Automatic Short Answer Grading and Feedback Using Text Mining Methods","Neslihan Suzen, Alexander Gorban, Jeremy Levesley, Evgeny Mirkes","Computation and Language (cs.CL)","Automatic grading is not a new approach but the need to adapt the latest technology to automatic grading has become very important. As the technology has rapidly became more powerful on scoring exams and essays, especially from the 1990s onwards, partially or wholly automated grading systems using computational methods have evolved and have become a major area of research. In particular, the demand of scoring of natural language responses has created a need for tools that can be applied to automatically grade these responses. In this paper, we focus on the concept of automatic grading of short answer questions such as are typical in the UK GCSE system, and providing useful feedback on their answers to students. We present experimental results on a dataset provided from the introductory computer science class in the University of North Texas. We first apply standard data mining techniques to the corpus of student answers for the purpose of measuring similarity between the student answers and the model answer. This is based on the number of common words. We then evaluate the relation between these similarities and marks awarded by scorers. We then consider an approach that groups student answers into clusters. Each cluster would be awarded the same mark, and the same feedback given to each answer in a cluster. In this manner, we demonstrate that clusters indicate the groups of students who are awarded the same or the similar scores. Words in each cluster are compared to show that clusters are constructed based on how many and which words of the model answer have been used. The main novelty in this paper is that we design a model to predict marks based on the similarities between the student answers and the model answer.","Fri, 27 Jul 2018 12:00:21 UTC (1,808 KB)[v2] Tue, 4 Jun 2019 17:21:46 UTC (1,920 KB)"
"82","Latent Dirichlet Allocation (LDA) for Topic Modeling of the CFPB Consumer Complaints","Kaveh Bastani, Hamed Namavari, Jeffry Shaffer","Information Retrieval (cs.IR)","A text mining approach is proposed based on latent Dirichlet allocation (LDA) to analyze the Consumer Financial Protection Bureau (CFPB) consumer complaints. The proposed approach aims to extract latent topics in the CFPB complaint narratives, and explores their associated trends over time. The time trends will then be used to evaluate the effectiveness of the CFPB regulations and expectations on financial institutions in creating a consumer oriented culture that treats consumers fairly and prioritizes consumer protection in their decision making processes. The proposed approach can be easily operationalized as a decision support system to automate detection of emerging topics in consumer complaints. Hence, the technology-human partnership between the proposed approach and the CFPB team could certainly improve consumer protections from unfair, deceptive or abusive practices in the financial markets by providing more efficient and effective investigations of consumer complaint narratives.","Wed, 18 Jul 2018 17:26:57 UTC (2,217 KB)"
"83","Making Efficient Use of a Domain Expert's Time in Relation Extraction","Linara Adilova, Sven Giesselbach, Stefan Ruping","Machine Learning (cs.LG)","Scarcity of labeled data is one of the most frequent problems faced in machine learning. This is particularly true in relation extraction in text mining, where large corpora of texts exists in many application domains, while labeling of text data requires an expert to invest much time to read the documents. Overall, state-of-the art models, like the convolutional neural network used in this paper, achieve great results when trained on large enough amounts of labeled data. However, from a practical point of view the question arises whether this is the most efficient approach when one takes the manual effort of the expert into account. In this paper, we report on an alternative approach where we first construct a relation extraction model using distant supervision, and only later make use of a domain expert to refine the results. Distant supervision provides a mean of labeling data given known relations in a knowledge base, but it suffers from noisy labeling. We introduce an active learning based extension, that allows our neural network to incorporate expert feedback and report on first results on a complex data set.","Thu, 12 Jul 2018 15:53:29 UTC (111 KB)"
"84","Natural Language Processing for Music Knowledge Discovery","Sergio Oramas, Luis Espinosa-Anke, Francisco Gomez, Xavier Serra","Computation and Language (cs.CL)","Today, a massive amount of musical knowledge is stored in written form, with testimonies dated as far back as several centuries ago. In this work, we present different Natural Language Processing (NLP) approaches to harness the potential of these text collections for automatic music knowledge discovery, covering different phases in a prototypical NLP pipeline, namely corpus compilation, text-mining, information extraction, knowledge graph generation and sentiment analysis. Each of these approaches is presented alongside different use cases (i.e., flamenco, Renaissance and popular music) where large collections of documents are processed, and conclusions stemming from data-driven analyses are presented and discussed.","Fri, 6 Jul 2018 00:07:27 UTC (2,982 KB)"
"85","Team assembly mechanisms and the knowledge produced in the Mexico's National Institute of Geriatrics: a network analysis and agent-based modelling approach","Carmen Garcia-Pena, Luis Miguel Gutierrez-Robledo, Augusto Cabrera-Becerril, David Fajardo-Ortiz","Digital Libraries (cs.DL)","Mexico's National Institute of Geriatrics (INGER) is the national research center of reference for matters related to human aging. INGER scientists perform basic, clinical and demographic research which may imply different scientific cultures working together in the same specialized institution. In this paper, by a combination of text mining, co-authorship network analysis and agent-based modeling we analyzed and modeled the team assembly practices and the structure of the knowledge produced by scientists from INGER. Our results showed a weak connection between basic and clinical research, and the emergence of a highly connected academic leadership. Importantly, basic and clinical-demographic researchers exhibited different team assembly strategies: Basic researchers tended to form larger teams mainly with external collaborators while clinical and demographic researchers formed smaller teams that very often incorporated internal (INGER) collaborators. We showed how these two different ways to form research teams impacted the organization of knowledge produced at INGER. Following these observations, we modeled, via agent-based modeling, the coexistence of different scientific cultures (basic and clinical research) exhibiting different team assembly strategies in the same institution. Our agent model successfully reproduced the current situation of INGER. Moreover, by modifying the values of homophily we obtain alternative scenarios in which multidisciplinary and interdisciplinary research could be done.","Thu, 21 Jun 2018 03:59:35 UTC (3,464 KB)[v2] Mon, 2 Jul 2018 21:48:03 UTC (1,403 KB)"
"86","Long-term stock index forecasting based on text mining of regulatory disclosures","Stefan Feuerriegel, Julius Gordon","Applications (stat.AP)","Share valuations are known to adjust to new information entering the market, such as regulatory disclosures. We study whether the language of such news items can improve short-term and especially long-term (24 months) forecasts of stock indices. For this purpose, this work utilizes predictive models suited to high-dimensional data and specifically compares techniques for data-driven and knowledge-driven dimensionality reduction in order to avoid overfitting. Our experiments, based on 75,927 ad hoc announcements from 1996-2016, reveal the following results: in the long run, text-based models succeed in reducing forecast errors below baseline predictions from historic lags at a statistically significant level. Our research provides implications to business applications of decision-support in financial markets, especially given the growing prevalence of index ETFs (exchange traded funds).","Tue, 26 Jun 2018 09:22:58 UTC (50 KB)"
"87","Paragraph-based complex networks: application to document classification and authenticity verification","Henrique F. de Arruda, Vanessa Q. Marinho, Luciano da F. Costa, Diego R. Amancio","Computation and Language (cs.CL)","With the increasing number of texts made available on the Internet, many applications have relied on text mining tools to tackle a diversity of problems. A relevant model to represent texts is the so-called word adjacency (co-occurrence) representation, which is known to capture mainly syntactical features of this http URL this study, we introduce a novel network representation that considers the semantic similarity between paragraphs. Two main properties of paragraph networks are considered: (i) their ability to incorporate characteristics that can discriminate real from artificial, shuffled manuscripts and (ii) their ability to capture syntactical and semantic textual features. Our results revealed that real texts are organized into communities, which turned out to be an important feature for discriminating them from artificial texts. Interestingly, we have also found that, differently from traditional co-occurrence networks, the adopted representation is able to capture semantic features. Additionally, the proposed framework was employed to analyze the Voynich manuscript, which was found to be compatible with texts written in natural languages. Taken together, our findings suggest that the proposed methodology can be combined with traditional network models to improve text classification tasks.","Fri, 22 Jun 2018 01:58:44 UTC (2,791 KB)"
"88","A Scalable Machine Learning Approach for Inferring Probabilistic US-LI-RADS Categorization","Imon Banerjee, Hailey H. Choi, Terry Desser, Daniel L. Rubin","Computation and Language (cs.CL)","We propose a scalable computerized approach for large-scale inference of Liver Imaging Reporting and Data System (LI-RADS) final assessment categories in narrative ultrasound (US) reports. Although our model was trained on reports created using a LI-RADS template, it was also able to infer LI-RADS scoring for unstructured reports that were created before the LI-RADS guidelines were established. No human-labelled data was required in any step of this study; for training, LI-RADS scores were automatically extracted from those reports that contained structured LI-RADS scores, and it translated the derived knowledge to reasoning on unstructured radiology reports. By providing automated LI-RADS categorization, our approach may enable standardizing screening recommendations and treatment planning of patients at risk for hepatocellular carcinoma, and it may facilitate AI-based healthcare research with US images by offering large scale text mining and data gathering opportunities from standard hospital clinical data repositories.","Fri, 15 Jun 2018 20:11:22 UTC (491 KB)"
"89","An Improved Text Sentiment Classification Model Using TF-IDF and Next Word Negation","Bijoyan Das, Sarit Chakraborty","Computation and Language (cs.CL)","With the rapid growth of Text sentiment analysis, the demand for automatic classification of electronic documents has increased by leaps and bound. The paradigm of text classification or text mining has been the subject of many research works in recent time. In this paper we propose a technique for text sentiment classification using term frequency- inverse document frequency (TF-IDF) along with Next Word Negation (NWN). We have also compared the performances of binary bag of words model, TF-IDF model and TF-IDF with next word negation (TF-IDF-NWN) model for text classification. Our proposed model is then applied on three different text mining algorithms and we found the Linear Support vector machine (LSVM) is the most appropriate to work with our proposed model. The achieved results show significant increase in accuracy compared to earlier methods.","Sun, 17 Jun 2018 16:25:57 UTC (635 KB)"
"90","Gender Prediction in English-Hindi Code-Mixed Social Media Content : Corpus and Baseline System","Ankush Khandelwal, Sahil Swami, Syed Sarfaraz Akhtar, Manish Shrivastava","Computation and Language (cs.CL)","The rapid expansion in the usage of social media networking sites leads to a huge amount of unprocessed user generated data which can be used for text mining. Author profiling is the problem of automatically determining profiling aspects like the author's gender and age group through a text is gaining much popularity in computational linguistics. Most of the past research in author profiling is concentrated on English texts \cite{1,2}. However many users often change the language while posting on social media which is called code-mixing, and it develops some challenges in the field of text classification and author profiling like variations in spelling, non-grammatical structure and transliteration \cite{3}. There are very few English-Hindi code-mixed annotated datasets of social media content present online \cite{4}. In this paper, we analyze the task of author's gender prediction in code-mixed content and present a corpus of English-Hindi texts collected from Twitter which is annotated with author's gender. We also explore language identification of every word in this corpus. We present a supervised classification baseline system which uses various machine learning algorithms to identify the gender of an author using a text, based on character and word level features.","Thu, 14 Jun 2018 15:08:22 UTC (118 KB)"
"91","Automatic Identification of Research Fields in Scientific Papers","Eric Kergosien (GERIICO), Amin Farvardin (UMR TETIS), Maguelonne Teisseire (UMR TETIS), Marie-Noelle Bessagnet (LIUPPA), Joachim Schopfel (GERIICO), Stephane Chaudiron (GERIICO), Bernard Jacquemin (GERIICO), Annig Le Parc-Lacayrelle (LIUPPA), Mathieu Roche (UMR TETIS), Christian Sallaberry (LIUPPA), Jean-Philippe Tonneau (UMR TETIS)","Information Retrieval (cs.IR)","The TERRE-ISTEX project aims to identify scientific research dealing with specific geographical territories areas based on heterogeneous digital content available in scientific papers. The project is divided into three main work packages: (1) identification of the periods and places of empirical studies, and which reflect the publications resulting from the analyzed text samples, (2) identification of the themes which appear in these documents, and (3) development of a web-based geographical information retrieval tool (GIR). The first two actions combine Natural Language Processing patterns with text mining methods. The integration of the spatial, thematic and temporal dimensions in a GIR contributes to a better understanding of what kind of research has been carried out, of its topics and its geographical and historical coverage. Another originality of the TERRE-ISTEX project is the heterogeneous character of the corpus, including PhD theses and scientific articles from the ISTEX digital libraries and the CIRAD research center.","Fri, 8 Jun 2018 13:29:46 UTC (1,523 KB)"
"92","An unsupervised and customizable misspelling generator for mining noisy health-related text sources","Abeed Sarker, Graciela Gonzalez-Hernandez","Computation and Language (cs.CL)","In this paper, we present a customizable datacentric system that automatically generates common misspellings for complex health-related terms. The spelling variant generator relies on a dense vector model learned from large unlabeled text, which is used to find semantically close terms to the original/seed keyword, followed by the filtering of terms that are lexically dissimilar beyond a given threshold. The process is executed recursively, converging when no new terms similar (lexically and semantically) to the seed keyword are found. Weighting of intra-word character sequence similarities allows further problem-specific customization of the system. On a dataset prepared for this study, our system outperforms the current state-of-the-art for medication name variant generation with best F1-score of 0.69 and F1/4-score of 0.78. Extrinsic evaluation of the system on a set of cancer-related terms showed an increase of over 67% in retrieval rate from Twitter posts when the generated variants are included. Our proposed spelling variant generator has several advantages over the current state-of-the-art and other types of variant generators-(i) it is capable of filtering out lexically similar but semantically dissimilar terms, (ii) the number of variants generated is low as many low-frequency and ambiguous misspellings are filtered out, and (iii) the system is fully automatic, customizable and easily executable. While the base system is fully unsupervised, we show how supervision maybe employed to adjust weights for task-specific customization. The performance and significant relative simplicity of our proposed approach makes it a much needed misspelling generation resource for health-related text mining from noisy sources. The source code for the system has been made publicly available for research purposes.","Mon, 4 Jun 2018 01:07:37 UTC (400 KB)"
"93","iLCM - A Virtual Research Infrastructure for Large-Scale Qualitative Data","Andreas Niekler, Arnim Bleier, Christian Kahmann, Lisa Posch, Gregor Wiedemann, Kenan Erdogan, Gerhard Heyer, Markus Strohmaier","Information Retrieval (cs.IR)","The iLCM project pursues the development of an integrated research environment for the analysis of structured and unstructured data in a ""Software as a Service"" architecture (SaaS). The research environment addresses requirements for the quantitative evaluation of large amounts of qualitative data with text mining methods as well as requirements for the reproducibility of data-driven research designs in the social sciences. For this, the iLCM research environment comprises two central components. First, the Leipzig Corpus Miner (LCM), a decentralized SaaS application for the analysis of large amounts of news texts developed in a previous Digital Humanities project. Second, the text mining tools implemented in the LCM are extended by an ""Open Research Computing"" (ORC) environment for executable script documents, so-called ""notebooks"". This novel integration allows to combine generic, high-performance methods to process large amounts of unstructured text data and with individual program scripts to address specific research requirements in computational social science and digital humanities.","Fri, 11 May 2018 10:24:11 UTC (1,296 KB)"
"94","Core Conflictual Relationship: Text Mining to Discover What and When","Fionn Murtagh, Giuseppe Iurato","Computation and Language (cs.CL)","Following detailed presentation of the Core Conflictual Relationship Theme (CCRT), there is the objective of relevant methods for what has been described as verbalization and visualization of data. Such is also termed data mining and text mining, and knowledge discovery in data. The Correspondence Analysis methodology, also termed Geometric Data Analysis, is shown in a case study to be comprehensive and revealing. Computational efficiency depends on how the analysis process is structured. For both illustrative and revealing aspects of the case study here, relatively extensive dream reports are used. This Geometric Data Analysis confirms the validity of CCRT method.","Mon, 28 May 2018 19:21:59 UTC (106 KB)"
"95","Effects of Social Bots in the Iran-Debate on Twitter","Andree Thieltges, Orestis Papakyriakopoulos, Juan Carlos Medina Serrano, Simon Hegelich","Social and Information Networks (cs.SI)","2018 started with massive protests in Iran, bringing back the impressions of the so called ""Arab Spring"" and it's revolutionary impact for the Maghreb states, Syria and Egypt. Many reports and scientific examinations considered online social networks (OSN's) such as Twitter or Facebook to play a critical role in the opinion making of people behind those protests. Beside that, there is also evidence for directed manipulation of opinion with the help of social bots and fake accounts. So, it is obvious to ask, if there is an attempt to manipulate the opinion-making process related to the Iranian protest in OSN by employing social bots, and how such manipulations will affect the discourse as a whole. Based on a sample of ca. 900,000 Tweets relating to the topic ""Iran"" we show, that there are Twitter profiles, that have to be considered as social bot accounts. By using text mining methods, we show that these social bots are responsible for negative sentiment in the debate. Thereby, we would like to illustrate a detectable effect of social bots on political discussions on Twitter.","Fri, 25 May 2018 12:33:14 UTC (2,208 KB)"
"96","CLINIQA: A Machine Intelligence Based Clinical Question Answering System","M A H Zahid, Ankush Mittal, R.C. Joshi, G. Atluri","Computation and Language (cs.CL)","The recent developments in the field of biomedicine have made large volumes of biomedical literature available to the medical practitioners. Due to the large size and lack of efficient searching strategies, medical practitioners struggle to obtain necessary information available in the biomedical literature. Moreover, the most sophisticated search engines of age are not intelligent enough to interpret the clinicians' questions. These facts reflect the urgent need of an information retrieval system that accepts the queries from medical practitioners' in natural language and returns the answers quickly and efficiently. In this paper, we present an implementation of a machine intelligence based CLINIcal Question Answering system (CLINIQA) to answer medical practitioner's questions. The system was rigorously evaluated on different text mining algorithms and the best components for the system were selected. The system makes use of Unified Medical Language System for semantic analysis of both questions and medical documents. In addition, the system employs supervised machine learning algorithms for classification of the documents, identifying the focus of the question and answer selection. Effective domain-specific heuristics are designed for answer ranking. The performance evaluation on hundred clinical questions shows the effectiveness of our approach.","Tue, 15 May 2018 17:45:25 UTC (252 KB)"
"97","Towards Autonomous Reinforcement Learning: Automatic Setting of Hyper-parameters using Bayesian Optimization","Juan Cruz Barsce, Jorge A. Palombarini, Ernesto C. Martinez","Artificial Intelligence (cs.AI)","With the increase of machine learning usage by industries and scientific communities in a variety of tasks such as text mining, image recognition and self-driving cars, automatic setting of hyper-parameter in learning algorithms is a key factor for achieving satisfactory performance regardless of user expertise in the inner workings of the techniques and methodologies. In particular, for a reinforcement learning algorithm, the efficiency of an agent learning a control policy in an uncertain environment is heavily dependent on the hyper-parameters used to balance exploration with exploitation. In this work, an autonomous learning framework that integrates Bayesian optimization with Gaussian process regression to optimize the hyper-parameters of a reinforcement learning algorithm, is proposed. Also, a bandits-based approach to achieve a balance between computational costs and decreasing uncertainty about the Q-values, is presented. A gridworld example is used to highlight how hyper-parameter configurations of a learning algorithm (SARSA) are iteratively improved based on two performance functions.","Sat, 12 May 2018 16:42:55 UTC (504 KB)"
"98","Text-mining and ontologies: new approaches to knowledge discovery of microbial diversity","Claire Nedellec, Robert Bossy, Estelle Chaix, Louise Deleger","Quantitative Methods (q-bio.QM)","Microbiology research has access to a very large amount of public information on the habitats of microorganisms. Many areas of microbiology research uses this information, primarily in biodiversity studies. However the habitat information is expressed in unstructured natural language form, which hinders its exploitation at large-scale. It is very common for similar habitats to be described by different terms, which makes them hard to compare automatically, e.g. intestine and gut. The use of a common reference to standardize these habitat descriptions as claimed by (Ivana et al., 2010) is a necessity. We propose the ontology called OntoBiotope that we have been developing since 2010. The OntoBiotope ontology is in a formal machine-readable representation that enables indexing of information as well as conceptualization and reasoning.","Thu, 10 May 2018 11:38:45 UTC (242 KB)[v2] Wed, 31 Oct 2018 14:52:02 UTC (261 KB)"
"99","Semi-orthogonal Non-negative Matrix Factorization with an Application in Text Mining","Jack Yutong Li, Ruoqing Zhu, Annie Qu, Han Ye, Zhankun Sun","Methodology (stat.ME)","Emergency Department (ED) crowding is a worldwide issue that affects the efficiency of hospital management and the quality of patient care. This occurs when the request for an admit ward-bed to receive a patient is delayed until an admission decision is made by a doctor. To reduce the overcrowding and waiting time of ED, we build a classifier to predict the disposition of patients using manually-typed nurse notes collected during triage, thereby allowing hospital staff to begin necessary preparation beforehand. However, these triage notes involve high dimensional, noisy, and also sparse text data which makes model fitting and interpretation difficult. To address this issue, we propose the semi-orthogonal non-negative matrix factorization (SONMF) for both continuous and binary design matrices to first bi-cluster the patients and words into a reduced number of topics. The subjects can then be interpreted as a non-subtractive linear combination of orthogonal basis topic vectors. These generated topic vectors provide the hospital with a direct understanding of the cause of admission. We show that by using a transformation of basis, the classification accuracy can be further increased compared to the conventional bag-of-words model and alternative matrix factorization approaches. Through simulated data experiments, we also demonstrate that the proposed method outperforms other non-negative matrix factorization (NMF) methods in terms of factorization accuracy, rate of convergence, and degree of orthogonality.","Mon, 7 May 2018 01:12:12 UTC (776 KB)[v2] Mon, 5 Nov 2018 19:12:37 UTC (1,277 KB)[v3] Thu, 4 Jul 2019 15:46:21 UTC (3,223 KB)"
"100","SynTF: Synthetic and Differentially Private Term Frequency Vectors for Privacy-Preserving Text Mining","Benjamin Weggenmann, Florian Kerschbaum","Cryptography and Security (cs.CR)","Text mining and information retrieval techniques have been developed to assist us with analyzing, organizing and retrieving documents with the help of computers. In many cases, it is desirable that the authors of such documents remain anonymous: Search logs can reveal sensitive details about a user, critical articles or messages about a company or government might have severe or fatal consequences for a critic, and negative feedback in customer surveys might negatively impact business relations if they are identified. Simply removing personally identifying information from a document is, however, insufficient to protect the writer's identity: Given some reference texts of suspect authors, so-called authorship attribution methods can reidentfy the author from the text itself. One of the most prominent models to represent documents in many common text mining and information retrieval tasks is the vector space model where each document is represented as a vector, typically containing its term frequencies or related quantities. We therefore propose an automated text anonymization approach that produces synthetic term frequency vectors for the input documents that can be used in lieu of the original vectors. We evaluate our method on an exemplary text classification task and demonstrate that it only has a low impact on its accuracy. In contrast, we show that our method strongly affects authorship attribution techniques to the level that they become infeasible with a much stronger decline in accuracy. Other than previous authorship obfuscation methods, our approach is the first that fulfills differential privacy and hence comes with a provable plausible deniability guarantee.","Wed, 2 May 2018 16:55:38 UTC (238 KB)"
"101","Hyperparameter Optimization for Effort Estimation","Tianpei Xia, Rahul Krishna, Jianfeng Chen, George Mathew, Xipeng Shen, Tim Menzies","Software Engineering (cs.SE)","Software analytics has been widely used in software engineering for many tasks such as generating effort estimates for software projects. One of the ""black arts"" of software analytics is tuning the parameters controlling a data mining algorithm. Such hyperparameter optimization has been widely studied in other software analytics domains (e.g. defect prediction and text mining) but, so far, has not been extensively explored for effort estimation. Accordingly, this paper seeks simple, automatic, effective and fast methods for finding good tunings for automatic software effort estimation. We introduce a hyperparameter optimization architecture called OIL (Optimized Inductive Learning). We test OIL on a wide range of hyperparameter optimizers using data from 945 software projects. After tuning, large improvements in effort estimation accuracy were observed (measured in terms of standardized accuracy). From those results, we recommend using regression trees (CART) tuned by different evolution combine with default analogy-based estimator. This particular combination of learner and optimizers often achieves in a few hours what other optimizers need days to weeks of CPU time to accomplish. An important part of this analysis is its reproducibility and refutability. All our scripts and data are on-line. It is hoped that this paper will prompt and enable much more research on better methods to tune software effort estimators.","Sat, 28 Apr 2018 02:51:43 UTC (467 KB)[v2] Thu, 16 Aug 2018 18:33:49 UTC (264 KB)[v3] Tue, 9 Oct 2018 17:36:18 UTC (264 KB)[v4] Thu, 31 Jan 2019 20:31:32 UTC (152 KB)"
"102","Can You Explain That, Better? Comprehensible Text Analytics for SE Applications","Amritanshu Agrawal, Huy Tu, Tim Menzies","Software Engineering (cs.SE)","Text mining methods are used for a wide range of Software Engineering (SE) tasks. The biggest challenge of text mining is high dimensional data, i.e., a corpus of documents can contain $10^4$ to $10^6$ unique words. To address this complexity, some very convoluted text mining methods have been applied. Is that complexity necessary? Are there simpler ways to quickly generate models that perform as well as the more convoluted methods and also be human-readable? To answer these questions, we explore a combination of LDA (Latent Dirichlet Allocation) and FFTs (Fast and Frugal Trees) to classify NASA software bug reports from six different projects. Designed using principles from psychological science, FFTs return very small models that are human-comprehensible. When compared to the commonly used text mining method and a recent state-of-the-art-system (search-based SE method that automatically tune the control parameters of LDA), these FFT models are very small (a binary tree of depth $d = 4$ that references only 4 topics) and hence easy to understand. They were also faster to generate and produced similar or better severity predictions. Hence we can conclude that, at least for datasets explored here, convoluted text mining models can be deprecated in favor of simpler method such as LDA+FFTs. At the very least, we recommend LDA+FFTs (a) when humans need to read, understand, and audit a model or (b) as an initial baseline method for the SE researchers exploring text artifacts from software projects.","Fri, 27 Apr 2018 19:20:39 UTC (2,066 KB)"
"103","Label-aware Double Transfer Learning for Cross-Specialty Medical Named Entity Recognition","Zhenghui Wang, Yanru Qu, Liheng Chen, Jian Shen, Weinan Zhang, Shaodian Zhang, Yimei Gao, Gen Gu, Ken Chen, Yong Yu","Computation and Language (cs.CL)","We study the problem of named entity recognition (NER) from electronic medical records, which is one of the most fundamental and critical problems for medical text mining. Medical records which are written by clinicians from different specialties usually contain quite different terminologies and writing styles. The difference of specialties and the cost of human annotation makes it particularly difficult to train a universal medical NER system. In this paper, we propose a label-aware double transfer learning framework (La-DTL) for cross-specialty NER, so that a medical NER system designed for one specialty could be conveniently applied to another one with minimal annotation efforts. The transferability is guaranteed by two components: (i) we propose label-aware MMD for feature representation transfer, and (ii) we perform parameter transfer with a theoretical upper bound which is also label aware. We conduct extensive experiments on 12 cross-specialty NER tasks. The experimental results demonstrate that La-DTL provides consistent accuracy improvement over strong baselines. Besides, the promising experimental results on non-medical NER scenarios indicate that La-DTL is potential to be seamlessly adapted to a wide range of NER tasks.","Tue, 24 Apr 2018 13:35:11 UTC (156 KB)[v2] Sat, 28 Apr 2018 09:46:39 UTC (144 KB)"
"104","PMC text mining subset in BioC: 2.3 million full text articles and growing","Donald C. Comeau, Chih-Hsuan Wei, Rezarta Islamaj Do<U+011F>an, Zhiyong Lu","Digital Libraries (cs.DL)","Interest in full text mining biomedical research articles is growing. NCBI provides the PMC Open Access and Author Manuscript sets of articles which are available for text mining. We have made all of these articles available in BioC, an XML and JSON format which is convenient for sharing text, annotations, and relations. These articles are available both via ftp for bulk download and via a Web API for updates or more focused collection. Availability: this https URL","Mon, 16 Apr 2018 21:55:28 UTC (427 KB)"
"105","Predicting Good Configurations for GitHub and Stack Overflow Topic Models","Christoph Treude, Markus Wagner","Computation and Language (cs.CL)","Software repositories contain large amounts of textual data, ranging from source code comments and issue descriptions to questions, answers, and comments on Stack Overflow. To make sense of this textual data, topic modelling is frequently used as a text-mining tool for the discovery of hidden semantic structures in text bodies. Latent Dirichlet allocation (LDA) is a commonly used topic model that aims to explain the structure of a corpus by grouping texts. LDA requires multiple parameters to work well, and there are only rough and sometimes conflicting guidelines available on how these parameters should be set. In this paper, we contribute (i) a broad study of parameters to arrive at good local optima for GitHub and Stack Overflow text corpora, (ii) an a-posteriori characterisation of text corpora related to eight programming languages, and (iii) an analysis of corpus feature importance via per-corpus LDA configuration. We find that (1) popular rules of thumb for topic modelling parameter configuration are not applicable to the corpora used in our experiments, (2) corpora sampled from GitHub and Stack Overflow have different characteristics and require different configurations to achieve good model fit, and (3) we can predict good configurations for unseen corpora reliably. These findings support researchers and practitioners in efficiently determining suitable configurations for topic modelling when analysing textual data contained in software repositories.","Fri, 13 Apr 2018 00:09:48 UTC (5,536 KB)[v2] Sat, 23 Jun 2018 11:30:52 UTC (5,511 KB)[v3] Sun, 10 Mar 2019 07:38:41 UTC (5,548 KB)"
"106","$$-hot Lexicon Embedding-based Two-level LSTM for Sentiment Analysis","Ou Wu, Tao Yang, Mengyang Li, Ming Li","Computation and Language (cs.CL)","Sentiment analysis is a key component in various text mining applications. Numerous sentiment classification techniques, including conventional and deep learning-based methods, have been proposed in the literature. In most existing methods, a high-quality training set is assumed to be given. Nevertheless, constructing a high-quality training set that consists of highly accurate labels is challenging in real applications. This difficulty stems from the fact that text samples usually contain complex sentiment representations, and their annotation is subjective. We address this challenge in this study by leveraging a new labeling strategy and utilizing a two-level long short-term memory network to construct a sentiment classifier. Lexical cues are useful for sentiment analysis, and they have been utilized in conventional studies. For example, polar and privative words play important roles in sentiment analysis. A new encoding strategy, that is, $\rho$-hot encoding, is proposed to alleviate the drawbacks of one-hot encoding and thus effectively incorporate useful lexical cues. We compile three Chinese data sets on the basis of our label strategy and proposed methodology. Experiments on the three data sets demonstrate that the proposed method outperforms state-of-the-art algorithms.","Wed, 21 Mar 2018 07:13:16 UTC (2,187 KB)"
"107","Natural Language or Not (NLoN) - A Package for Software Engineering Text Analysis Pipeline","Mika V. Mantyla, Fabio Calefato, Maelick Claes","Software Engineering (cs.SE)","The use of natural language processing (NLP) is gaining popularity in software engineering. In order to correctly perform NLP, we must pre-process the textual information to separate natural language from other information, such as log messages, that are often part of the communication in software engineering. We present a simple approach for classifying whether some textual input is natural language or not. Although our NLoN package relies on only 11 language features and character tri-grams, we are able to achieve an area under the ROC curve performances between 0.976-0.987 on three different data sources, with Lasso regression from Glmnet as our learner and two human raters for providing ground truth. Cross-source prediction performance is lower and has more fluctuation with top ROC performances from 0.913 to 0.980. Compared with prior work, our approach offers similar performance but is considerably more lightweight, making it easier to apply in software engineering text mining pipelines. Our source code and data are provided as an R-package for further improvements.","Tue, 20 Mar 2018 08:32:56 UTC (14 KB)"
"108","How to evaluate sentiment classifiers for Twitter time-ordered data?","Igor Mozeti<U+010D>, Luis Torgo, Vitor Cerqueira, Jasmina Smailovi<U+0107>","Computation and Language (cs.CL)","Social media are becoming an increasingly important source of information about the public mood regarding issues such as elections, Brexit, stock market, etc. In this paper we focus on sentiment classification of Twitter data. Construction of sentiment classifiers is a standard text mining task, but here we address the question of how to properly evaluate them as there is no settled way to do so. Sentiment classes are ordered and unbalanced, and Twitter produces a stream of time-ordered data. The problem we address concerns the procedures used to obtain reliable estimates of performance measures, and whether the temporal ordering of the training and test data matters. We collected a large set of 1.5 million tweets in 13 European languages. We created 138 sentiment models and out-of-sample datasets, which are used as a gold standard for evaluations. The corresponding 138 in-sample datasets are used to empirically compare six different estimation procedures: three variants of cross-validation, and three variants of sequential validation (where test set always follows the training set). We find no significant difference between the best cross-validation and sequential validation. However, we observe that all cross-validation variants tend to overestimate the performance, while the sequential methods tend to underestimate it. Standard cross-validation with random selection of examples is significantly worse than the blocked cross-validation, and should not be used to evaluate classifiers in time-ordered data scenarios.","Wed, 14 Mar 2018 08:16:48 UTC (249 KB)"
"109","IDEL: In-Database Entity Linking with Neural Embeddings","Torsten Kilias, Alexander Loser, Felix A. Gers, Richard Koopmanschap, Ying Zhang, Martin Kersten","Databases (cs.DB)","We present a novel architecture, In-Database Entity Linking (IDEL), in which we integrate the analytics-optimized RDBMS MonetDB with neural text mining abilities. Our system design abstracts core tasks of most neural entity linking systems for MonetDB. To the best of our knowledge, this is the first defacto implemented system integrating entity-linking in a database. We leverage the ability of MonetDB to support in-database-analytics with user defined functions (UDFs) implemented in Python. These functions call machine learning libraries for neural text mining, such as TensorFlow. The system achieves zero cost for data shipping and transformation by utilizing MonetDB's ability to embed Python processes in the database kernel and exchange data in NumPy arrays. IDEL represents text and relational data in a joint vector space with neural embeddings and can compensate errors with ambiguous entity representations. For detecting matching entities, we propose a novel similarity function based on joint neural embeddings which are learned via minimizing pairwise contrastive ranking loss. This function utilizes a high dimensional index structures for fast retrieval of matching entities. Our first implementation and experiments using the WebNLG corpus show the effectiveness and the potentials of IDEL.","Tue, 13 Mar 2018 15:35:42 UTC (2,302 KB)"
"110","Poisson Kernel-Based Clustering on the Sphere: Convergence Properties, Identifiability, and a Method of Sampling","Mojgan Golzy, Marianthi Markatou","Methodology (stat.ME)","Many applications of interest involve data that can be analyzed as unit vectors on a d-dimensional sphere. Specific examples include text mining, in particular clustering of documents, biology, astronomy and medicine among others. Previous work has proposed a clustering method using mixtures of Poisson kernel-based distributions (PKBD) on the sphere. We prove identifiability of mixtures of the aforementioned model, convergence of the associated EM-type algorithm and study its operational characteristics. Furthermore, we propose an empirical densities distance plot for estimating the number of clusters in a PKBD model. Finally, we propose a method to simulate data from Poisson kernel-based densities and exemplify our methods via application on real data sets and simulation experiments.","Mon, 12 Mar 2018 19:39:37 UTC (1,581 KB)"
"111","Sentiment Analysis on Speaker Specific Speech Data","Maghilnan S, Rajesh Kumar M","Computation and Language (cs.CL)","Sentiment analysis has evolved over past few decades, most of the work in it revolved around textual sentiment analysis with text mining techniques. But audio sentiment analysis is still in a nascent stage in the research community. In this proposed research, we perform sentiment analysis on speaker discriminated speech transcripts to detect the emotions of the individual speakers involved in the conversation. We analyzed different techniques to perform speaker discrimination and sentiment analysis to find efficient algorithms to perform this task.","Sat, 17 Feb 2018 08:39:47 UTC (354 KB)"
"112","500+ Times Faster Than Deep Learning (A Case Study Exploring Faster Methods for Text Mining StackOverflow)","Suvodeep Majumder, Nikhila Balaji, Katie Brey, Wei Fu, Tim Menzies","Software Engineering (cs.SE)","Deep learning methods are useful for high-dimensional data and are becoming widely used in many areas of software engineering. Deep learners utilizes extensive computational power and can take a long time to train-- making it difficult to widely validate and repeat and improve their results. Further, they are not the best solution in all domains. For example, recent results show that for finding related Stack Overflow posts, a tuned SVM performs similarly to a deep learner, but is significantly faster to train. This paper extends that recent result by clustering the dataset, then tuning very learners within each cluster. This approach is over 500 times faster than deep learning (and over 900 times faster if we use all the cores on a standard laptop computer). Significantly, this faster approach generates classifiers nearly as good (within 2\% F1 Score) as the much slower deep learning method. Hence we recommend this faster methods since it is much easier to reproduce and utilizes far fewer CPU resources. More generally, we recommend that before researchers release research results, that they compare their supposedly sophisticated methods against simpler alternatives (e.g applying simpler learners to build local models).","Wed, 14 Feb 2018 20:57:48 UTC (231 KB)"
"113","Robust Continuous Co-Clustering","Xiao He, Luis Moreira-Matias","Machine Learning (cs.LG)","Clustering consists of grouping together samples giving their similar properties. The problem of modeling simultaneously groups of samples and features is known as Co-Clustering. This paper introduces ROCCO - a Robust Continuous Co-Clustering algorithm. ROCCO is a scalable, hyperparameter-free, easy and ready to use algorithm to address Co-Clustering problems in practice over massive cross-domain datasets. It operates by learning a graph-based two-sided representation of the input matrix. The underlying proposed optimization problem is non-convex, which assures a flexible pool of solutions. Moreover, we prove that it can be solved with a near linear time complexity on the input size. An exhaustive large-scale experimental testbed conducted with both synthetic and real-world datasets demonstrates ROCCO's properties in practice: (i) State-of-the-art performance in cross-domain real-world problems including Biomedicine and Text Mining; (ii) very low sensitivity to hyperparameter settings; (iii) robustness to noise and (iv) a linear empirical scalability in practice. These results highlight ROCCO as a powerful general-purpose co-clustering algorithm for cross-domain practitioners, regardless of their technical background.","Wed, 14 Feb 2018 11:07:16 UTC (1,318 KB)"
"114","Mining Public Opinion about Economic Issues: Twitter and the U.S. Presidential Election","Amir Karami, London S. Bennett, Xiaoyun He","Social and Information Networks (cs.SI)","Opinion polls have been the bridge between public opinion and politicians in elections. However, developing surveys to disclose people's feedback with respect to economic issues is limited, expensive, and time-consuming. In recent years, social media such as Twitter has enabled people to share their opinions regarding elections. Social media has provided a platform for collecting a large amount of social media data. This paper proposes a computational public opinion mining approach to explore the discussion of economic issues in social media during an election. Current related studies use text mining methods independently for election analysis and election prediction; this research combines two text mining methods: sentiment analysis and topic modeling. The proposed approach has effectively been deployed on millions of tweets to analyze economic concerns of people during the 2012 US presidential election.","Tue, 6 Feb 2018 03:55:37 UTC (451 KB)"
"115","Chemical-protein relation extraction with ensembles of SVM, CNN, and RNN models","Yifan Peng, Anthony Rios, Ramakanth Kavuluru, Zhiyong Lu","Computation and Language (cs.CL)","Text mining the relations between chemicals and proteins is an increasingly important task. The CHEMPROT track at BioCreative VI aims to promote the development and evaluation of systems that can automatically detect the chemical-protein relations in running text (PubMed abstracts). This manuscript describes our submission, which is an ensemble of three systems, including a Support Vector Machine, a Convolutional Neural Network, and a Recurrent Neural Network. Their output is combined using a decision based on majority voting or stacking. Our CHEMPROT system obtained 0.7266 in precision and 0.5735 in recall for an f-score of 0.6410, demonstrating the effectiveness of machine learning-based approaches for automatic relation extraction from biomedical literature. Our submission achieved the highest performance in the task during the 2017 challenge.","Mon, 5 Feb 2018 03:42:36 UTC (497 KB)"
"116","Preparation of Improved Turkish DataSet for Sentiment Analysis in Social Media","Semiha Makinist, Ibrahim Riza Hallac, Betul Ay Karakus, Galip Aydin","Computation and Language (cs.CL)","A public dataset, with a variety of properties suitable for sentiment analysis [1], event prediction, trend detection and other text mining applications, is needed in order to be able to successfully perform analysis studies. The vast majority of data on social media is text-based and it is not possible to directly apply machine learning processes into these raw data, since several different processes are required to prepare the data before the implementation of the algorithms. For example, different misspellings of same word enlarge the word vector space unnecessarily, thereby it leads to reduce the success of the algorithm and increase the computational power requirement. This paper presents an improved Turkish dataset with an effective spelling correction algorithm based on Hadoop [2]. The collected data is recorded on the Hadoop Distributed File System and the text based data is processed by MapReduce programming model. This method is suitable for the storage and processing of large sized text based social media data. In this study, movie reviews have been automatically recorded with Apache ManifoldCF (MCF) [3] and data clusters have been created. Various methods compared such as Levenshtein and Fuzzy String Matching have been proposed to create a public dataset from collected data. Experimental results show that the proposed algorithm, which can be used as an open source dataset in sentiment analysis studies, have been performed successfully to the detection and correction of spelling errors.","Tue, 30 Jan 2018 13:18:51 UTC (433 KB)[v2] Wed, 31 Jan 2018 14:08:16 UTC (433 KB)"
"117","Entity Retrieval and Text Mining for Online Reputation Monitoring","Pedro Saleiro","Information Retrieval (cs.IR)","Online Reputation Monitoring (ORM) is concerned with the use of computational tools to measure the reputation of entities online, such as politicians or companies. In practice, current ORM methods are constrained to the generation of data analytics reports, which aggregate statistics of popularity and sentiment on social media. We argue that this format is too restrictive as end users often like to have the flexibility to search for entity-centric information that is not available in predefined charts. As such, we propose the inclusion of entity retrieval capabilities as a first step towards the extension of current ORM capabilities. However, an entity's reputation is also influenced by the entity's relationships with other entities. Therefore, we address the problem of Entity-Relationship (E-R) retrieval in which the goal is to search for multiple connected entities. This is a challenging problem which traditional entity search systems cannot cope with. Besides E-R retrieval we also believe ORM would benefit of text-based entity-centric prediction capabilities, such as predicting entity popularity on social media based on news events or the outcome of political surveys. However, none of these tasks can provide useful results if there is no effective entity disambiguation and sentiment analysis tailored to the context of ORM. Consequently, this thesis address two computational problems in Online Reputation Monitoring: Entity Retrieval and Text Mining. We researched and developed methods to extract, retrieve and predict entity-centric information spread across the Web.","Tue, 23 Jan 2018 19:36:29 UTC (2,216 KB)"
"118","Using Deep Learning for Title-Based Semantic Subject Indexing to Reach Competitive Performance to Full-Text","Florian Mai, Lukas Galke, Ansgar Scherp","Digital Libraries (cs.DL)","For (semi-)automated subject indexing systems in digital libraries, it is often more practical to use metadata such as the title of a publication instead of the full-text or the abstract. Therefore, it is desirable to have good text mining and text classification algorithms that operate well already on the title of a publication. So far, the classification performance on titles is not competitive with the performance on the full-texts if the same number of training samples is used for training. However, it is much easier to obtain title data in large quantities and to use it for training than full-text data. In this paper, we investigate the question how models obtained from training on increasing amounts of title training data compare to models from training on a constant number of full-texts. We evaluate this question on a large-scale dataset from the medical domain (PubMed) and from economics (EconBiz). In these datasets, the titles and annotations of millions of publications are available, and they outnumber the available full-texts by a factor of 20 and 15, respectively. To exploit these large amounts of data to their full potential, we develop three strong deep learning classifiers and evaluate their performance on the two datasets. The results are promising. On the EconBiz dataset, all three classifiers outperform their full-text counterparts by a large margin. The best title-based classifier outperforms the best full-text method by 9.4%. On the PubMed dataset, the best title-based method almost reaches the performance of the best full-text classifier, with a difference of only 2.9%.","Sat, 20 Jan 2018 19:26:20 UTC (854 KB)[v2] Tue, 29 May 2018 10:20:34 UTC (152 KB)"
"119","DCDistance: A Supervised Text Document Feature extraction based on class labels","Charles Henrique Porto Ferreira, Debora Maria Rossi de Medeiros, Fabricio Olivetti de Franca","Information Retrieval (cs.IR)","Text Mining is a field that aims at extracting information from textual data. One of the challenges of such field of study comes from the pre-processing stage in which a vector (and structured) representation should be extracted from unstructured data. The common extraction creates large and sparse vectors representing the importance of each term to a document. As such, this usually leads to the curse-of-dimensionality that plagues most machine learning algorithms. To cope with this issue, in this paper we propose a new supervised feature extraction and reduction algorithm, named DCDistance, that creates features based on the distance between a document to a representative of each class label. As such, the proposed technique can reduce the features set in more than 99% of the original set. Additionally, this algorithm was also capable of improving the classification accuracy over a set of benchmark datasets when compared to traditional and state-of-the-art features selection algorithms.","Sun, 14 Jan 2018 13:28:19 UTC (384 KB)"
"120","How the Taiwanese Do China Studies: Applications of Text Mining","Hsuan-Lei Shao, Sieh-Chuen Huang, Yun-Cheng Tsai","Digital Libraries (cs.DL)","With the rapid evolution of cross-strait situation, ""Mainland China"" as a subject of social science study has evoked the voice of ""Rethinking China Study"" among intelligentsia recently. This essay tried to apply an automatic content analysis tool (CATAR) to the journal ""Mainland China Studies"" (1998-2015) in order to observe the research trends based on the clustering of text from the title and abstract of each paper in the journal. The results showed that the 473 articles published by the journal were clustered into seven salient topics. From the publication number of each topic over time (including ""volume of publications"", ""percentage of publications""), there are two major topics of this journal while other topics varied over time widely. The contribution of this study includes: 1. We could group each ""independent"" study into a meaningful topic, as a small scale experiment verified that this topic clustering is feasible. 2. This essay reveals the salient research topics and their trends for the Taiwan journal ""Mainland China Studies"". 3. Various topical keywords were identified, providing easy access to the past study. 4. The yearly trends of the identified topics could be viewed as signature of future research directions.","Wed, 3 Jan 2018 06:18:14 UTC (1,131 KB)[v2] Tue, 20 Mar 2018 00:53:27 UTC (1,133 KB)[v3] Thu, 26 Apr 2018 08:32:10 UTC (465 KB)"
"121","A Deep Belief Network Based Machine Learning System for Risky Host Detection","Wangyan Feng, Shuning Wu, Xiaodan Li, Kevin Kunkle","Cryptography and Security (cs.CR)","To assure cyber security of an enterprise, typically SIEM (Security Information and Event Management) system is in place to normalize security event from different preventive technologies and flag alerts. Analysts in the security operation center (SOC) investigate the alerts to decide if it is truly malicious or not. However, generally the number of alerts is overwhelming with majority of them being false positive and exceeding the SOC's capacity to handle all alerts. There is a great need to reduce the false positive rate as much as possible. While most previous research focused on network intrusion detection, we focus on risk detection and propose an intelligent Deep Belief Network machine learning system. The system leverages alert information, various security logs and analysts' investigation results in a real enterprise environment to flag hosts that have high likelihood of being compromised. Text mining and graph based method are used to generate targets and create features for machine learning. In the experiment, Deep Belief Network is compared with other machine learning algorithms, including multi-layer neural network, random forest, support vector machine and logistic regression. Results on real enterprise data indicate that the deep belief network machine learning system performs better than other algorithms for our problem and is six times more effective than current rule-based system. We also implement the whole system from data collection, label creation, feature engineering to host score generation in a real enterprise production environment.","Fri, 29 Dec 2017 19:46:09 UTC (887 KB)"
"122","A Deep Network Model for Paraphrase Detection in Short Text Messages","Basant Agarwal, Heri Ramampiaro, Helge Langseth, Massimiliano Ruocco","Information Retrieval (cs.IR)","This paper is concerned with paraphrase detection. The ability to detect similar sentences written in natural language is crucial for several applications, such as text mining, text summarization, plagiarism detection, authorship authentication and question answering. Given two sentences, the objective is to detect whether they are semantically identical. An important insight from this work is that existing paraphrase systems perform well when applied on clean texts, but they do not necessarily deliver good performance against noisy texts. Challenges with paraphrase detection on user generated short texts, such as Twitter, include language irregularity and noise. To cope with these challenges, we propose a novel deep neural network-based approach that relies on coarse-grained sentence modeling using a convolutional neural network and a long short-term memory model, combined with a specific fine-grained word-level similarity matching model. Our experimental results show that the proposed approach outperforms existing state-of-the-art approaches on user-generated noisy social media data, such as Twitter texts, and achieves highly competitive performance on a cleaner corpus.","Thu, 7 Dec 2017 19:10:45 UTC (303 KB)"
"123","An innovative solution for breast cancer textual big data analysis","Nicolas Thiebaut, Antoine Simoulin, Karl Neuberger, Issam Ibnouhsein, Nicolas Bousquet, Nathalie Reix, Sebastien Moliere, Carole Mathelin","Machine Learning (stat.ML)","The digitalization of stored information in hospitals now allows for the exploitation of medical data in text format, as electronic health records (EHRs), initially gathered for other purposes than epidemiology. Manual search and analysis operations on such data become tedious. In recent years, the use of natural language processing (NLP) tools was highlighted to automatize the extraction of information contained in EHRs, structure it and perform statistical analysis on this structured information. The main difficulties with the existing approaches is the requirement of synonyms or ontology dictionaries, that are mostly available in English only and do not include local or custom notations. In this work, a team composed of oncologists as domain experts and data scientists develop a custom NLP-based system to process and structure textual clinical reports of patients suffering from breast cancer. The tool relies on the combination of standard text mining techniques and an advanced synonym detection method. It allows for a global analysis by retrieval of indicators such as medical history, tumor characteristics, therapeutic responses, recurrences and prognosis. The versatility of the method allows to obtain easily new indicators, thus opening up the way for retrospective studies with a substantial reduction of the amount of manual work. With no need for biomedical annotators or pre-defined ontologies, this language-agnostic method reached an good extraction accuracy for several concepts of interest, according to a comparison with a manually structured file, without requiring any existing corpus with local or new notations.","Wed, 6 Dec 2017 16:18:31 UTC (1,800 KB)"
"124","Listening to Chaotic Whispers: A Deep Learning Framework for News-oriented Stock Trend Prediction","Ziniu Hu, Weiqing Liu, Jiang Bian, Xuanzhe Liu, Tie-Yan Liu","Social and Information Networks (cs.SI)","Stock trend prediction plays a critical role in seeking maximized profit from stock investment. However, precise trend prediction is very difficult since the highly volatile and non-stationary nature of stock market. Exploding information on Internet together with advancing development of natural language processing and text mining techniques have enable investors to unveil market trends and volatility from online content. Unfortunately, the quality, trustworthiness and comprehensiveness of online content related to stock market varies drastically, and a large portion consists of the low-quality news, comments, or even rumors. To address this challenge, we imitate the learning process of human beings facing such chaotic online news, driven by three principles: sequential content dependency, diverse influence, and effective and efficient learning. In this paper, to capture the first two principles, we designed a Hybrid Attention Networks to predict the stock trend based on the sequence of recent related news. Moreover, we apply the self-paced learning mechanism to imitate the third principle. Extensive experiments on real-world stock market data demonstrate the effectiveness of our approach.","Wed, 6 Dec 2017 11:33:21 UTC (2,968 KB)[v2] Fri, 16 Mar 2018 17:32:11 UTC (0 KB)[v3] Wed, 20 Feb 2019 04:30:16 UTC (2,968 KB)"
"125","Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals","Girish Keshav Palshikar, Sachin Pawar, Saheb Chourasia, Nitin Ramrakhiyani","Computation and Language (cs.CL)","Performance appraisal (PA) is an important HR process to periodically measure and evaluate every employee's performance vis-a-vis the goals established by the organization. A PA process involves purposeful multi-step multi-modal communication between employees, their supervisors and their peers, such as self-appraisal, supervisor assessment and peer feedback. Analysis of the structured data and text produced in PA is crucial for measuring the quality of appraisals and tracking actual improvements. In this paper, we apply text mining techniques to produce insights from PA text. First, we perform sentence classification to identify strengths, weaknesses and suggestions of improvements found in the supervisor assessments and then use clustering to discover broad categories among them. Next we use multi-class multi-label classification techniques to match supervisor assessments to predefined broad perspectives on performance. Finally, we propose a short-text summarization technique to produce a summary of peer feedback comments for a given employee and compare it with manual summaries. All techniques are illustrated using a real-life dataset of supervisor assessment and peer feedback text produced during the PA of 4528 employees in a large multi-national IT company.","Mon, 4 Dec 2017 10:30:18 UTC (38 KB)"
"126","Exploration of an Interdisciplinary Scientific Landscape","Juste Raimbault","Digital Libraries (cs.DL)","Patterns of interdisciplinarity in science can be quantified through diverse complementary dimensions. This paper studies as a case study the scientific environment of a generalist journal in Geography, Cybergeo, in order to introduce a novel methodology combining citation network analysis and semantic analysis. We collect a large corpus of around 200,000 articles with their abstracts and the corresponding citation network that provides a first citation classification. Relevant keywords are extracted for each article through text-mining, allowing us to construct a semantic classification. We study the qualitative patterns of relations between endogenous disciplines within each classification, and finally show the complementarity of classifications and of their associated interdisciplinarity measures. The tools we develop accordingly are open and reusable for similar large scale studies of scientific environments.","Sun, 3 Dec 2017 17:55:30 UTC (1,890 KB)"
"127","Joint Topic-Semantic-aware Social Recommendation for Online Voting","Hongwei Wang, Jia Wang, Miao Zhao, Jiannong Cao, Minyi Guo","Machine Learning (stat.ML)","Online voting is an emerging feature in social networks, in which users can express their attitudes toward various issues and show their unique interest. Online voting imposes new challenges on recommendation, because the propagation of votings heavily depends on the structure of social networks as well as the content of votings. In this paper, we investigate how to utilize these two factors in a comprehensive manner when doing voting recommendation. First, due to the fact that existing text mining methods such as topic model and semantic model cannot well process the content of votings that is typically short and ambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to learn word and document representation by jointly considering their topics and semantics. Then we propose our Joint Topic-Semantic-aware social Matrix Factorization (JTS-MF) model for voting recommendation. JTS-MF model calculates similarity among users and votings by combining their TEWE representation and structural information of social networks, and preserves this topic-semantic-social similarity during matrix factorization. To evaluate the performance of TEWE representation and JTS-MF model, we conduct extensive experiments on real online voting dataset. The results prove the efficacy of our approach against several state-of-the-art baselines.","Sun, 3 Dec 2017 08:11:43 UTC (1,515 KB)"
"128","KIBS Innovative Entrepreneurship Networks on Social Media","Jose N. Franco-Riquelme, Isaac Lemus-Aguilar, Joaquin Ordieres-Mere","Social and Information Networks (cs.SI)","The analysis of the use of social media for innovative entrepreneurship in the context has received little attention in the literature, especially in the context of Knowledge Intensive Business Services (KIBS). Therefore, this paper focuses on bridging this gap by applying text mining and sentiment analysis techniques to identify the innovative entrepreneurship reflected by these companies in their social media. Finally, we present and analyze the results of our quantitative analysis of 23.483 posts based on eleven Spanish and Italian consultancy KIBS Twitter Usernames and Keywords using data interpretation techniques such as clustering and topic modeling. This paper suggests that there is a significant gap between the perceived potential of social media and the entrepreneurial behaviors at the social context in business-to-business (B2B) companies.","Thu, 30 Nov 2017 14:08:19 UTC (1,006 KB)"
"129","Emerging basic, clinical and translational research fronts in dental biomaterials R&D","David Fajardo-Ortiz, Pablo Jaramillo, Claudia Jaramillo, Raul Resendiz, Miguel Lara-Flores, Victor M. Castano","Digital Libraries (cs.DL)","The current (2007-2007) structure and content of dental materials research has been investigated by identifying and describing the emergent research fronts which can be related to basic, translational and clinical observation research. By a combination of network analysis and text mining of the literature on dental materials indexed in the Web of Science, we have identified eleven emerging research fronts. These fronts are related to different dental materials applications which are at different levels in the knowledge translation and biomedical innovation process. We identified fronts related to dominant designs like titanium implants, competing technologies like ceramics and composites applications to prothesis and restauration, and disruptive technologies like nanomaterials and mineral trioxide aggregates. Our results suggest the possible relation between the technological complexity of the dental materials and the level of advance in terms of knowledge translation. This is the first time the structure and content of research on dental materials research is analyzed.","Thu, 30 Nov 2017 00:30:24 UTC (493 KB)"
"130","The evolution and structure of biomedical knowledge on cytochrome P450","David Fajardo-Ortiz, Miguel Lara, Victor M. Castano","Other Quantitative Biology (q-bio.OT)","Cytochrome P450 are fundamental proteins to the metabolism of drugs and other relevant processes. through a combination of text mining and network analysis of the P450 literature we mapped the emergence and evolution of the biomedical research communities working on this family of proteins. Our results suggest that the historical research communities that worked on P450 emerged and were organized mainly around methodological achievements like the induction of animal liver microsomal P450 by drugs, the use of chemical inhibitors of P450 enzymes in in-vitro metabolism studies and the development of E. coli expression systems. We found clear evidence that P450 research indeed constitutes a material scientific culture, as we discuss in the text.","Mon, 27 Nov 2017 21:37:32 UTC (496 KB)"
"131","Effective Use of Bidirectional Language Modeling for Transfer Learning in Biomedical Named Entity Recognition","Devendra Singh Sachan, Pengtao Xie, Mrinmaya Sachan, Eric P Xing","Computation and Language (cs.CL)","Biomedical named entity recognition (NER) is a fundamental task in text mining of medical documents and has many applications. Deep learning based approaches to this task have been gaining increasing attention in recent years as their parameters can be learned end-to-end without the need for hand-engineered features. However, these approaches rely on high-quality labeled data, which is expensive to obtain. To address this issue, we investigate how to use unlabeled text data to improve the performance of NER models. Specifically, we train a bidirectional language model (BiLM) on unlabeled data and transfer its weights to ""pretrain"" an NER model with the same architecture as the BiLM, which results in a better parameter initialization of the NER model. We evaluate our approach on four benchmark datasets for biomedical NER and show that it leads to a substantial improvement in the F1 scores compared with the state-of-the-art approaches. We also show that BiLM weight transfer leads to a faster model training and the pretrained model requires fewer training examples to achieve a particular F1 score.","Tue, 21 Nov 2017 16:55:18 UTC (1,026 KB)[v2] Sat, 25 Nov 2017 18:08:33 UTC (1,026 KB)[v3] Wed, 15 Aug 2018 03:31:30 UTC (392 KB)"
"132","Text Mining Descriptions Of Dreams: aesthetic and clinical efforts","Renato Fabbri, Fabiane M. Borges","Computers and Society (cs.CY)","Dreams are highly valued in both Freudian psychoanalysis and less conservative clinical traditions. Text mining enables the extraction of meaning from writings in powerful and unexpected ways. In this work, we report methods, uses and results obtained by mining descriptions of dreams. The texts were collected as part of a course in Schizoanalysis (Clinical Psychology) from dozens of participants. They were subsequently mined using various techniques for the achievement of poems and summaries, which were then used in clinical sessions by means of music and declamation. The results were found aesthetically appealing and effective to engage the audience. The expansion of the corpus, mining methods and strategies for using the derivatives for art and therapy are considered for future work.","Thu, 26 Oct 2017 23:19:18 UTC (90 KB)"
"133","Latent Dirichlet Allocation (LDA) and Topic modeling: models, applications, a survey","Hamed Jelodar, Yongli Wang, Chi Yuan, Xia Feng, Xiahui Jiang, Yanchao Li, Liang Zhao","Information Retrieval (cs.IR)","Topic modeling is one of the most powerful techniques in text mining for data mining, latent data discovery, and finding relationships among data, text documents. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modeling, which Latent Dirichlet allocation (LDA) is one of the most popular methods in this field. Researchers have proposed various models based on the LDA in topic modeling. According to previous work, this paper can be very useful and valuable for introducing LDA approaches in topic modeling. In this paper, we investigated scholarly articles highly (between 2003 to 2016) related to Topic Modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling. Also, we summarize challenges and introduce famous tools and datasets in topic modeling based on LDA.","Sun, 12 Nov 2017 14:50:14 UTC (898 KB)[v2] Thu, 6 Dec 2018 04:20:48 UTC (1,177 KB)"
"134","Joint Sentiment/Topic Modeling on Text Data Using Boosted Restricted Boltzmann Machine","Masoud Fatemi, Mehran Safayani","Computation and Language (cs.CL)","Recently by the development of the Internet and the Web, different types of social media such as web blogs become an immense source of text data. Through the processing of these data, it is possible to discover practical information about different topics, individuals opinions and a thorough understanding of the society. Therefore, applying models which can automatically extract the subjective information from the documents would be efficient and helpful. Topic modeling methods, also sentiment analysis are the most raised topics in the natural language processing and text mining fields. In this paper a new structure for joint sentiment-topic modeling based on Restricted Boltzmann Machine (RBM) which is a type of neural networks is proposed. By modifying the structure of RBM as well as appending a layer which is analogous to sentiment of text data to it, we propose a generative structure for joint sentiment topic modeling based on neutral networks. The proposed method is supervised and trained by the Contrastive Divergence algorithm. The new attached layer in the proposed model is a layer with the multinomial probability distribution which can be used in text data sentiment classification or any other supervised application. The proposed model is compared with existing models in the experiments such as evaluating as a generative model, sentiment classification, information retrieval and the corresponding results demonstrate the efficiency of the method.","Fri, 10 Nov 2017 09:17:02 UTC (144 KB)"
"135","Weakly-supervised Relation Extraction by Pattern-enhanced Embedding Learning","Meng Qu, Xiang Ren, Yu Zhang, Jiawei Han","Computation and Language (cs.CL)","Extracting relations from text corpora is an important task in text mining. It becomes particularly challenging when focusing on weakly-supervised relation extraction, that is, utilizing a few relation instances (i.e., a pair of entities and their relation) as seeds to extract more instances from corpora. Existing distributional approaches leverage the corpus-level co-occurrence statistics of entities to predict their relations, and require large number of labeled instances to learn effective relation classifiers. Alternatively, pattern-based approaches perform bootstrapping or apply neural networks to model the local contexts, but still rely on large number of labeled instances to build reliable models. In this paper, we study integrating the distributional and pattern-based methods in a weakly-supervised setting, such that the two types of methods can provide complementary supervision for each other to build an effective, unified model. We propose a novel co-training framework with a distributional module and a pattern module. During training, the distributional module helps the pattern module discriminate between the informative patterns and other patterns, and the pattern module generates some highly-confident instances to improve the distributional module. The whole framework can be effectively optimized by iterating between improving the pattern module and updating the distributional module. We conduct experiments on two tasks: knowledge base completion with text corpora and corpus-level relation extraction. Experimental results prove the effectiveness of our framework in the weakly-supervised setting.","Thu, 9 Nov 2017 01:42:14 UTC (1,094 KB)[v2] Tue, 26 Dec 2017 02:54:11 UTC (1,094 KB)"
"136","MEDOC: a Python wrapper to load MEDLINE into a local MySQL database","Emeric Dynomant, Mathilde Gorieu, Helene Perrin, Marion Denorme, Fabien Pichon, Arnaud Desfeux","Digital Libraries (cs.DL)","Since the MEDLINE database was released, the number of documents indexed by this entity has risen every year. Several tools have been developed by the National Institutes of Health (NIH) to query this corpus of scientific publications. However, in terms of advances in big data, text-mining and data science, an option to build a local relational database containing all metadata available on MEDLINE would be truly useful to optimally exploit these resources. MEDOC (MEdline DOwnloading Contrivance) is a Python program designed to download data on an FTP and to load all extracted information into a local MySQL database. It took MEDOC 4 days and 17 hours to load the 26 million documents available on this server onto a standard computer. This indexed relational database allows the user to build complex and rapid queries. All fields can thus be searched for desired information, a task that is difficult to accomplish through the PubMed graphical interface. MEDOC is free and publicly available at this https URL.","Wed, 18 Oct 2017 06:14:53 UTC (143 KB)"
"137","Classifying Web Exploits with Topic Modeling","Jukka Ruohonen","Cryptography and Security (cs.CR)","This short empirical paper investigates how well topic modeling and database meta-data characteristics can classify web and other proof-of-concept (PoC) exploits for publicly disclosed software vulnerabilities. By using a dataset comprised of over 36 thousand PoC exploits, near a 0.9 accuracy rate is obtained in the empirical experiment. Text mining and topic modeling are a significant boost factor behind this classification performance. In addition to these empirical results, the paper contributes to the research tradition of enhancing software vulnerability information with text mining, providing also a few scholarly observations about the potential for semi-automatic classification of exploits in the existing tracking infrastructures.","Mon, 16 Oct 2017 08:34:24 UTC (120 KB)"
"138","Decision support from financial disclosures with deep neural networks and transfer learning","Mathias Kraus, Stefan Feuerriegel","Computation and Language (cs.CL)","Company disclosures greatly aid in the process of financial decision-making; therefore, they are consulted by financial investors and automated traders before exercising ownership in stocks. While humans are usually able to correctly interpret the content, the same is rarely true of computerized decision support systems, which struggle with the complexity and ambiguity of natural language. A possible remedy is represented by deep learning, which overcomes several shortcomings of traditional methods of text mining. For instance, recurrent neural networks, such as long short-term memories, employ hierarchical structures, together with a large number of hidden layers, to automatically extract features from ordered sequences of words and capture highly non-linear relationships such as context-dependent meanings. However, deep learning has only recently started to receive traction, possibly because its performance is largely untested. Hence, this paper studies the use of deep neural networks for financial decision support. We additionally experiment with transfer learning, in which we pre-train the network on a different corpus with a length of 139.1 million words. Our results reveal a higher directional accuracy as compared to traditional machine learning when predicting stock price movements in response to financial disclosures. Our work thereby helps to highlight the business value of deep learning and provides recommendations to practitioners and executives.","Wed, 11 Oct 2017 08:22:10 UTC (295 KB)"
"139","Food for Thought: Analyzing Public Opinion on the Supplemental Nutrition Assistance Program","Miriam Chappelka (1), Jihwan Oh (2), Dorris Scott (3), Mizzani Walker-Holmes (4) ((1) University of Pennsylvania, (2) Georgia Institute of Technology, (3) University of Georgia, (4) Kennesaw State University)","Computers and Society (cs.CY)","This project explores public opinion on the Supplemental Nutrition Assistance Program (SNAP) in news and social media outlets, and tracks elected representatives' voting records on issues relating to SNAP and food insecurity. We used machine learning, sentiment analysis, and text mining to analyze national and state level coverage of SNAP in order to gauge perceptions of the program over time across these outlets. Results indicate that the majority of news coverage has negative sentiment, more partisan news outlets have more extreme sentiment, and that clustering of negative reporting on SNAP occurs in the Midwest. Our final results and tools will be displayed in an on-line application that the ACFB Advocacy team can use to inform their communication to relevant stakeholders.","Fri, 6 Oct 2017 15:10:48 UTC (535 KB)"
"140","Predicting Disease-Gene Associations using Cross-Document Graph-based Features","Hendrik ter Horst, Matthias Hartung, Roman Klinger, Matthias Zwick, Philipp Cimiano","Computation and Language (cs.CL)","In the context of personalized medicine, text mining methods pose an interesting option for identifying disease-gene associations, as they can be used to generate novel links between diseases and genes which may complement knowledge from structured databases. The most straightforward approach to extract such links from text is to rely on a simple assumption postulating an association between all genes and diseases that co-occur within the same document. However, this approach (i) tends to yield a number of spurious associations, (ii) does not capture different relevant types of associations, and (iii) is incapable of aggregating knowledge that is spread across documents. Thus, we propose an approach in which disease-gene co-occurrences and gene-gene interactions are represented in an RDF graph. A machine learning-based classifier is trained that incorporates features extracted from the graph to separate disease-gene pairs into valid disease-gene associations and spurious ones. On the manually curated Genetic Testing Registry, our approach yields a 30 points increase in F1 score over a plain co-occurrence baseline.","Tue, 26 Sep 2017 19:59:16 UTC (201 KB)"
"141","Computational Content Analysis of Negative Tweets for Obesity, Diet, Diabetes, and Exercise","George Shaw Jr., Amir Karami","Social and Information Networks (cs.SI)","Social media based digital epidemiology has the potential to support faster response and deeper understanding of public health related threats. This study proposes a new framework to analyze unstructured health related textual data via Twitter users' post (tweets) to characterize the negative health sentiments and non-health related concerns in relations to the corpus of negative sentiments, regarding Diet Diabetes Exercise, and Obesity (DDEO). Through the collection of 6 million Tweets for one month, this study identified the prominent topics of users as it relates to the negative sentiments. Our proposed framework uses two text mining methods, sentiment analysis and topic modeling, to discover negative topics. The negative sentiments of Twitter users support the literature narratives and the many morbidity issues that are associated with DDEO and the linkage between obesity and diabetes. The framework offers a potential method to understand the publics' opinions and sentiments regarding DDEO. More importantly, this research provides new opportunities for computational social scientists, medical experts, and public health professionals to collectively address DDEO-related issues.","Fri, 22 Sep 2017 19:18:42 UTC (413 KB)"
"142","A textual transform of multivariate time-series for prognostics","Abhay Harpale (1), Abhishek Srivastav (1) ((1) GE Global Research)","Machine Learning (stat.ML)","Prognostics or early detection of incipient faults is an important industrial challenge for condition-based and preventive maintenance. Physics-based approaches to modeling fault progression are infeasible due to multiple interacting components, uncontrolled environmental factors and observability constraints. Moreover, such approaches to prognostics do not generalize to new domains. Consequently, domain-agnostic data-driven machine learning approaches to prognostics are desirable. Damage progression is a path-dependent process and explicitly modeling the temporal patterns is critical for accurate estimation of both the current damage state and its progression leading to total failure. In this paper, we present a novel data-driven approach to prognostics that employs a novel textual representation of multivariate temporal sensor observations for predicting the future health state of the monitored equipment early in its life. This representation enables us to utilize well-understood concepts from text-mining for modeling, prediction and understanding distress patterns in a domain agnostic way. The approach has been deployed and successfully tested on large scale multivariate time-series data from commercial aircraft engines. We report experiments on well-known publicly available benchmark datasets and simulation datasets. The proposed approach is shown to be superior in terms of prediction accuracy, lead time to prediction and interpretability.","Tue, 19 Sep 2017 22:54:10 UTC (3,026 KB)"
"143","ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models","Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, Cho-Jui Hsieh","Machine Learning (stat.ML)","Deep neural networks (DNNs) are one of the most prominent technologies of our time, as they achieve state-of-the-art performance in many machine learning tasks, including but not limited to image classification, text mining, and speech processing. However, recent research on DNNs has indicated ever-increasing concern on the robustness to adversarial examples, especially for security-critical tasks such as traffic sign identification for autonomous driving. Studies have unveiled the vulnerability of a well-trained DNN by demonstrating the ability of generating barely noticeable (to both human and machines) adversarial images that lead to misclassification. Furthermore, researchers have shown that these adversarial images are highly transferable by simply training and attacking a substitute model built upon the target model, known as a black-box attack to DNNs. Similar to the setting of training substitute models, in this paper we propose an effective black-box attack that also only has access to the input (images) and the output (confidence scores) of a targeted DNN. However, different from leveraging attack transferability from substitute models, we propose zeroth order optimization (ZOO) based attacks to directly estimate the gradients of the targeted DNN for generating adversarial examples. We use zeroth order stochastic coordinate descent along with dimension reduction, hierarchical attack and importance sampling techniques to efficiently attack black-box models. By exploiting zeroth order optimization, improved attacks to the targeted DNN can be accomplished, sparing the need for training substitute models and avoiding the loss in attack transferability. Experimental results on MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective as the state-of-the-art white-box attack and significantly outperforms existing black-box attacks via substitute models.","Mon, 14 Aug 2017 03:48:03 UTC (2,147 KB)[v2] Thu, 2 Nov 2017 04:18:44 UTC (2,852 KB)"
"144","Recent Developments and Future Challenges in Medical Mixed Reality","Long Chen, Thomas Day, Wen Tang, Nigel W. John","Computer Vision and Pattern Recognition (cs.CV)","Mixed Reality (MR) is of increasing interest within technology-driven modern medicine but is not yet used in everyday practice. This situation is changing rapidly, however, and this paper explores the emergence of MR technology and the importance of its utility within medical applications. A classification of medical MR has been obtained by applying an unbiased text mining method to a database of 1,403 relevant research papers published over the last two decades. The classification results reveal a taxonomy for the development of medical MR research during this period as well as suggesting future trends. We then use the classification to analyse the technology and applications developed in the last five years. Our objective is to aid researchers to focus on the areas where technology advancements in medical MR are most needed, as well as providing medical practitioners with a useful source of reference.","Thu, 3 Aug 2017 17:15:18 UTC (7,302 KB)"
"145","From Image to Text Classification: A Novel Approach based on Clustering Word Embeddings","Andrei M. Butnaru, Radu Tudor Ionescu","Computation and Language (cs.CL)","In this paper, we propose a novel approach for text classification based on clustering word embeddings, inspired by the bag of visual words model, which is widely used in computer vision. After each word in a collection of documents is represented as word vector using a pre-trained word embeddings model, a k-means algorithm is applied on the word vectors in order to obtain a fixed-size set of clusters. The centroid of each cluster is interpreted as a super word embedding that embodies all the semantically related word vectors in a certain region of the embedding space. Every embedded word in the collection of documents is then assigned to the nearest cluster centroid. In the end, each document is represented as a bag of super word embeddings by computing the frequency of each super word embedding in the respective document. We also diverge from the idea of building a single vocabulary for the entire collection of documents, and propose to build class-specific vocabularies for better performance. Using this kind of representation, we report results on two text mining tasks, namely text categorization by topic and polarity classification. On both tasks, our model yields better performance than the standard bag of words.","Tue, 25 Jul 2017 17:29:18 UTC (735 KB)"
"146","Cooperative Hierarchical Dirichlet Processes: Superposition vs. Maximization","Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu","Machine Learning (cs.LG)","The cooperative hierarchical structure is a common and significant data structure observed in, or adopted by, many research areas, such as: text mining (author-paper-word) and multi-label classification (label-instance-feature). Renowned Bayesian approaches for cooperative hierarchical structure modeling are mostly based on topic models. However, these approaches suffer from a serious issue in that the number of hidden topics/factors needs to be fixed in advance and an inappropriate number may lead to overfitting or underfitting. One elegant way to resolve this issue is Bayesian nonparametric learning, but existing work in this area still cannot be applied to cooperative hierarchical structure modeling. In this paper, we propose a cooperative hierarchical Dirichlet process (CHDP) to fill this gap. Each node in a cooperative hierarchical structure is assigned a Dirichlet process to model its weights on the infinite hidden factors/topics. Together with measure inheritance from hierarchical Dirichlet process, two kinds of measure cooperation, i.e., superposition and maximization, are defined to capture the many-to-many relationships in the cooperative hierarchical structure. Furthermore, two constructive representations for CHDP, i.e., stick-breaking and international restaurant process, are designed to facilitate the model inference. Experiments on synthetic and real-world data with cooperative hierarchical structures demonstrate the properties and the ability of CHDP for cooperative hierarchical structure modeling and its potential for practical application scenarios.","Tue, 18 Jul 2017 00:42:10 UTC (2,251 KB)"
"147","Leipzig Corpus Miner - A Text Mining Infrastructure for Qualitative Data Analysis","Andreas Niekler, Gregor Wiedemann, Gerhard Heyer","Computation and Language (cs.CL)","This paper presents the ""Leipzig Corpus Miner"", a technical infrastructure for supporting qualitative and quantitative content analysis. The infrastructure aims at the integration of 'close reading' procedures on individual documents with procedures of 'distant reading', e.g. lexical characteristics of large document collections. Therefore information retrieval systems, lexicometric statistics and machine learning procedures are combined in a coherent framework which enables qualitative data analysts to make use of state-of-the-art Natural Language Processing techniques on very large document collections. Applicability of the framework ranges from social sciences to media studies and market research. As an example we introduce the usage of the framework in a political science study on post-democracy and neoliberalism.","Tue, 11 Jul 2017 13:04:15 UTC (747 KB)"
"148","A Brief Survey of Text Mining: Classification, Clustering and Extraction Techniques","Mehdi Allahyari, Seyedamin Pouriyeh, Mehdi Assefi, Saied Safaei, Elizabeth D. Trippe, Juan B. Gutierrez, Krys Kochut","Computation and Language (cs.CL)","The amount of text that is generated every day is increasing dramatically. This tremendous volume of mostly unstructured text cannot be simply processed and perceived by computers. Therefore, efficient and effective techniques and algorithms are required to discover useful patterns. Text mining is the task of extracting meaningful information from text, which has gained significant attentions in recent years. In this paper, we describe several of the most fundamental text mining tasks and techniques including text pre-processing, classification and clustering. Additionally, we briefly explain text mining in biomedical and health care domains.","Mon, 10 Jul 2017 16:02:44 UTC (116 KB)[v2] Fri, 28 Jul 2017 16:32:25 UTC (116 KB)"
"149","Identifying Condition-Action Statements in Medical Guidelines Using Domain-Independent Features","Hossein Hematialam, Wlodek Zadrozny","Computation and Language (cs.CL)","This paper advances the state of the art in text understanding of medical guidelines by releasing two new annotated clinical guidelines datasets, and establishing baselines for using machine learning to extract condition-action pairs. In contrast to prior work that relies on manually created rules, we report experiment with several supervised machine learning techniques to classify sentences as to whether they express conditions and actions. We show the limitations and possible extensions of this work on text mining of medical guidelines.","Tue, 13 Jun 2017 18:02:27 UTC (14 KB)[v2] Wed, 21 Jun 2017 18:35:26 UTC (14 KB)"
"150","Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2017)","Muthu Kumar Chandrasekaran, Kokil Jaidka, Philipp Mayr","Digital Libraries (cs.DL)","The large scale of scholarly publications poses a challenge for scholars in information seeking and sensemaking. Bibliometrics, information retrieval (IR), text mining and NLP techniques could help in these search and look-up activities, but are not yet widely used. This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometrics, text mining and recommendation techniques that can advance the state-of-the-art in scholarly document understanding, analysis, and retrieval at scale. The BIRNDL workshop at SIGIR 2017 will incorporate an invited talk, paper sessions and the third edition of the Computational Linguistics (CL) Scientific Summarization Shared Task.","Thu, 8 Jun 2017 10:53:57 UTC (48 KB)"
"151","Max-Cosine Matching Based Neural Models for Recognizing Textual Entailment","Zhipeng Xie, Junfeng Hu","Computation and Language (cs.CL)","Recognizing textual entailment is a fundamental task in a variety of text mining or natural language processing applications. This paper proposes a simple neural model for RTE problem. It first matches each word in the hypothesis with its most-similar word in the premise, producing an augmented representation of the hypothesis conditioned on the premise as a sequence of word pairs. The LSTM model is then used to model this augmented sequence, and the final output from the LSTM is fed into a softmax layer to make the prediction. Besides the base model, in order to enhance its performance, we also proposed three techniques: the integration of multiple word-embedding library, bi-way integration, and ensemble based on model averaging. Experimental results on the SNLI dataset have shown that the three techniques are effective in boosting the predicative accuracy and that our method outperforms several state-of-the-state ones.","Thu, 25 May 2017 05:45:42 UTC (14 KB)"
"152","Towards Interrogating Discriminative Machine Learning Models","Wenbo Guo, Kaixuan Zhang, Lin Lin, Sui Huang, Xinyu Xing","Machine Learning (cs.LG)","It is oftentimes impossible to understand how machine learning models reach a decision. While recent research has proposed various technical approaches to provide some clues as to how a learning model makes individual decisions, they cannot provide users with ability to inspect a learning model as a complete entity. In this work, we propose a new technical approach that augments a Bayesian regression mixture model with multiple elastic nets. Using the enhanced mixture model, we extract explanations for a target model through global approximation. To demonstrate the utility of our approach, we evaluate it on different learning models covering the tasks of text mining and image recognition. Our results indicate that the proposed approach not only outperforms the state-of-the-art technique in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of a learning model.","Tue, 23 May 2017 23:51:37 UTC (731 KB)"
"153","Social Media-based Substance Use Prediction","Tao Ding, Warren K. Bickel, Shimei Pan","Computation and Language (cs.CL)","In this paper, we demonstrate how the state-of-the-art machine learning and text mining techniques can be used to build effective social media-based substance use detection systems. Since a substance use ground truth is difficult to obtain on a large scale, to maximize system performance, we explore different feature learning methods to take advantage of a large amount of unsupervised social media data. We also demonstrate the benefit of using multi-view unsupervised feature learning to combine heterogeneous user information such as Facebook `""likes"" and ""status updates"" to enhance system performance. Based on our evaluation, our best models achieved 86% AUC for predicting tobacco use, 81% for alcohol use and 84% for drug use, all of which significantly outperformed existing methods. Our investigation has also uncovered interesting relations between a user's social media behavior (e.g., word usage) and substance use.","Tue, 16 May 2017 10:37:52 UTC (78 KB)[v2] Wed, 31 May 2017 19:38:14 UTC (78 KB)"
"154","ResumeVis: A Visual Analytics System to Discover Semantic Information in Semi-structured Resume Data","Chen Zhang, Hao Wang, Yingcai Wu","Human-Computer Interaction (cs.HC)","Massive public resume data emerging on the WWW indicates individual-related characteristics in terms of profile and career experiences. Resume Analysis (RA) provides opportunities for many applications, such as talent seeking and evaluation. Existing RA studies based on statistical analyzing have primarily focused on talent recruitment by identifying explicit attributes. However, they failed to discover the implicit semantic information, i.e., individual career progress patterns and social-relations, which are vital to comprehensive understanding of career development. Besides, how to visualize them for better human cognition is also challenging. To tackle these issues, we propose a visual analytics system ResumeVis to mine and visualize resume data. Firstly, a text-mining based approach is presented to extract semantic information. Then, a set of visualizations are devised to represent the semantic information in multiple perspectives. By interactive exploration on ResumeVis performed by domain experts, the following tasks can be accomplished: to trace individual career evolving trajectory; to mine latent social-relations among individuals; and to hold the full picture of massive resumes' collective mobility. Case studies with over 2500 online officer resumes demonstrate the effectiveness of our system. We provide a demonstration video.","Mon, 15 May 2017 13:13:47 UTC (1,061 KB)"
"155","Clustering Airbnb Reviews","Yang Tang, Paul D. McNicholas","Applications (stat.AP)","In the last decade, online customer reviews increasingly exert influence on consumers' decision when booking accommodation online. The renewal importance to the concept of word-of mouth is reflected in the growing interests in investigating consumers' experience by analyzing their online reviews through the process of text mining and sentiment analysis. A clustering approach is developed for Boston Airbnb reviews submitted in the English language and collected from 2009 to 2016. This approach is based on a mixture of latent variable models, which provides an appealing framework for handling clustered binary data. We address here the problem of discovering meaningful segments of consumers that are coherent from both the underlying topics and the sentiment behind the reviews. A penalized mixture of latent traits approach is developed to reduce the number of parameters and identify variables that are not informative for clustering. The introduction of component-specific rate parameters avoids the over-penalization that can occur when inferring a shared rate parameter on clustered data. We divided the guests into four groups -- property driven guests, host driven guests, guests with recent overall negative stay and guests with some negative experiences.","Tue, 9 May 2017 00:53:36 UTC (38 KB)[v2] Tue, 4 Jul 2017 00:13:54 UTC (37 KB)[v3] Thu, 27 Jun 2019 23:34:08 UTC (91 KB)"
"156","ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases","Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, Ronald M. Summers","Computer Vision and Pattern Recognition (cs.CV)","The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely ""ChestX-ray8"", which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based ""reading chest X-rays"" (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems. Data download link: this https URL","Fri, 5 May 2017 17:31:12 UTC (6,954 KB)[v2] Fri, 19 May 2017 17:45:07 UTC (7,325 KB)[v3] Wed, 19 Jul 2017 19:12:50 UTC (7,573 KB)[v4] Wed, 27 Sep 2017 14:33:36 UTC (7,326 KB)[v5] Thu, 14 Dec 2017 19:35:31 UTC (7,327 KB)"
"157","Virtual Machine Introspection Based Malware Behavior Profiling and Family Grouping","Shun-Wen Hsiao, Yeali S.Sun, Meng Chang Chen","Cryptography and Security (cs.CR)","The proliferation of malwares have been attributed to the alternations of a handful of original malware source codes. The malwares alternated from the same origin share some intrinsic behaviors and form a malware family. Expediently, identifying its malware family when a malware is first seen on the Internet can provide useful clues to mitigate the threat. In this paper, a malware profiler (VMP) is proposed to profile the execution behaviors of a malware by leveraging virtual machine introspection (VMI) technique. The VMP inserts plug-ins inside the virtual machine monitor (VMM) to record the invoked API calls with their input parameters and return values as the profile of malware. In this paper, a popular similarity measurement Jaccard distance and a phylogenetic tree construction method are adopted to discover malware families. The studies of malware profiles show the malwares from a malware family are very similar to each others and distinct from other malware families as well as benign software. This paper also examines VMP against existing anti-malware detection engines and some well-known malware grouping methods to compare the goodness in their malware family constructions. A peer voting approach is proposed and the results show VMP is better than almost all of the compared anti-malware engines, and compatible with the fine tuned text-mining approach and high order N-gram approaches. We also establish a malware profiling website based on VMP for malware research.","Thu, 4 May 2017 04:20:32 UTC (978 KB)"
"158","A Methodology of Guiding Web Content Mining and Knowledge Discovery in Evidence-based Software Engineering","Zheng Li, Yan Liu","Software Engineering (cs.SE)","Systematic Literature Review (SLR) is a rigorous methodology applied for Evidence-Based Software Engineering (EBSE) that identify, assess and synthesize the relevant evidence for answering specific research questions. Benefiting from the booming online materials in the era of Web 2.0, the technical Web content starts acting as alternative sources for EBSE. Web knowledge has been investigated and derived from Web content mining and knowledge discovery techniques, however they are still significantly different from reviewing academic literature. Thus the direct adoption of Web knowledge in EBSE lacks of systematic guidelines. In this paper, we propose to make an SLR adaptation to bridge the aforementioned gap along two stages. Firstly, we follow the general logic and procedure of SLR to regulate Web mining activities. Secondly, we substitute and enhance particular SLR processes with Web-mining-friendly methods and approaches. At the second stage, we mainly focus on adapting Conducting Review by integrating a set of automated components ranging from programmatic searching to various text mining techniques.","Tue, 25 Apr 2017 06:33:39 UTC (287 KB)"
"159","Predicting Role Relevance with Minimal Domain Expertise in a Financial Domain","Mayank Kejriwal","Computation and Language (cs.CL)","Word embeddings have made enormous inroads in recent years in a wide variety of text mining applications. In this paper, we explore a word embedding-based architecture for predicting the relevance of a role between two financial entities within the context of natural language sentences. In this extended abstract, we propose a pooled approach that uses a collection of sentences to train word embeddings using the skip-gram word2vec architecture. We use the word embeddings to obtain context vectors that are assigned one or more labels based on manual annotations. We train a machine learning classifier using the labeled context vectors, and use the trained classifier to predict contextual role relevance on test data. Our approach serves as a good minimal-expertise baseline for the task as it is simple and intuitive, uses open-source modules, requires little feature crafting effort and performs well across roles.","Wed, 19 Apr 2017 00:55:23 UTC (787 KB)"
"160","MRA - Proof of Concept of a Multilingual Report Annotator Web Application","Luis Campos, Francisco Couto","Computation and Language (cs.CL)","MRA (Multilingual Report Annotator) is a web application that translates Radiology text and annotates it with RadLex terms. Its goal is to explore the solution of translating non-English Radiology reports as a way to solve the problem of most of the Text Mining tools being developed for English. In this brief paper we explain the language barrier problem and shortly describe the application. MRA can be found at this https URL .","Thu, 6 Apr 2017 08:32:16 UTC (136 KB)[v2] Fri, 7 Apr 2017 08:16:57 UTC (136 KB)[v3] Wed, 12 Apr 2017 13:15:48 UTC (136 KB)"
"161","Review on Requirements Modeling and Analysis for Self-Adaptive Systems: A Ten-Year Perspective","Zhuoqun Yang, Zhi Li, Zhi Jin, He Zhang","Software Engineering (cs.SE)","Context: Over the last decade, software researchers and engineers have developed a vast body of methodologies and technologies in requirements engineering for self-adaptive systems. Although existing studies have explored various aspects of this field, no systematic study has been performed on summarizing modeling methods and corresponding requirements activities. Objective: This study summarizes the state-of-the-art research trends, details the modeling methods and corresponding requirements activities, identifies relevant quality attributes and application domains and assesses the quality of each study. Method: We perform a systematic literature review underpinned by a rigorously established and reviewed protocol. To ensure the quality of the study, we choose 21 highly regarded publication venues and 8 popular digital libraries. In addition, we apply text mining to derive search strings and use Kappa coefficient to mitigate disagreements of researchers. Results: We selected 109 papers during the period of 2003-2013 and presented the research distributions over various kinds of factors. We extracted 29 modeling methods which are classified into 8 categories and identified 14 requirements activities which are classified into 4 requirements timelines. We captured 8 concerned software quality attributes based on the ISO 9126 standard and 12 application domains. Conclusion: The frequency of application of modeling methods varies greatly. Enterprise models were more widely used while behavior models were more rigorously evaluated. Requirements-driven runtime adaptation was the most frequently studied requirements activity. Activities at runtime were conveyed with more details. Finally, we draw other conclusions by discussing how well modeling dimensions were considered in these modeling methods and how well assurance dimensions were conveyed in requirements activities.","Mon, 3 Apr 2017 04:33:33 UTC (1,637 KB)"
"162","Opinion Mining on Non-English Short Text","Esra Akbas","Computation and Language (cs.CL)","As the type and the number of such venues increase, automated analysis of sentiment on textual resources has become an essential data mining task. In this paper, we investigate the problem of mining opinions on the collection of informal short texts. Both positive and negative sentiment strength of texts are detected. We focus on a non-English language that has few resources for text mining. This approach would help enhance the sentiment analysis in languages where a list of opinionated words does not exist. We propose a new method projects the text into dense and low dimensional feature vectors according to the sentiment strength of the words. We detect the mixture of positive and negative sentiments on a multi-variant scale. Empirical evaluation of the proposed framework on Turkish tweets shows that our approach gets good results for opinion mining.","Fri, 31 Mar 2017 18:05:44 UTC (52 KB)[v2] Tue, 4 Apr 2017 01:51:43 UTC (52 KB)"
"163","OncoScore: a novel, Internet-based tool to assess the oncogenic potential of genes","Rocco Piazza, Daniele Ramazzotti, Roberta Spinelli, Alessandra Pirola, Luca De Sano, Pierangelo Ferrari, Vera Magistroni, Nicoletta Cordani, Nitesh Sharma, Carlo Gambacorti-Passerini","Genomics (q-bio.GN)","The complicated, evolving landscape of cancer mutations poses a formidable challenge to identify cancer genes among the large lists of mutations typically generated in NGS experiments. The ability to prioritize these variants is therefore of paramount importance. To address this issue we developed OncoScore, a text-mining tool that ranks genes according to their association with cancer, based on available biomedical literature. Receiver operating characteristic curve and the area under the curve (AUC) metrics on manually curated datasets confirmed the excellent discriminating capability of OncoScore (OncoScore cut-off threshold = 21.09; AUC = 90.3%, 95% CI: 88.1-92.5%), indicating that OncoScore provides useful results in cases where an efficient prioritization of cancer-associated genes is needed.","Thu, 9 Mar 2017 01:24:23 UTC (948 KB)[v2] Fri, 7 Apr 2017 19:41:25 UTC (948 KB)"
"164","MetaPAD: Meta Pattern Discovery from Massive Text Corpora","Meng Jiang, Jingbo Shang, Taylor Cassidy, Xiang Ren, Lance M. Kaplan, Timothy P. Hanratty, Jiawei Han","Computation and Language (cs.CL)","Mining textual patterns in news, tweets, papers, and many other kinds of text corpora has been an active theme in text mining and NLP research. Previous studies adopt a dependency parsing-based pattern discovery approach. However, the parsing results lose rich context around entities in the patterns, and the process is costly for a corpus of large scale. In this study, we propose a novel typed textual pattern structure, called meta pattern, which is extended to a frequent, informative, and precise subsequence pattern in certain context. We propose an efficient framework, called MetaPAD, which discovers meta patterns from massive corpora with three techniques: (1) it develops a context-aware segmentation method to carefully determine the boundaries of patterns with a learnt pattern quality assessment function, which avoids costly dependency parsing and generates high-quality patterns; (2) it identifies and groups synonymous meta patterns from multiple facets---their types, contexts, and extractions; and (3) it examines type distributions of entities in the instances extracted by each group of patterns, and looks for appropriate type levels to make discovered patterns precise. Experiments demonstrate that our proposed framework discovers high-quality typed textual patterns efficiently from different genres of massive corpora and facilitates information extraction.","Mon, 13 Mar 2017 01:06:19 UTC (1,150 KB)[v2] Tue, 14 Mar 2017 20:26:32 UTC (1,150 KB)"
"165","Introduction to Formal Concept Analysis and Its Applications in Information Retrieval and Related Fields","Dmitry I. Ignatov","Information Retrieval (cs.IR)","This paper is a tutorial on Formal Concept Analysis (FCA) and its applications. FCA is an applied branch of Lattice Theory, a mathematical discipline which enables formalisation of concepts as basic units of human thinking and analysing data in the object-attribute form. Originated in early 80s, during the last three decades, it became a popular human-centred tool for knowledge representation and data analysis with numerous applications. Since the tutorial was specially prepared for RuSSIR 2014, the covered FCA topics include Information Retrieval with a focus on visualisation aspects, Machine Learning, Data Mining and Knowledge Discovery, Text Mining and several others.","Wed, 8 Mar 2017 12:53:21 UTC (3,541 KB)"
"166","LTSG: Latent Topical Skip-Gram for Mutually Learning Topic Model and Vector Representations","Jarvan Law, Hankz Hankui Zhuo, Junhua He, Erhu Rong (Dept. of Computer Science, Sun Yat-Sen University, GuangZhou, China.)","Computation and Language (cs.CL)","Topic models have been widely used in discovering latent topics which are shared across documents in text mining. Vector representations, word embeddings and topic embeddings, map words and topics into a low-dimensional and dense real-value vector space, which have obtained high performance in NLP tasks. However, most of the existing models assume the result trained by one of them are perfect correct and used as prior knowledge for improving the other model. Some other models use the information trained from external large corpus to help improving smaller corpus. In this paper, we aim to build such an algorithm framework that makes topic models and vector representations mutually improve each other within the same corpus. An EM-style algorithm framework is employed to iteratively optimize both topic model and vector representations. Experimental results show that our model outperforms state-of-art methods on various NLP tasks.","Thu, 23 Feb 2017 07:16:03 UTC (96 KB)"
"167","A Technical Report: Entity Extraction using Both Character-based and Token-based Similarity","Zeyi Wen, Dong Deng, Rui Zhang, Kotagiri Ramamohanarao","Databases (cs.DB)","Entity extraction is fundamental to many text mining tasks such as organisation name recognition. A popular approach to entity extraction is based on matching sub-string candidates in a document against a dictionary of entities. To handle spelling errors and name variations of entities, usually the matching is approximate and edit or Jaccard distance is used to measure dissimilarity between sub-string candidates and the entities. For approximate entity extraction from free text, existing work considers solely character-based or solely token-based similarity and hence cannot simultaneously deal with minor variations at token level and typos. In this paper, we address this problem by considering both character-based similarity and token-based similarity (i.e. two-level similarity). Measuring one-level (e.g. character-based) similarity is computationally expensive, and measuring two-level similarity is dramatically more expensive. By exploiting the properties of the two-level similarity and the weights of tokens, we develop novel techniques to significantly reduce the number of sub-string candidates that require computation of two-level similarity against the dictionary of entities. A comprehensive experimental study on real world datasets show that our algorithm can efficiently extract entities from documents and produce a high F1 score in the range of [0.91, 0.97].","Sun, 12 Feb 2017 12:46:40 UTC (89 KB)"
"168","Learning Concept Embeddings for Efficient Bag-of-Concepts Densification","Walid Shalaby, Wlodek Zadrozny","Computation and Language (cs.CL)","Explicit concept space models have proven efficacy for text representation in many natural language and text mining applications. The idea is to embed textual structures into a semantic space of concepts which captures the main ideas, objects, and the characteristics of these structures. The so called Bag of Concepts (BoC) representation suffers from data sparsity causing low similarity scores between similar texts due to low concept overlap. To address this problem, we propose two neural embedding models to learn continuous concept vectors. Once they are learned, we propose an efficient vector aggregation method to generate fully continuous BoC representations. We evaluate our concept embedding models on three tasks: 1) measuring entity semantic relatedness and ranking where we achieve 1.6% improvement in correlation scores, 2) dataless concept categorization where we achieve state-of-the-art performance and reduce the categorization error rate by more than 5% compared to five prior word and entity embedding models, and 3) dataless document classification where our models outperform the sparse BoC representations. In addition, by exploiting our efficient linear time vector aggregation method, we achieve better accuracy scores with much less concept dimensions compared to previous BoC densification methods which operate in polynomial time and require hundreds of dimensions in the BoC representation.","Fri, 10 Feb 2017 22:44:59 UTC (344 KB)[v2] Thu, 20 Dec 2018 02:25:15 UTC (1,369 KB)"
"169","Exact heat kernel on a hypersphere and its applications in kernel SVM","Chenchao Zhao, Jun S. Song","Machine Learning (stat.ML)","Many contemporary statistical learning methods assume a Euclidean feature space. This paper presents a method for defining similarity based on hyperspherical geometry and shows that it often improves the performance of support vector machine compared to other competing similarity measures. Specifically, the idea of using heat diffusion on a hypersphere to measure similarity has been previously proposed, demonstrating promising results based on a heuristic heat kernel obtained from the zeroth order parametrix expansion; however, how well this heuristic kernel agrees with the exact hyperspherical heat kernel remains unknown. This paper presents a higher order parametrix expansion of the heat kernel on a unit hypersphere and discusses several problems associated with this expansion method. We then compare the heuristic kernel with an exact form of the heat kernel expressed in terms of a uniformly and absolutely convergent series in high-dimensional angular momentum eigenmodes. Being a natural measure of similarity between sample points dwelling on a hypersphere, the exact kernel often shows superior performance in kernel SVM classifications applied to text mining, tumor somatic mutation imputation, and stock market analysis.","Sun, 5 Feb 2017 04:55:14 UTC (1,209 KB)[v2] Mon, 20 Nov 2017 04:39:59 UTC (1,087 KB)"
"170","On Practical Accuracy of Edit Distance Approximation Algorithms","Hiroyuki Hanada, Mineichi Kudo, Atsuyoshi Nakamura","Data Structures and Algorithms (cs.DS)","The edit distance is a basic string similarity measure used in many applications such as text mining, signal processing, bioinformatics, and so on. However, the computational cost can be a problem when we repeat many distance calculations as seen in real-life searching situations. A promising solution to cope with the problem is to approximate the edit distance by another distance with a lower computational cost. There are, indeed, many distances have been proposed for approximating the edit distance. However, their approximation accuracies are evaluated only theoretically: many of them are evaluated only with big-oh (asymptotic) notations, and without experimental analysis. Therefore, it is beneficial to know their actual performance in real applications. In this study we compared existing six approximation distances in two approaches: (i) we refined their theoretical approximation accuracy by calculating up to the constant coefficients, and (ii) we conducted some experiments, in one artificial and two real-life data sets, to reveal under which situations they perform best. As a result we obtained the following results: [Batu 2006] is the best theoretically and [Andoni 2010] experimentally. Theoretical considerations show that [Batu 2006] is the best if the string length n is large enough (n >= 300). [Andoni 2010] is experimentally the best for most data sets and theoretically the second best. [Bar-Yossef 2004], [Charikar 2006] and [Sokolov 2007], despite their middle-level theoretical performance, are experimentally as good as [Andoni 2010] for pairs of strings with large alphabet size.","Sun, 22 Jan 2017 07:40:52 UTC (1,402 KB)"
"171","Fuzzy Based Implicit Sentiment Analysis on Quantitative Sentences","Amir Hossein Yazdavar, Monireh Ebrahimi, Naomie Salim","Computation and Language (cs.CL)","With the rapid growth of social media on the web, emotional polarity computation has become a flourishing frontier in the text mining community. However, it is challenging to understand the latest trends and summarize the state or general opinions about products due to the big diversity and size of social media data and this creates the need of automated and real time opinion extraction and mining. On the other hand, the bulk of current research has been devoted to study the subjective sentences which contain opinion keywords and limited work has been reported for objective statements that imply sentiment. In this paper, fuzzy based knowledge engineering model has been developed for sentiment classification of special group of such sentences including the change or deviation from desired range or value. Drug reviews are the rich source of such statements. Therefore, in this research, some experiments were carried out on patient's reviews on several different cholesterol lowering drugs to determine their sentiment polarity. The main conclusion through this study is, in order to increase the accuracy level of existing drug opinion mining systems, objective sentences which imply opinion should be taken into account. Our experimental results demonstrate that our proposed model obtains over 72 percent F1 value.","Tue, 3 Jan 2017 19:41:24 UTC (1,049 KB)"
"172","The leveled approach. Using and evaluating text mining tools AVResearcherXL and Texcavator for historical research on public perceptions of drugs","Berrie van der Molen, Lars Buitinck, Toine Pieters","Human-Computer Interaction (cs.HC)","We introduce our explorative historical leveled approach that we use to understand drug debates in the Royal Dutch Library's digital newspaper archive. In this approach we alternate between distant reading and close reading. Furthermore, we use this approach to evaluate two text mining tools: AVResearcherXL and Texcavator.","Mon, 2 Jan 2017 12:16:11 UTC (135 KB)"
"173","PAMPO: using pattern matching and pos-tagging for effective Named Entities recognition in Portuguese","Conceicao Rocha, Alipio Jorge, Roberta Sionara, Paula Brito, Carlos Pimenta, Solange Rezende","Information Retrieval (cs.IR)","This paper deals with the entity extraction task (named entity recognition) of a text mining process that aims at unveiling non-trivial semantic structures, such as relationships and interaction between entities or communities. In this paper we present a simple and efficient named entity extraction algorithm. The method, named PAMPO (PAttern Matching and POs tagging based algorithm for NER), relies on flexible pattern matching, part-of-speech tagging and lexical-based rules. It was developed to process texts written in Portuguese, however it is potentially applicable to other languages as well. We compare our approach with current alternatives that support Named Entity Recognition (NER) for content written in Portuguese. These are Alchemy, Zemanta and Rembrandt. Evaluation of the efficacy of the entity extraction method on several texts written in Portuguese indicates a considerable improvement on $recall$ and $F_1$ measures.","Fri, 30 Dec 2016 17:10:29 UTC (131 KB)"
"174","Evaluating Marijuana-Related Tweets On Twitter","Anh Nguyen, Quang Hoang, Hung Nguyen, Dong Nguyen, Tuan Tran","Social and Information Networks (cs.SI)","This paper studies marijuana-related tweets in social network Twitter. We collected more than 300,000 marijuana related tweets during November 2016 in our study. Our text-mining based algorithms and data analysis unveil some interesting patterns including: (i) users' attitudes (e.g., positive or negative) can be characterized by the existence of outer links in a tweet; (ii) 67% users use their mobile phones to post their messages while many users publish their messages using third-party automatic posting services; and (3) the number of tweets during weekends is much higher than during weekdays. Our data also showed the impact of the political events such as the U.S. presidential election or state marijuana legalization votes on the marijuana-related tweeting frequencies.","Wed, 28 Dec 2016 16:11:28 UTC (740 KB)"
"175","""420 Friendly"": Revealing Marijuana Use via Craigslist Rental Ads","Anh Nguyen, Long Nguyen, Dong Nguyen, Uyen Le, Tuan Tran","Computers and Society (cs.CY)","Recent studies have shown that information mined from Craigslist can be used for informing public health policy or monitoring risk behavior. This paper presents a text-mining method for conducting public health surveillance of marijuana use concerns in the U.S. using online classified ads in Craigslist. We collected more than 200 thousands of rental ads in the housing categories in Craigslist and devised text-mining methods for efficiently and accurately extract rental ads associated with concerns about the uses of marijuana in different states across the U.S. We linked the extracted ads to their geographic locations and computed summary statistics of the ads having marijuana use concerns. Our data is then compared with the State Marijuana Laws Map published by the U.S. government and marijuana related keywords search in Google to verify our collected data with respect to the demographics of marijuana use concerns. Our data not only indicates strong correlations between Craigslist ads, Google search and the State Marijuana Laws Map in states where marijuana uses are legal, but also reveals some hidden world of marijuana use concerns in other states where marijuana use is illegal. Our approach can be utilized as a marijuana surveillance tool for policy makers to develop public health policy and regulations.","Thu, 22 Dec 2016 15:00:16 UTC (509 KB)[v2] Wed, 28 Dec 2016 16:13:48 UTC (509 KB)"
"176","Upper Bound of Bayesian Generalization Error in Non-negative Matrix Factorization","Naoki Hayashi, Sumio Watanabe","Statistics Theory (math.ST)","Non-negative matrix factorization (NMF) is a new knowledge discovery method that is used for text mining, signal processing, bioinformatics, and consumer analysis. However, its basic property as a learning machine is not yet clarified, as it is not a regular statistical model, resulting that theoretical optimization method of NMF has not yet established. In this paper, we study the real log canonical threshold of NMF and give an upper bound of the generalization error in Bayesian learning. The results show that the generalization error of the matrix factorization can be made smaller than regular statistical models if Bayesian learning is applied.","Tue, 13 Dec 2016 12:02:24 UTC (11 KB)[v2] Wed, 15 Feb 2017 07:24:03 UTC (14 KB)[v3] Wed, 22 Feb 2017 10:30:36 UTC (14 KB)[v4] Fri, 16 Jun 2017 03:54:56 UTC (14 KB)[v5] Sun, 1 Oct 2017 03:41:30 UTC (14 KB)"
"177","A New Spectral Method for Latent Variable Models","Matteo Ruffini, Marta Casanellas, Ricard Gavalda","Machine Learning (stat.ML)","This paper presents an algorithm for the unsupervised learning of latent variable models from unlabeled sets of data. We base our technique on spectral decomposition, providing a technique that proves to be robust both in theory and in practice. We also describe how to use this algorithm to learn the parameters of two well known text mining models: single topic model and Latent Dirichlet Allocation, providing in both cases an efficient technique to retrieve the parameters to feed the algorithm. We compare the results of our algorithm with those of existing algorithms on synthetic data, and we provide examples of applications to real world text corpora for both single topic model and LDA, obtaining meaningful results.","Sun, 11 Dec 2016 13:31:58 UTC (363 KB)[v2] Tue, 4 Apr 2017 09:06:11 UTC (265 KB)"
"178","The Evolution of Sentiment Analysis - A Review of Research Topics, Venues, and Top Cited Papers","Mika Viking Mantyla, Daniel Graziotin, Miikka Kuutila","Computation and Language (cs.CL)","Sentiment analysis is one of the fastest growing research areas in computer science, making it challenging to keep track of all the activities in the area. We present a computer-assisted literature review, where we utilize both text mining and qualitative coding, and analyze 6,996 papers from Scopus. We find that the roots of sentiment analysis are in the studies on public opinion analysis at the beginning of 20th century and in the text subjectivity analysis performed by the computational linguistics community in 1990's. However, the outbreak of computer-based sentiment analysis only occurred with the availability of subjective texts on the Web. Consequently, 99% of the papers have been published after 2004. Sentiment analysis papers are scattered to multiple publication venues, and the combined number of papers in the top-15 venues only represent ca. 30% of the papers in total. We present the top-20 cited papers from Google Scholar and Scopus and a taxonomy of research topics. In recent years, sentiment analysis has shifted from analyzing online product reviews to social media texts from Twitter and Facebook. Many topics beyond product reviews like stock markets, elections, disasters, medicine, software engineering and cyberbullying extend the utilization of sentiment analysis","Mon, 5 Dec 2016 21:28:06 UTC (1,732 KB)[v2] Tue, 21 Mar 2017 18:09:41 UTC (1,205 KB)[v3] Tue, 14 Nov 2017 10:40:58 UTC (1,682 KB)[v4] Tue, 21 Nov 2017 08:16:59 UTC (2,991 KB)"
"179","The emergence and evolution of the research fronts in HIV/AIDS research","David Fajardo-Ortiz, Malaquias Lopez-Cervantes, Luis Duran, Michel Dumontier, Miguel Lara, Hector Ochoa, Victor M Castano","Social and Information Networks (cs.SI)","In this paper, we have identified and analyzed the emergence, structure and dynamics of the paradigmatic research fronts that established the fundamentals of the biomedical knowledge on HIV/AIDS. A search of papers with the identifiers ""HIV/AIDS"", ""Human Immunodeficiency Virus"", ""HIV-1"" and ""Acquired Immunodeficiency Syndrome"" in the Web of Science (Thomson Reuters), was carried out. A citation network of those papers was constructed. Then, a sub-network of the papers with the highest number of inter-citations (with a minimal in-degree of 28) was selected to perform a combination of network clustering and text mining to identify the paradigmatic research fronts and analyze their dynamics. Thirteen research fronts were identified in this sub-network. The biggest and oldest front is related to the clinical knowledge on the disease in the patient. Nine of the fronts are related to the study of specific molecular structures and mechanisms and two of these fronts are related to the development of drugs. The rest of the fronts are related to the study of the disease at the cellular level. Interestingly, the emergence of these fronts occurred in successive ""waves"" over the time which suggest a transition in the paradigmatic focus. The emergence and evolution of the biomedical fronts in HIV/AIDS research is explained not just by the partition of the problem in elements and interactions leading to increasingly specialized communities, but also by changes in the technological context of this health problem and the dramatic changes in the epidemiological reality of HIV/AIDS that occurred between 1993 and 1995.","Wed, 16 Nov 2016 10:13:08 UTC (2,080 KB)[v2] Thu, 1 Jun 2017 01:33:31 UTC (1,241 KB)"
"180","SimDoc: Topic Sequence Alignment based Document Similarity Framework","Gaurav Maheshwari, Priyansh Trivedi, Harshita Sahijwani, Kunal Jha, Sourish Dasgupta, Jens Lehmann","Computation and Language (cs.CL)","Document similarity is the problem of estimating the degree to which a given pair of documents has similar semantic content. An accurate document similarity measure can improve several enterprise relevant tasks such as document clustering, text mining, and question-answering. In this paper, we show that a document's thematic flow, which is often disregarded by bag-of-word techniques, is pivotal in estimating their similarity. To this end, we propose a novel semantic document similarity framework, called SimDoc. We model documents as topic-sequences, where topics represent latent generative clusters of related words. Then, we use a sequence alignment algorithm to estimate their semantic similarity. We further conceptualize a novel mechanism to compute topic-topic similarity to fine tune our system. In our experiments, we show that SimDoc outperforms many contemporary bag-of-words techniques in accurately computing document similarity, and on practical applications such as document clustering.","Tue, 15 Nov 2016 13:31:28 UTC (323 KB)[v2] Sat, 11 Nov 2017 23:07:54 UTC (325 KB)"
"181","Using text mining and machine learning for detection of child abuse","Chintan Amrit, Tim Paauw, Robin Aly, Miha Lavric","Computers and Society (cs.CY)","Abuse in any form is a grave threat to a child's health. Public health institutions in the Netherlands try to identify and prevent different kinds of abuse, and building a decision support system can help such institutions achieve this goal. Such decision support relies on the analysis of relevant child health data. A significant part of the medical data that the institutions have on children is unstructured, and in the form of free text notes. In this research, we employ machine learning and text mining techniques to detect patterns of possible child abuse in the data. The resulting model achieves a high score in classifying cases of possible abuse. We then describe our implementation of the decision support API at a municipality in the Netherlands.","Fri, 11 Nov 2016 11:27:07 UTC (2,805 KB)[v2] Wed, 16 Nov 2016 09:57:41 UTC (2,805 KB)"
"182","Distributed Coordinate Descent for Generalized Linear Models with Regularization","Ilya Trofimov, Alexander Genkin","Machine Learning (stat.ML)","Generalized linear model with $L_1$ and $L_2$ regularization is a widely used technique for solving classification, class probability estimation and regression problems. With the numbers of both features and examples growing rapidly in the fields like text mining and clickstream data analysis parallelization and the use of cluster architectures becomes important. We present a novel algorithm for fitting regularized generalized linear models in the distributed environment. The algorithm splits data between nodes by features, uses coordinate descent on each node and line search to merge results globally. Convergence proof is provided. A modifications of the algorithm addresses slow node problem. For an important particular case of logistic regression we empirically compare our program with several state-of-the art approaches that rely on different algorithmic and data spitting methods. Experiments demonstrate that our approach is scalable and superior when training on large and sparse datasets.","Mon, 7 Nov 2016 15:19:54 UTC (1,684 KB)[v2] Mon, 26 Jun 2017 13:35:23 UTC (2,104 KB)"
"183","Rapid Prototyping of a Text Mining Application for Cryptocurrency Market Intelligence","Marek Laskowski, Henry M. Kim","Computers and Society (cs.CY)","Blockchain represents a technology for establishing a shared, immutable version of the truth between a network of participants that do not trust one another, and therefore has the potential to disrupt any financial or other industries that rely on third-parties to establish trust. Recent trends in computing including: prevalence of Free and Open Source Software (FOSS); easy access to High Performance Computing (HPC i.e. 'The Cloud'); and increasingly advanced analytics capabilities such as Natural Language Processing (NLP) and Machine Learning (ML) allow for rapidly prototyping applications for analysis of trends in the emergence of Blockchain technology. A scaleable proof-of-concept pipeline that lays the groundwork for analysis of multiple streams of semi-structured data posted on social media is demonstrated. Preliminary analysis and performance metrics are presented and discussed. Future work is described that will scale the system to cloud-based, real-time, analysis of multiple data streams, with Information Extraction (IE) (ex. sentiment analysis) and Machine Learning capability.","Sun, 28 Aug 2016 20:04:25 UTC (669 KB)"
"184","Scientific Literature Text Mining and the Case for Open Access","Gopal P. Sarma","Computers and Society (cs.CY)","""Open access"" has become a central theme of journal reform in academic publishing. In this article, I examine the relationship between open access publishing and an important infrastructural element of a modern research enterprise, scientific literature text mining, or the use of data analytic techniques to conduct meta-analyses and investigations into the scientific corpus. I give a brief history of the open access movement, discuss novel journalistic practices, and an overview of data-driven investigation of the scientific corpus. I argue that particularly in an era where the veracity of many research studies has been called into question, scientific literature text mining should be one of the key motivations for open access publishing, not only in the basic sciences, but in the engineering and applied sciences as well. The enormous benefits of unrestricted access to the research literature should prompt scholars from all disciplines to lend their vocal support to enabling legal, wholesale access to the scientific literature as part of a data science pipeline.","Tue, 1 Nov 2016 01:15:53 UTC (9 KB)[v2] Sun, 5 Mar 2017 14:59:40 UTC (9 KB)[v3] Mon, 23 Apr 2018 19:00:58 UTC (13 KB)"
"185","Clinical Text Prediction with Numerically Grounded Conditional Language Models","Georgios P. Spithourakis, Steffen E. Petersen, Sebastian Riedel","Computation and Language (cs.CL)","Assisted text input techniques can save time and effort and improve text quality. In this paper, we investigate how grounded and conditional extensions to standard neural language models can bring improvements in the tasks of word prediction and completion. These extensions incorporate a structured knowledge base and numerical values from the text into the context used to predict the next word. Our automated evaluation on a clinical dataset shows extended models significantly outperform standard models. Our best system uses both conditioning and grounding, because of their orthogonal benefits. For word prediction with a list of 5 suggestions, it improves recall from 25.03% to 71.28% and for word completion it improves keystroke savings from 34.35% to 44.81%, where theoretical bound for this dataset is 58.78%. We also perform a qualitative investigation of how models with lower perplexity occasionally fare better at the tasks. We found that at test time numbers have more influence on the document level than on individual word probabilities.","Thu, 20 Oct 2016 11:48:30 UTC (502 KB)"
"186","A New Data Representation Based on Training Data Characteristics to Extract Drug Named-Entity in Medical Text","Sadikin Mujiono, Mohamad Ivan Fanany, Chan Basaruddin","Computation and Language (cs.CL)","One essential task in information extraction from the medical corpus is drug name recognition. Compared with text sources come from other domains, the medical text is special and has unique characteristics. In addition, the medical text mining poses more challenges, e.g., more unstructured text, the fast growing of new terms addition, a wide range of name variation for the same drug. The mining is even more challenging due to the lack of labeled dataset sources and external knowledge, as well as multiple token representations for a single drug name that is more common in the real application setting. Although many approaches have been proposed to overwhelm the task, some problems remained with poor F-score performance (less than 0.75). This paper presents a new treatment in data representation techniques to overcome some of those challenges. We propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training. The first technique is evaluated with the standard NN model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked Denoising Encoders). The third technique represents the sentence as a sequence that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term Memory). In extracting the drug name entities, the third technique gives the best F-score performance compared to the state of the art, with its average F-score being 0.8645.","Thu, 6 Oct 2016 14:38:09 UTC (337 KB)"
"187","MPI-FAUN: An MPI-Based Framework for Alternating-Updating Nonnegative Matrix Factorization","Ramakrishnan Kannan, Grey Ballard, Haesun Park","Distributed, Parallel, and Cluster Computing (cs.DC)","Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors $W$ and $H$, for the given input matrix $A$, such that $A \approx W H$. NMF is a useful tool for many applications in different domains such as topic modeling in text mining, background separation in video analysis, and community detection in social networks. Despite its popularity in the data mining community, there is a lack of efficient parallel algorithms to solve the problem for big data sets. The main contribution of this work is a new, high-performance parallel computational framework for a broad class of NMF algorithms that iteratively solves alternating non-negative least squares (NLS) subproblems for $W$ and $H$. It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). The framework is flexible and able to leverage a variety of NMF and NLS algorithms, including Multiplicative Update, Hierarchical Alternating Least Squares, and Block Principal Pivoting. Our implementation allows us to benchmark and compare different algorithms on massive dense and sparse data matrices of size that spans for few hundreds of millions to billions. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements. The code and the datasets used for conducting the experiments are available online.","Wed, 28 Sep 2016 23:31:45 UTC (4,171 KB)"
"188","Psychologically Motivated Text Mining","Ekaterina Shutova, Patricia Lichtenstein","Computation and Language (cs.CL)","Natural language processing techniques are increasingly applied to identify social trends and predict behavior based on large text collections. Existing methods typically rely on surface lexical and syntactic information. Yet, research in psychology shows that patterns of human conceptualisation, such as metaphorical framing, are reliable predictors of human expectations and decisions. In this paper, we present a method to learn patterns of metaphorical framing from large text collections, using statistical techniques. We apply the method to data in three different languages and evaluate the identified patterns, demonstrating their psychological validity.","Wed, 28 Sep 2016 17:58:23 UTC (549 KB)"
"189","An Investigation of Recurrent Neural Architectures for Drug Name Recognition","Raghavendra Chalapathy, Ehsan Zare Borzeshi, Massimo Piccardi","Computation and Language (cs.CL)","Drug name recognition (DNR) is an essential step in the Pharmacovigilance (PV) pipeline. DNR aims to find drug name mentions in unstructured biomedical texts and classify them into predefined categories. State-of-the-art DNR approaches heavily rely on hand crafted features and domain specific resources which are difficult to collect and tune. For this reason, this paper investigates the effectiveness of contemporary recurrent neural architectures - the Elman and Jordan networks and the bidirectional LSTM with CRF decoding - at performing DNR straight from the text. The experimental results achieved on the authoritative SemEval-2013 Task 9.1 benchmarks show that the bidirectional LSTM-CRF ranks closely to highly-dedicated, hand-crafted systems.","Sat, 24 Sep 2016 08:45:17 UTC (16 KB)"
"190","Building accurate HAV exploiting User Profiling and Sentiment Analysis","Alan Ferrari, Angelo Consoli","Cryptography and Security (cs.CR)","Social Engineering (SE) is one of the most dangerous aspect an attacker can use against a given entity (private citizen, industry, government, ...). In order to perform SE attacks, it is necessary to collect as much information as possible about the target (or victim(s)). The aim of this paper is to report the details of an activity which took to the development of an automatic tool that extracts, categorizes and summarizes the target interests, thus possible weaknesses with respect to specific topics. Data is collected from the user's activity on social networks, parsed and analyzed using text mining techniques. The main contribution of the proposed tool consists in delivering some reports that allow the citizen, institutions as well as private bodies the screening of their exposure to SE attacks, with a strong awareness potential that will be reflected in a decrease of the risks and a good opportunity to save money.","Fri, 23 Sep 2016 10:35:28 UTC (216 KB)"
"191","What is Wrong with Topic Modeling? (and How to Fix it Using Search-based Software Engineering)","Amritanshu Agrawal, Wei Fu, Tim Menzies","Software Engineering (cs.SE)","Context: Topic modeling finds human-readable structures in unstructured textual data. A widely used topic modeler is Latent Dirichlet allocation. When run on different datasets, LDA suffers from ""order effects"" i.e. different topics are generated if the order of training data is shuffled. Such order effects introduce a systematic error for any study. This error can relate to misleading results;specifically, inaccurate topic descriptions and a reduction in the efficacy of text mining classification results. Objective: To provide a method in which distributions generated by LDA are more stable and can be used for further analysis. Method: We use LDADE, a search-based software engineering tool that tunes LDA's parameters using DE (Differential Evolution). LDADE is evaluated on data from a programmer information exchange site (Stackoverflow), title and abstract text of thousands ofSoftware Engineering (SE) papers, and software defect reports from NASA. Results were collected across different implementations of LDA (Python+Scikit-Learn, Scala+Spark); across different platforms (Linux, Macintosh) and for different kinds of LDAs (VEM,or using Gibbs sampling). Results were scored via topic stability and text mining classification accuracy. Results: In all treatments: (i) standard LDA exhibits very large topic instability; (ii) LDADE's tunings dramatically reduce cluster instability; (iii) LDADE also leads to improved performances for supervised as well as unsupervised learning. Conclusion: Due to topic instability, using standard LDA with its ""off-the-shelf"" settings should now be depreciated. Also, in future, we should require SE papers that use LDA to test and (if needed) mitigate LDA topic instability. Finally, LDADE is a candidate technology for effectively and efficiently reducing that instability.","Mon, 29 Aug 2016 18:45:00 UTC (3,893 KB)[v2] Wed, 8 Feb 2017 01:19:06 UTC (4,153 KB)[v3] Wed, 8 Nov 2017 04:49:42 UTC (6,964 KB)[v4] Tue, 20 Feb 2018 17:26:51 UTC (6,966 KB)"
"192","Finding Trends in Software Research","George Mathew, Amritanshu Agrawal, Tim Menzies","Software Engineering (cs.SE)","This paper explores the structure of research papers in software engineering. Using text mining, we study 35,391 software engineering (SE) papers from 34 leading SE venues over the last 25 years. These venues were divided, nearly evenly, between conferences and journals. An important aspect of this analysis is that it is fully automated and repeatable. To achieve that automation, we used a stable topic modeling technique called LDADE that fully automates parameter tuning in LDA. Using LDADE, we mine 11 topics that represent much of the structure of contemporary SE. The 11 topics presented here should not be ""set in stone"" as the only topics worthy of study in SE. Rather our goal is to report that (a) text mining methods can detect large scale trends within our community; (b) those topic change with time; so (c) it is important to have automatic agents that can update our understanding of our community whenever new data arrives.","Mon, 29 Aug 2016 15:17:09 UTC (1,199 KB)[v2] Tue, 30 Aug 2016 01:34:40 UTC (1,199 KB)[v3] Tue, 10 Jan 2017 04:05:43 UTC (538 KB)[v4] Sat, 14 Jan 2017 02:22:51 UTC (537 KB)[v5] Fri, 7 Jul 2017 20:35:03 UTC (573 KB)[v6] Thu, 5 Apr 2018 04:35:03 UTC (563 KB)[v7] Mon, 3 Sep 2018 08:05:52 UTC (571 KB)[v8] Mon, 17 Sep 2018 03:13:47 UTC (1,650 KB)[v9] Tue, 18 Sep 2018 01:29:48 UTC (1,649 KB)[v10] Wed, 3 Oct 2018 01:42:46 UTC (1,649 KB)"
"193","Sequence Graph Transform (SGT): A Feature Extraction Function for Sequence Data Mining (Extended Version)","Chitta Ranjan, Samaneh Ebrahimi, Kamran Paynabar","Machine Learning (stat.ML)","The ubiquitous presence of sequence data across fields such as the web, healthcare, bioinformatics, and text mining has made sequence mining a vital research area. However, sequence mining is particularly challenging because of difficulty in finding (dis)similarity/distance between sequences. This is because a distance measure between sequences is not obvious due to their unstructuredness---arbitrary strings of arbitrary length. Feature representations, such as n-grams, are often used but they either compromise on extracting both short- and long-term sequence patterns or have a high computation. We propose a new function, Sequence Graph Transform (SGT), that extracts the short- and long-term sequence features and embeds them in a finite-dimensional feature space. Importantly, SGT has low computation and can extract any amount of short- to long-term patterns without any increase in the computation, also proved theoretically in this paper. Due to this, SGT yields superior result with significantly higher accuracy and lower computation compared to the existing methods. We show it via several experimentation and SGT's real world application for clustering, classification, search and visualization as examples.","Thu, 11 Aug 2016 16:59:19 UTC (1,852 KB)[v2] Fri, 12 Aug 2016 14:01:03 UTC (1,852 KB)[v3] Tue, 23 Aug 2016 20:03:41 UTC (1,853 KB)[v4] Wed, 28 Sep 2016 00:20:49 UTC (679 KB)[v5] Wed, 19 Oct 2016 05:04:59 UTC (419 KB)[v6] Sun, 27 Nov 2016 01:43:12 UTC (435 KB)[v7] Wed, 30 Nov 2016 06:35:26 UTC (435 KB)[v8] Tue, 31 Jan 2017 03:50:58 UTC (434 KB)[v9] Sun, 30 Apr 2017 07:21:43 UTC (568 KB)"
"194","Levy NMF for robust nonnegative source separation","Paul Magron, Roland Badeau, Antoine Liutkus","Sound (cs.SD)","Source separation, which consists in decomposing data into meaningful structured components, is an active research topic in many areas, such as music and image signal processing, applied physics and text mining. In this paper, we introduce the Positive $\alpha$-stable (P$\alpha$S) distributions to model the latent sources, which are a subclass of the stable distributions family. They notably permit us to model random variables that are both nonnegative and impulsive. Considering the Levy distribution, the only P$\alpha$S distribution whose density is tractable, we propose a mixture model called Levy Nonnegative Matrix Factorization (Levy NMF). This model accounts for low-rank structures in nonnegative data that possibly has high variability or is corrupted by very adverse noise. The model parameters are estimated in a maximum-likelihood sense. We also derive an estimator of the sources given the parameters, which extends the validity of the generalized Wiener filtering to the P$\alpha$S case. Experiments on synthetic data show that Levy NMF compares favorably with state-of-the art techniques in terms of robustness to impulsive noise. The analysis of two types of realistic signals is also considered: musical spectrograms and fluorescence spectra of chemical species. The results highlight the potential of the Levy NMF model for decomposing nonnegative data.","Fri, 5 Aug 2016 11:48:58 UTC (436 KB)[v2] Tue, 8 Nov 2016 12:25:32 UTC (367 KB)"
"195","Leveraging Unstructured Data to Detect Emerging Reliability Issues","Deovrat Kakde, Arin Chaudhuri","Artificial Intelligence (cs.AI)","Unstructured data refers to information that does not have a predefined data model or is not organized in a pre-defined manner. Loosely speaking, unstructured data refers to text data that is generated by humans. In after-sales service businesses, there are two main sources of unstructured data: customer complaints, which generally describe symptoms, and technician comments, which outline diagnostics and treatment information. A legitimate customer complaint can eventually be tracked to a failure or a claim. However, there is a delay between the time of a customer complaint and the time of a failure or a claim. A proactive strategy aimed at analyzing customer complaints for symptoms can help service providers detect reliability problems in advance and initiate corrective actions such as recalls. This paper introduces essential text mining concepts in the context of reliability analysis and a method to detect emerging reliability issues. The application of the method is illustrated using a case study.","Tue, 26 Jul 2016 15:19:12 UTC (373 KB)"
"196","Representation of texts as complex networks: a mesoscopic approach","Henrique F. de Arruda, Filipi N. Silva, Vanessa Q. Marinho, Diego R. Amancio, Luciano da F. Costa","Computation and Language (cs.CL)","Statistical techniques that analyze texts, referred to as text analytics, have departed from the use of simple word count statistics towards a new paradigm. Text mining now hinges on a more sophisticated set of methods, including the representations in terms of complex networks. While well-established word-adjacency (co-occurrence) methods successfully grasp syntactical features of written texts, they are unable to represent important aspects of textual data, such as its topical structure, i.e. the sequence of subjects developing at a mesoscopic level along the text. Such aspects are often overlooked by current methodologies. In order to grasp the mesoscopic characteristics of semantical content in written texts, we devised a network model which is able to analyze documents in a multi-scale fashion. In the proposed model, a limited amount of adjacent paragraphs are represented as nodes, which are connected whenever they share a minimum semantical content. To illustrate the capabilities of our model, we present, as a case example, a qualitative analysis of ""Alice's Adventures in Wonderland"". We show that the mesoscopic structure of a document, modeled as a network, reveals many semantic traits of texts. Such an approach paves the way to a myriad of semantic-based applications. In addition, our approach is illustrated in a machine learning context, in which texts are classified among real texts and randomized instances.","Thu, 30 Jun 2016 19:47:17 UTC (846 KB)[v2] Sat, 25 Feb 2017 00:06:48 UTC (1,508 KB)"
"197","How marketing vocabulary was evolving from 2005 to 2014? An illustrative application of statistical methods on text mining","Igor Barahona, Daria Micaela Hernandez, Hector Hugo Perez-Villarreal","Digital Libraries (cs.DL)","Here a collection of 1169 abstracts, which corresponds to articles that the Journal of Marketing Research has published from 2005 to 2014, are analysed under a novel approach. We apply several statistical methods, such as Principal Components Analysis and Correspondence Analysis to identify the way Marketing vocabulary is evolving. Similarly those articles that introduce new vocabulary are identified and the preferred words by authors are also detected. In order to provide an easy-to-understand explanation, we provide our results graphically. A word-cloud with the most frequent words is given first. Secondly abstracts-words are represented on the factorial plane. Finally one representation of word-years allows us to detect changes on the vocabulary through the passing of time.","Fri, 5 Feb 2016 18:06:26 UTC (368 KB)"
"198","Models Coupling Urban Growth and Transportation Network Growth : An Algorithmic Systematic Review Approach","Juste Raimbault","Digital Libraries (cs.DL)","A broad bibliographical study suggests a scarcity of quantitative models of simulation integrating both network and urban growth. This absence may be due to diverging interests of concerned disciplines, resulting in a lack of communication. We propose to proceed to an algorithmic systematic review to give quantitative elements of answer to this question. A formal iterative algorithm to retrieve corpuses of references from initial keywords, based on text-mining, is developed and implemented. We study its convergence properties and do a sensitivity analysis. We then apply it on queries representative of the specific question, for which results tend to confirm the assumption of disciplines compartmentalisation.","Sat, 28 May 2016 12:55:49 UTC (576 KB)"
"199","SS4MCT: A Statistical Stemmer for Morphologically Complex Texts","Javid Dadashkarimi, Hossein Nasr Esfahani, Heshaam Faili, Azadeh Shakery","Information Retrieval (cs.IR)","There have been multiple attempts to resolve various inflection matching problems in information retrieval. Stemming is a common approach to this end. Among many techniques for stemming, statistical stemming has been shown to be effective in a number of languages, particularly highly inflected languages. In this paper we propose a method for finding affixes in different positions of a word. Common statistical techniques heavily rely on string similarity in terms of prefix and suffix matching. Since infixes are common in irregular/informal inflections in morphologically complex texts, it is required to find infixes for stemming. In this paper we propose a method whose aim is to find statistical inflectional rules based on minimum edit distance table of word pairs and the likelihoods of the rules in a language. These rules are used to statistically stem words and can be used in different text mining tasks. Experimental results on CLEF 2008 and CLEF 2009 English-Persian CLIR tasks indicate that the proposed method significantly outperforms all the baselines in terms of MAP.","Wed, 25 May 2016 12:25:26 UTC (1,026 KB)[v2] Mon, 20 Jun 2016 21:37:19 UTC (1,019 KB)"
"200","As Cool as a Cucumber: Towards a Corpus of Contemporary Similes in Serbian","Nikola Milosevic, Goran Nenadic","Computation and Language (cs.CL)","Similes are natural language expressions used to compare unlikely things, where the comparison is not taken literally. They are often used in everyday communication and are an important part of cultural heritage. Having an up-to-date corpus of similes is challenging, as they are constantly coined and/or adapted to the contemporary times. In this paper we present a methodology for semi-automated collection of similes from the world wide web using text mining techniques. We expanded an existing corpus of traditional similes (containing 333 similes) by collecting 446 additional expressions. We, also, explore how crowdsourcing can be used to extract and curate new similes.","Fri, 20 May 2016 12:20:27 UTC (83 KB)"
"201","Text-mining the NeuroSynth corpus using Deep Boltzmann Machines","Ricardo Pio Monti, Romy Lorenz, Robert Leech, Christoforos Anagnostopoulos, Giovanni Montana","Machine Learning (cs.LG)","Large-scale automated meta-analysis of neuroimaging data has recently established itself as an important tool in advancing our understanding of human brain function. This research has been pioneered by NeuroSynth, a database collecting both brain activation coordinates and associated text across a large cohort of neuroimaging research papers. One of the fundamental aspects of such meta-analysis is text-mining. To date, word counts and more sophisticated methods such as Latent Dirichlet Allocation have been proposed. In this work we present an unsupervised study of the NeuroSynth text corpus using Deep Boltzmann Machines (DBMs). The use of DBMs yields several advantages over the aforementioned methods, principal among which is the fact that it yields both word and document embeddings in a high-dimensional vector space. Such embeddings serve to facilitate the use of traditional machine learning techniques on the text corpus. The proposed DBM model is shown to learn embeddings with a clear semantic structure.","Sun, 1 May 2016 09:01:13 UTC (1,296 KB)"
"202","Liposomes versus metallic nanostructures: differences in the process of knowledge translation in cancer","David Fajardo-Ortiz, Luis Duran, Laura Moreno, Hector Ochoa, Victor-M Castano","Digital Libraries (cs.DL)","This research maps the knowledge translation process for two different types of nanotechnologies applied to cancer: liposomes and metallic nanostructures (MNs). We performed a structural analysis of citation networks and text mining supported in controlled vocabularies. In the case of liposomes, our results identify subnetworks (invisible colleges) associated with different therapeutic strategies: nanopharmacology, hyperthermia, and gene therapy. Only in the pharmacological strategy was an organized knowledge translation process identified, which, however, is monopolized by the liposomal doxorubicins. In the case of MNs, subnetworks are not differentiated by the type of therapeutic strategy, and the content of the documents is still basic research. Research on MNs is highly focused on developing a combination of molecular imaging and photothermal therapy.","Tue, 12 Apr 2016 15:06:02 UTC (1,993 KB)"
"203","Accessing accurate documents by mining auxiliary document information","Jinju Joby, Jyothi Korra","Information Retrieval (cs.IR)","Earlier techniques of text mining included algorithms like k-means, Naive Bayes, SVM which classify and cluster the text document for mining relevant information about the documents. The need for improving the mining techniques has us searching for techniques using the available algorithms. This paper proposes one technique which uses the auxiliary information that is present inside the text documents to improve the mining. This auxiliary information can be a description to the content. This information can be either useful or completely useless for mining. The user should assess the worth of the auxiliary information before considering this technique for text mining. In this paper, a combination of classical clustering algorithms is used to mine the datasets. The algorithm runs in two stages which carry out mining at different levels of abstraction. The clustered documents would then be classified based on the necessary groups. The proposed technique is aimed at improved results of document clustering.","Fri, 15 Apr 2016 16:27:38 UTC (891 KB)"
"204","Semantic Properties of Customer Sentiment in Tweets","Eun Hee Ko, Diego Klabjan","Computation and Language (cs.CL)","An increasing number of people are using online social networking services (SNSs), and a significant amount of information related to experiences in consumption is shared in this new media form. Text mining is an emerging technique for mining useful information from the web. We aim at discovering in particular tweets semantic patterns in consumers' discussions on social media. Specifically, the purposes of this study are twofold: 1) finding similarity and dissimilarity between two sets of textual documents that include consumers' sentiment polarities, two forms of positive vs. negative opinions and 2) driving actual content from the textual data that has a semantic trend. The considered tweets include consumers opinions on US retail companies (e.g., Amazon, Walmart). Cosine similarity and K-means clustering methods are used to achieve the former goal, and Latent Dirichlet Allocation (LDA), a popular topic modeling algorithm, is used for the latter purpose. This is the first study which discover semantic properties of textual data in consumption context beyond sentiment analysis. In addition to major findings, we apply LDA (Latent Dirichlet Allocations) to the same data and drew latent topics that represent consumers' positive opinions and negative opinions on social media.","Thu, 24 Mar 2016 15:22:52 UTC (623 KB)"
"205","The Anatomy of a Search and Mining System for Digital Archives","Martyn Harris, Mark Levene, Dell Zhang, Dan Levene","Digital Libraries (cs.DL)","Samtla (Search And Mining Tools with Linguistic Analysis) is a digital humanities system designed in collaboration with historians and linguists to assist them with their research work in quantifying the content of any textual corpora through approximate phrase search and document comparison. The retrieval engine uses a character-based n-gram language model rather than the conventional word-based one so as to achieve great flexibility in language agnostic query processing. The index is implemented as a space-optimised character-based suffix tree with an accompanying database of document content and metadata. A number of text mining tools are integrated into the system to allow researchers to discover textual patterns, perform comparative analysis, and find out what is currently popular in the research community. Herein we describe the system architecture, user interface, models and algorithms, and data storage of the Samtla system. We also present several case studies of its usage in practice together with an evaluation of the systems' ranking performance through crowdsourcing.","Wed, 23 Mar 2016 12:02:12 UTC (5,388 KB)"
"206","Mining Valence, Arousal, and Dominance - Possibilities for Detecting Burnout and Productivity?","Mika Mantyla, Bram Adams, Giuseppe Destefanis, Daniel Graziotin, Marco Ortu","Software Engineering (cs.SE)","Similar to other industries, the software engineering domain is plagued by psychological diseases such as burnout, which lead developers to lose interest, exhibit lower activity and/or feel powerless. Prevention is essential for such diseases, which in turn requires early identification of symptoms. The emotional dimensions of Valence, Arousal and Dominance (VAD) are able to derive a person's interest (attraction), level of activation and perceived level of control for a particular situation from textual communication, such as emails. As an initial step towards identifying symptoms of productivity loss in software engineering, this paper explores the VAD metrics and their properties on 700,000 Jira issue reports containing over 2,000,000 comments, since issue reports keep track of a developer's progress on addressing bugs or new features. Using a general-purpose lexicon of 14,000 English words with known VAD scores, our results show that issue reports of different type (e.g., Feature Request vs. Bug) have a fair variation of Valence, while increase in issue priority (e.g., from Minor to Critical) typically increases Arousal. Furthermore, we show that as an issue's resolution time increases, so does the arousal of the individual the issue is assigned to. Finally, the resolution of an issue increases valence, especially for the issue Reporter and for quickly addressed issues. The existence of such relations between VAD and issue report activities shows promise that text mining in the future could offer an alternative way for work health assessment surveys.","Mon, 14 Mar 2016 14:50:39 UTC (361 KB)"
"207","Intrusion Detection A Text Mining Based Approach","Gunupudi RajeshKumar, N Mangathayaru, G Narsimha","Cryptography and Security (cs.CR)","Intrusion Detection is one of major threats for organization. The approach of intrusion detection using text processing has been one of research interests which is gaining significant importance from researchers. In text mining based approach for intrusion detection, system calls serve as source for mining and predicting possibility of intrusion or attack. When an application runs, there might be several system calls which are initiated in the background. These system calls form the strong basis and the deciding factor for intrusion detection. In this paper, we mainly discuss the approach for intrusion detection by designing a distance measure which is designed by taking into consideration the conventional Gaussian function and modified to suit the need for similarity function. A Framework for intrusion detection is also discussed as part of this research.","Sat, 12 Mar 2016 01:12:08 UTC (994 KB)"
"208","Corpus analysis without prior linguistic knowledge - unsupervised mining of phrases and subphrase structure","Stefan Gerdjikov, Klaus U. Schulz","Computation and Language (cs.CL)","When looking at the structure of natural language, ""phrases"" and ""words"" are central notions. We consider the problem of identifying such ""meaningful subparts"" of language of any length and underlying composition principles in a completely corpus-based and language-independent way without using any kind of prior linguistic knowledge. Unsupervised methods for identifying ""phrases"", mining subphrase structure and finding words in a fully automated way are described. This can be considered as a step towards automatically computing a ""general dictionary and grammar of the corpus"". We hope that in the long run variants of our approach turn out to be useful for other kind of sequence data as well, such as, e.g., speech, genom sequences, or music annotation. Even if we are not primarily interested in immediate applications, results obtained for a variety of languages show that our methods are interesting for many practical tasks in text mining, terminology extraction and lexicography, search engine technology, and related fields.","Thu, 18 Feb 2016 12:08:05 UTC (679 KB)"
"209","Scalable Text Mining with Sparse Generative Models","Antti Puurula","Information Retrieval (cs.IR)","The information age has brought a deluge of data. Much of this is in text form, insurmountable in scope for humans and incomprehensible in structure for computers. Text mining is an expanding field of research that seeks to utilize the information contained in vast document collections. General data mining methods based on machine learning face challenges with the scale of text data, posing a need for scalable text mining methods. This thesis proposes a solution to scalable text mining: generative models combined with sparse computation. A unifying formalization for generative text models is defined, bringing together research traditions that have used formally equivalent models, but ignored parallel developments. This framework allows the use of methods developed in different processing tasks such as retrieval and classification, yielding effective solutions across different text mining tasks. Sparse computation using inverted indices is proposed for inference on probabilistic models. This reduces the computational complexity of the common text mining operations according to sparsity, yielding probabilistic models with the scalability of modern search engines. The proposed combination provides sparse generative models: a solution for text mining that is general, effective, and scalable. Extensive experimentation on text classification and ranked retrieval datasets are conducted, showing that the proposed solution matches or outperforms the leading task-specific methods in effectiveness, with a order of magnitude decrease in classification times for Wikipedia article categorization with a million classes. The developed methods were further applied in two 2014 Kaggle data mining prize competitions with over a hundred competing teams, earning first and second places.","Sun, 7 Feb 2016 02:49:27 UTC (4,651 KB)"
"210","Marvin: Semantic annotation using multiple knowledge sources","Nikola Milosevic","Artificial Intelligence (cs.AI)","People are producing more written material then anytime in the history. The increase is so high that professionals from the various fields are no more able to cope with this amount of publications. Text mining tools can offer tools to help them and one of the tools that can aid information retrieval and information extraction is semantic text annotation. In this report we present Marvin, a text annotator written in Java, which can be used as a command line tool and as a Java library. Marvin is able to annotate text using multiple sources, including WordNet, MetaMap, DBPedia and thesauri represented as SKOS.","Mon, 1 Feb 2016 13:27:34 UTC (217 KB)[v2] Tue, 2 Feb 2016 11:31:17 UTC (218 KB)"
"211","A multiple instance learning approach for sequence data with across bag dependencies","Manel Zoghlami, Sabeur Aridhi, Haitham Sghaier, Mondher Maddouri, Engelbert Mephu Nguifo","Machine Learning (cs.LG)","In Multiple Instance Learning (MIL) problem for sequence data, the learning data consist of a set of bags where each bag contains a set of instances/sequences. In many real world applications such as bioinformatics, web mining, and text mining, comparing a random couple of sequences makes no sense. In fact, each instance of each bag may have structural and/or temporal relation with other instances in other bags. Thus, the classification task should take into account the relation between semantically related instances across bags. In this paper, we present two novel MIL approaches for sequence data classification: (1) ABClass and (2) ABSim. In ABClass, each sequence is represented by one vector of attributes. For each sequence of the unknown bag, a discriminative classifier is applied in order to compute a partial classification result. Then, an aggregation method is applied to these partial results in order to generate the final result. In ABSim, we use a similarity measure between each sequence of the unknown bag and the corresponding sequences in the learning bags. An unknown bag is labeled with the bag that presents more similar sequences. We applied both approaches to the problem of bacterial Ionizing Radiation Resistance (IRR) prediction. We evaluated and discussed the proposed approaches on well known Ionizing Radiation Resistance Bacteria (IRRB) and Ionizing Radiation Sensitive Bacteria (IRSB) represented by primary structure of basal DNA repair proteins. The experimental results show that both ABClass and ABSim approaches are efficient.","Sat, 30 Jan 2016 21:15:10 UTC (302 KB)"
"212","Using Text Mining To Analyze Real Estate Classifieds","Sherief Abdallah","Information Retrieval (cs.IR)","Many brokers have adapted their operation to exploit the potential of the web. Despite the importance of the real estate classifieds, there has been little work in analyzing such data. In this paper we propose a two-stage regression model that exploits the textual data in real estate classifieds. We show how our model can be used to predict the price of a real estate classified. We also show how our model can be used to highlight keywords that affect the price positively or negatively. To assess our contributions, we analyze four real world data sets, which we gathered from three different property websites. The analysis shows that our model (which exploits textual features) achieves significantly lower root mean squared error across the different data sets and against variety of regression models.","Sun, 15 Nov 2015 09:00:56 UTC (121 KB)"
"213","Bayesian Analysis of Dynamic Linear Topic Models","Chris Glynn, Surya T. Tokdar, David L. Banks, Brian Howard","Machine Learning (stat.ML)","In dynamic topic modeling, the proportional contribution of a topic to a document depends on the temporal dynamics of that topic's overall prevalence in the corpus. We extend the Dynamic Topic Model of Blei and Lafferty (2006) by explicitly modeling document level topic proportions with covariates and dynamic structure that includes polynomial trends and periodicity. A Markov Chain Monte Carlo (MCMC) algorithm that utilizes Polya-Gamma data augmentation is developed for posterior inference. Conditional independencies in the model and sampling are made explicit, and our MCMC algorithm is parallelized where possible to allow for inference in large corpora. To address computational bottlenecks associated with Polya-Gamma sampling, we appeal to the Central Limit Theorem to develop a Gaussian approximation to the Polya-Gamma random variable. This approximation is fast and reliable for parameter values relevant in the text mining domain. Our model and inference algorithm are validated with multiple simulation examples, and we consider the application of modeling trends in PubMed abstracts. We demonstrate that sharing information across documents is critical for accurately estimating document-specific topic proportions. We also show that explicitly modeling polynomial and periodic behavior improves our ability to predict topic prevalence at future time points.","Thu, 12 Nov 2015 16:26:13 UTC (1,067 KB)"
"214","Social Media Analysis for Product Safety using Text Mining and Sentiment Analysis","Haruna Isah, Daniel Neagu, Paul Trundle","Social and Information Networks (cs.SI)","The growing incidents of counterfeiting and associated economic and health consequences necessitate the development of active surveillance systems capable of producing timely and reliable information for all stake holders in the anti-counterfeiting fight. User generated content from social media platforms can provide early clues about product allergies, adverse events and product counterfeiting. This paper reports a work in progresswith contributions including: the development of a framework for gathering and analyzing the views and experiences of users of drug and cosmetic products using machine learning, text mining and sentiment analysis, the application of the proposed framework on Facebook comments and data from Twitter for brand analysis, and the description of how to develop a product safety lexicon and training data for modeling a machine learning classifier for drug and cosmetic product sentiment prediction. The initial brand and product comparison results signify the usefulness of text mining and sentiment analysis on social media data while the use of machine learning classifier for predicting the sentiment orientation provides a useful tool for users, product manufacturers, regulatory and enforcement agencies to monitor brand or product sentiment trends in order to act in the event of sudden or significant rise in negative sentiment.","Sun, 18 Oct 2015 20:05:08 UTC (2,852 KB)"
"215","A Novel Approach to Document Classification using WordNet","Koushiki Sarkar, Ritwika Law","Information Retrieval (cs.IR)","Content based Document Classification is one of the biggest challenges in the context of free text mining. Current algorithms on document classifications mostly rely on cluster analysis based on bag-of-words approach. However that method is still being applied to many modern scientific dilemmas. It has established a strong presence in fields like economics and social science to merit serious attention from the researchers. In this paper we would like to propose and explore an alternative grounded more securely on the dictionary classification and correlatedness of words and phrases. It is expected that application of our existing knowledge about the underlying classification structure may lead to improvement of the classifier's performance.","Sun, 4 Oct 2015 09:24:27 UTC (774 KB)[v2] Sat, 12 Dec 2015 17:56:52 UTC (781 KB)"
"216","A High-Performance Parallel Algorithm for Nonnegative Matrix Factorization","Ramakrishnan Kannan, Grey Ballard, Haesun Park","Distributed, Parallel, and Cluster Computing (cs.DC)","Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors $W$ and $H$, for the given input matrix $A$, such that $A \approx W H$. NMF is a useful tool for many applications in different domains such as topic modeling in text mining, background separation in video analysis, and community detection in social networks. Despite its popularity in the data mining community, there is a lack of efficient parallel software to solve the problem for big datasets. Existing distributed-memory algorithms are limited in terms of performance and applicability, as they are implemented using Hadoop and are designed only for sparse matrices. We propose a distributed-memory parallel algorithm that computes the factorization by iteratively solving alternating non-negative least squares (NLS) subproblems for $W$ and $H$. To our knowledge, our algorithm is the first high-performance parallel algorithm for NMF. It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). As opposed to previous implementations, our algorithm is also flexible: (1) it performs well for dense and sparse matrices, and (2) it allows the user to choose from among multiple algorithms for solving local NLS subproblems within the alternating iterations. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements.","Wed, 30 Sep 2015 19:47:39 UTC (468 KB)"
"217","Simple Text Mining for Sentiment Analysis of Political Figure Using Naive Bayes Classifier Method","Yustinus Eko Soelistio, Martinus Raditia Sigit Surendra","Computation and Language (cs.CL)","Text mining can be applied to many fields. One of the application is using text mining in digital newspaper to do politic sentiment analysis. In this paper sentiment analysis is applied to get information from digital news articles about its positive or negative sentiment regarding particular politician. This paper suggests a simple model to analyze digital newspaper sentiment polarity using naive Bayes classifier method. The model uses a set of initial data to begin with which will be updated when new information appears. The model showed promising result when tested and can be implemented to some other sentiment analysis problems.","Fri, 21 Aug 2015 01:40:54 UTC (592 KB)"
"218","Social Influence in the Concurrent Diffusion of Information and Behaviors in Online Social Networks","Kang Zhao, Shiyao Wang, Ion B. Vasi, Qi Zhang","Social and Information Networks (cs.SI)","The emergence of online social networks has greatly facilitated the diffusion of information and behaviors. While the two diffusion processes are often intertwined, ""talking the talk"" does not necessarily mean ""walking the talk""--those who share information about an action may not actually participate in it. We do not know if the diffusion of information and behaviors are similar, or if social influence plays an equally important role in these processes. Integrating text mining, social network analyses, and survival analysis, this research examines the concurrent spread of information and behaviors related to the Ice Bucket Challenge on Twitter. We show that the two processes follow different patterns. Unilateral social influence contributes to the diffusion of information, but not to the diffusion of behaviors; bilateral influence conveyed via the communication process is a significant and positive predictor of the diffusion of behaviors, but not of information. These results have implications for theories of social influence, social networks, and contagion.","Tue, 18 Aug 2015 19:33:58 UTC (537 KB)[v2] Tue, 23 Jan 2018 14:30:49 UTC (845 KB)"
"219","Improving Decision Analytics with Deep Learning: The Case of Financial Disclosures","Stefan Feuerriegel, Ralph Fehrer","Machine Learning (stat.ML)","Decision analytics commonly focuses on the text mining of financial news sources in order to provide managerial decision support and to predict stock market movements. Existing predictive frameworks almost exclusively apply traditional machine learning methods, whereas recent research indicates that traditional machine learning methods are not sufficiently capable of extracting suitable features and capturing the non-linear nature of complex tasks. As a remedy, novel deep learning models aim to overcome this issue by extending traditional neural network models with additional hidden layers. Indeed, deep learning has been shown to outperform traditional methods in terms of predictive performance. In this paper, we adapt the novel deep learning technique to financial decision support. In this instance, we aim to predict the direction of stock movements following financial disclosures. As a result, we show how deep learning can outperform the accuracy of random forests as a benchmark for machine learning by 5.66%.","Sun, 9 Aug 2015 07:39:24 UTC (626 KB)[v2] Wed, 4 Jul 2018 09:32:57 UTC (625 KB)"
"220","Tag-Weighted Topic Model For Large-scale Semi-Structured Documents","Shuangyin Li, Jiefei Li, Guan Huang, Ruiyang Tan, Rong Pan","Computation and Language (cs.CL)","To date, there have been massive Semi-Structured Documents (SSDs) during the evolution of the Internet. These SSDs contain both unstructured features (e.g., plain text) and metadata (e.g., tags). Most previous works focused on modeling the unstructured text, and recently, some other methods have been proposed to model the unstructured text with specific tags. To build a general model for SSDs remains an important problem in terms of both model fitness and efficiency. We propose a novel method to model the SSDs by a so-called Tag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the tags and words information, not only to learn the document-topic and topic-word distributions, but also to infer the tag-topic distributions for text mining tasks. We present an efficient variational inference method with an EM algorithm for estimating the model parameters. Meanwhile, we propose three large-scale solutions for our model under the MapReduce distributed computing platform for modeling large-scale SSDs. The experimental results show the effectiveness, efficiency and the robustness by comparing our model with the state-of-the-art methods in document modeling, tags prediction and text classification. We also show the performance of the three distributed solutions in terms of time and accuracy on document modeling.","Thu, 30 Jul 2015 06:44:37 UTC (654 KB)"
"221","Data Mining of Causal Relations from Text: Analysing Maritime Accident Investigation Reports","Santosh Tirunagari","Information Retrieval (cs.IR)","Text mining is a process of extracting information of interest from text. Such a method includes techniques from various areas such as Information Retrieval (IR), Natural Language Processing (NLP), and Information Extraction (IE). In this study, text mining methods are applied to extract causal relations from maritime accident investigation reports collected from the Marine Accident Investigation Branch (MAIB). These causal relations provide information on various mechanisms behind accidents, including human and organizational factors relating to the accident. The objective of this study is to facilitate the analysis of the maritime accident investigation reports, by means of extracting contributory causes with more feasibility. A careful investigation of contributory causes from the reports provide opportunity to improve safety in future. Two methods have been employed in this study to extract the causal relations. They are 1) Pattern classification method and 2) Connectives method. The earlier one uses naive Bayes and Support Vector Machines (SVM) as classifiers. The latter simply searches for the words connecting cause and effect in sentences. The causal patterns extracted using these two methods are compared to the manual (human expert) extraction. The pattern classification method showed a fair and sensible performance with F-measure(average) = 65% when compared to connectives method with F-measure(average) = 58%. This study is an evidence, that text mining methods could be employed in extracting causal relations from marine accident investigation reports.","Thu, 9 Jul 2015 10:20:52 UTC (983 KB)"
"222","How to improve robustness in Kohonen maps and display additional information in Factorial Analysis: application to text mining","Nicolas Bourgeois (SAMM), Marie Cottrell (SAMM), Benjamin Deruelle (LAMOP), Stephane Lamasse (LAMOP), Patrick Letremy (SAMM)","Statistics Theory (math.ST)","This article is an extended version of a paper presented in the WSOM'2012 conference [1]. We display a combination of factorial projections, SOM algorithm and graph techniques applied to a text mining problem. The corpus contains 8 medieval manuscripts which were used to teach arithmetic techniques to merchants. Among the techniques for Data Analysis, those used for Lexicometry (such as Factorial Analysis) highlight the discrepancies between manuscripts. The reason for this is that they focus on the deviation from the independence between words and manuscripts. Still, we also want to discover and characterize the common vocabulary among the whole corpus. Using the properties of stochastic Kohonen maps, which define neighborhood between inputs in a non-deterministic way, we highlight the words which seem to play a special role in the vocabulary. We call them fickle and use them to improve both Kohonen map robustness and significance of FCA visualization. Finally we use graph algorithmic to exploit this fickleness for classification of words.","Thu, 25 Jun 2015 12:56:23 UTC (38 KB)"
"223","Scientific Discovery by Machine Intelligence: A New Avenue for Drug Research","Carlo A. Trugenberger","Artificial Intelligence (cs.AI)","The majority of big data is unstructured and of this majority the largest chunk is text. While data mining techniques are well developed and standardized for structured, numerical data, the realm of unstructured data is still largely unexplored. The general focus lies on information extraction, which attempts to retrieve known information from text. The Holy Grail, however is knowledge discovery, where machines are expected to unearth entirely new facts and relations that were not previously known by any human expert. Indeed, understanding the meaning of text is often considered as one of the main characteristics of human intelligence. The ultimate goal of semantic artificial intelligence is to devise software that can understand the meaning of free text, at least in the practical sense of providing new, actionable information condensed out of a body of documents. As a stepping stone on the road to this vision I will introduce a totally new approach to drug research, namely that of identifying relevant information by employing a self-organizing semantic engine to text mine large repositories of biomedical research papers, a technique pioneered by Merck with the InfoCodex software. I will describe the methodology and a first successful experiment for the discovery of new biomarkers and phenotypes for diabetes and obesity on the basis of PubMed abstracts, public clinical trials and Merck internal documents. The reported approach shows much promise and has potential to impact fundamentally pharmaceutical research as a way to shorten time-to-market of novel drugs, and for early recognition of dead ends.","Tue, 23 Jun 2015 18:04:14 UTC (351 KB)"
"224","Early Predictions of Movie Success: the Who, What, and When of Profitability","Michael T. Lash, Kang Zhao","Artificial Intelligence (cs.AI)","This paper proposes a decision support system to aid movie investment decisions at the early stage of movie productions. The system predicts the success of a movie based on its profitability by leveraging historical data from various sources. Using social network analysis and text mining techniques, the system automatically extracts several groups of features, including ""who"" are on the cast, ""what"" a movie is about, ""when"" a movie will be released, as well as ""hybrid"" features that match ""who"" with ""what"", and ""when"" with ""what"". Experiment results with movies during an 11-year period showed that the system outperforms benchmark methods by a large margin in predicting movie profitability. Novel features we proposed also made great contributions to the prediction. In addition to designing a decision support system with practical utilities, our analysis of key factors for movie profitability may also have implications for theoretical research on team performance and the success of creative work.","Wed, 17 Jun 2015 16:40:48 UTC (397 KB)[v2] Fri, 29 Jan 2016 20:10:52 UTC (1,109 KB)"
"225","Open Data Platform for Knowledge Access in Plant Health Domain : VESPA Mining","Nicolas Turenne, Mathieu Andro, Roselyne Corbiere, Tien T. Phan","Information Retrieval (cs.IR)","Important data are locked in ancient literature. It would be uneconomic to produce these data again and today or to extract them without the help of text mining technologies. Vespa is a text mining project whose aim is to extract data on pest and crops interactions, to model and predict attacks on crops, and to reduce the use of pesticides. A few attempts proposed an agricultural information access. Another originality of our work is to parse documents with a dependency of the document architecture.","Thu, 23 Apr 2015 08:27:29 UTC (240 KB)"
"226","A new generation of science overlay maps with an application to the history of biosystematics","Sandor Soos (Dept. Science Policy and Scientometrics, Library and Information Centre of the Hungarian Academy of Sciences, MTA)","Digital Libraries (cs.DL)","The paper proposes a text-mining based analytical framework aiming at the cognitive organization of complex scientific discourses. The approach is based on models recently developed in science mapping, being a generalization of the so-called Science Overlay Mapping methodology, referred to as Topic Overlay Mapping (TOM). It is shown that via applications of TOM in visualization, document clustering, time series analysis etc. the in-depth exploration and even the measurement of cognitive complexity and its dynamics is feasible for scientific domains. As a use case, an empirical study is presented into the discovery of a long-standing complex, interdisciplinary discourse, the debate on the species concept in biosystematics.","Wed, 22 Apr 2015 14:17:38 UTC (1,383 KB)"
"227","Mining and discovering biographical information in Difangzhi with a language-model-based approach","Peter K. Bol, Chao-Lin Liu, Hongsu Wang","Computation and Language (cs.CL)","We present results of expanding the contents of the China Biographical Database by text mining historical local gazetteers, difangzhi. The goal of the database is to see how people are connected together, through kinship, social connections, and the places and offices in which they served. The gazetteers are the single most important collection of names and offices covering the Song through Qing periods. Although we begin with local officials we shall eventually include lists of local examination candidates, people from the locality who served in government, and notable local figures with biographies. The more data we collect the more connections emerge. The value of doing systematic text mining work is that we can identify relevant connections that are either directly informative or can become useful without deep historical research. Academia Sinica is developing a name database for officials in the central governments of the Ming and Qing dynasties.","Wed, 8 Apr 2015 22:38:35 UTC (969 KB)"
"228","Infinite Author Topic Model based on Mixed Gamma-Negative Binomial Process","Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo","Machine Learning (stat.ML)","Incorporating the side information of text corpus, i.e., authors, time stamps, and emotional tags, into the traditional text mining models has gained significant interests in the area of information retrieval, statistical natural language processing, and machine learning. One branch of these works is the so-called Author Topic Model (ATM), which incorporates the authors's interests as side information into the classical topic model. However, the existing ATM needs to predefine the number of topics, which is difficult and inappropriate in many real-world settings. In this paper, we propose an Infinite Author Topic (IAT) model to resolve this issue. Instead of assigning a discrete probability on fixed number of topics, we use a stochastic process to determine the number of topics from the data itself. To be specific, we extend a gamma-negative binomial process to three levels in order to capture the author-document-keyword hierarchical structure. Furthermore, each document is assigned a mixed gamma process that accounts for the multi-author's contribution towards this document. An efficient Gibbs sampling inference algorithm with each conditional distribution being closed-form is developed for the IAT model. Experiments on several real-world datasets show the capabilities of our IAT model to learn the hidden topics, authors' interests on these topics and the number of topics simultaneously.","Mon, 30 Mar 2015 05:03:37 UTC (472 KB)"
"229","Construction of FuzzyFind Dictionary using Golay Coding Transformation for Searching Applications","Kamran Kowsari, Maryam Yammahi, Nima Bari, Roman Vichr, Faisal Alsaby, Simon Y. Berkovich","Databases (cs.DB)","Searching through a large volume of data is very critical for companies, scientists, and searching engines applications due to time complexity and memory complexity. In this paper, a new technique of generating FuzzyFind Dictionary for text mining was introduced. We simply mapped the 23 bits of the English alphabet into a FuzzyFind Dictionary or more than 23 bits by using more FuzzyFind Dictionary, and reflecting the presence or absence of particular letters. This representation preserves closeness of word distortions in terms of closeness of the created binary vectors within Hamming distance of 2 deviations. This paper talks about the Golay Coding Transformation Hash Table and how it can be used on a FuzzyFind Dictionary as a new technology for using in searching through big data. This method is introduced by linear time complexity for generating the dictionary and constant time complexity to access the data and update by new data sets, also updating for new data sets is linear time depends on new data points. This technique is based on searching only for letters of English that each segment has 23 bits, and also we have more than 23-bit and also it could work with more segments as reference table.","Sun, 22 Mar 2015 21:46:12 UTC (1,121 KB)"
"230","Experimental Estimation of Number of Clusters Based on Cluster Quality","G. Hannah Grace, Kalyani Desikan","Information Retrieval (cs.IR)","Text Clustering is a text mining technique which divides the given set of text documents into significant clusters. It is used for organizing a huge number of text documents into a well-organized form. In the majority of the clustering algorithms, the number of clusters must be specified apriori, which is a drawback of these algorithms. The aim of this paper is to show experimentally how to determine the number of clusters based on cluster quality. Since partitional clustering algorithms are well-suited for clustering large document datasets, we have confined our analysis to a partitional clustering algorithm.","Tue, 10 Mar 2015 10:34:06 UTC (465 KB)"
"231","A framework to discover potential ideas of new product development from crowdsourcing application","Thanh-Cong Dinh, Hyerim Bae, Jaehun Park, Joonsoo Bae","Information Retrieval (cs.IR)","In this paper, we study idea mining from crowdsourcing applications which encourage a group of people, who are usually undefined and very large sized, to generate ideas for new product development (NPD). In order to isolate the relatively small number of potential ones among ideas from crowd, decision makers not only have to identify the key textual information representing the ideas, but they also need to consider online opinions of people who gave comments and votes on the ideas. Due to the extremely large size of text data generated by people on the Internet, identifying textual information has been carried out in manual ways, and has been considered very time consuming and costly. To overcome the ineffectiveness, this paper introduces a novel framework that can help decision makers discover ideas having the potential to be used in an NPD process. To achieve this, a semi-automatic text mining technique that retrieves useful text patterns from ideas posted on crowdsourcing application is proposed. Then, we provide an online learning algorithm to evaluate whether the idea is potential or not. Finally to verify the effectiveness of our algorithm, we conducted experiments on the data, which are collected from an existing crowd sourcing website.","Wed, 25 Feb 2015 00:13:44 UTC (2,279 KB)"
"232","Unified vector space mapping for knowledge representation systems","Dmytro Filatov, Taras Filatov","Artificial Intelligence (cs.AI)","One of the most significant problems which inhibits further developments in the areas of Knowledge Representation and Artificial Intelligence is a problem of semantic alignment or knowledge mapping. The progress in its solution will be greatly beneficial for further advances of information retrieval, ontology alignment, relevance calculation, text mining, natural language processing etc. In the paper the concept of multidimensional global knowledge map, elaborated through unsupervised extraction of dependencies from large documents corpus, is proposed. In addition, the problem of direct Human - Knowledge Representation System interface is addressed and a concept of adaptive decoder proposed for the purpose of interaction with previously described unified mapping model. In combination these two approaches are suggested as basis for a development of a new generation of knowledge representation systems.","Sat, 21 Feb 2015 18:29:57 UTC (167 KB)"
"233","Evolutionary algorithm based adaptive navigation in information retrieval interfaces","Dmytro Filatov, Taras Filatov","Information Retrieval (cs.IR)","In computer interfaces in general, especially in information retrieval tasks, it is important to be able to quickly find and retrieve information. State of the art approach, used, for example, in search engines, is not effective as it introduces losses of meanings due to context to keywords back and forth translation. Authors argue it increases the time and reduces the accuracy of information retrieval compared to what it could be in the system that employs modern information retrieval and text mining methods while presenting results in an adaptive human- computer interface where system effectively learns what operator needs through iterative interaction. In current work, a combination of adaptive navigational interface and real time collaborative feedback analysis for documents relevance weighting is proposed as an viable alternative to prevailing ""telegraphic"" approach in information retrieval systems. Adaptive navigation is provided through a dynamic links panel controlled by an evolutionary algorithm. Documents relevance is initially established with standard information retrieval techniques and is further refined in real time through interaction of users with the system. Introduced concepts of multidimensional Knowledge Map and Weighted Point of Interest allow finding relevant documents and users with common interests through a trivial calculation. Browsing search approach, the ability of the algorithm to adapt navigation to users interests, collaborative refinement and the self-organising features of the system are the main factors making such architecture effective in various fields where non-structured knowledge shall be represented to the users.","Thu, 19 Feb 2015 11:47:28 UTC (1,619 KB)"
"234","Optimizing Text Quantifiers for Multivariate Loss Functions","Andrea Esuli, Fabrizio Sebastiani","Machine Learning (cs.LG)","We address the problem of \emph{quantification}, a supervised learning task whose goal is, given a class, to estimate the relative frequency (or \emph{prevalence}) of the class in a dataset of unlabelled items. Quantification has several applications in data and text mining, such as estimating the prevalence of positive reviews in a set of reviews of a given product, or estimating the prevalence of a given support issue in a dataset of transcripts of phone calls to tech support. So far, quantification has been addressed by learning a general-purpose classifier, counting the unlabelled items which have been assigned the class, and tuning the obtained counts according to some heuristics. In this paper we depart from the tradition of using general-purpose classifiers, and use instead a supervised learning model for \emph{structured prediction}, capable of generating classifiers directly optimized for the (multivariate and non-linear) function used for evaluating quantification accuracy. The experiments that we have run on 5500 binary high-dimensional datasets (averaging more than 14,000 documents each) show that this method is more accurate, more stable, and more efficient than existing, state-of-the-art quantification methods.","Thu, 19 Feb 2015 08:06:54 UTC (237 KB)[v2] Wed, 15 Apr 2015 14:45:59 UTC (237 KB)"
"235","The Evolution of Popular Music: USA 1960-2010","Matthias Mauch, Robert M. MacCallum, Mark Levy, Armand M. Leroi","Physics and Society (physics.soc-ph)","In modern societies, cultural change seems ceaseless. The flux of fashion is especially obvious for popular music. While much has been written about the origin and evolution of pop, most claims about its history are anecdotal rather than scientific in nature. To rectify this we investigate the US Billboard Hot 100 between 1960 and 2010. Using Music Information Retrieval (MIR) and text-mining tools we analyse the musical properties of ~17,000 recordings that appeared in the charts and demonstrate quantitative trends in their harmonic and timbral properties. We then use these properties to produce an audio-based classification of musical styles and study the evolution of musical diversity and disparity, testing, and rejecting, several classical theories of cultural change. Finally, we investigate whether pop musical evolution has been gradual or punctuated. We show that, although pop music has evolved continuously, it did so with particular rapidity during three stylistic ""revolutions"" around 1964, 1983 and 1991. We conclude by discussing how our study points the way to a quantitative science of cultural change.","Tue, 17 Feb 2015 18:32:39 UTC (6,182 KB)"
"236","Count-Min-Log sketch: Approximately counting with approximate counters","Guillaume Pitel, Geoffroy Fouquier","Information Retrieval (cs.IR)","Count-Min Sketch is a widely adopted algorithm for approximate event counting in large scale processing. However, the original version of the Count-Min-Sketch (CMS) suffers of some deficiences, especially if one is interested by the low-frequency items, such as in text-mining related tasks. Several variants of CMS have been proposed to compensate for the high relative error for low-frequency events, but the proposed solutions tend to correct the errors instead of preventing them. In this paper, we propose the Count-Min-Log sketch, which uses logarithm-based, approximate counters instead of linear counters to improve the average relative error of CMS at constant memory footprint.","Tue, 17 Feb 2015 13:17:17 UTC (30 KB)"
"237","Improving Access to Digitized Historical Newspapers with Text Mining, Coordinated Models, and Formative User Interface Design","Robert B. Allen","Digital Libraries (cs.DL)","Most tools for accessing digitized historical newspapers emphasize relatively simple search; but, as increasing numbers of digitized historical newspapers and other historical resources become available we can consider much richer modes of interaction with these collections. For instance, users might use exploratory search for looking at larger issues and events such as elections and campaigns or to get a sense of ""the texture of the city"" or ""what the city was thinking"". To take full advantage of rich interface tools, the content of the newspapers needs to be described systematically and accurately. Moreover, collections of multiple newspapers need to be richly cross-indexed across titles and even with historical resources beyond the newspapers.","Fri, 13 Feb 2015 11:12:55 UTC (109 KB)"
"238","Deep Belief Nets for Topic Modeling","Lars Maaloe, Morten Arngren, Ole Winther","Computation and Language (cs.CL)","Applying traditional collaborative filtering to digital publishing is challenging because user data is very sparse due to the high volume of documents relative to the number of users. Content based approaches, on the other hand, is attractive because textual content is often very informative. In this paper we describe large-scale content based collaborative filtering for digital publishing. To solve the digital publishing recommender problem we compare two approaches: latent Dirichlet allocation (LDA) and deep belief nets (DBN) that both find low-dimensional latent representations for documents. Efficient retrieval can be carried out in the latent representation. We work both on public benchmarks and digital media content provided by Issuu, an online publishing platform. This article also comes with a newly developed deep belief nets toolbox for topic modeling tailored towards performance evaluation of the DBN model and comparisons to the LDA model.","Sun, 18 Jan 2015 17:12:59 UTC (6,521 KB)"
"239","The potential and challenges of Big data - Recommendation systems next level application","Fatima El Jamiy, Abderrahmane Daif, Mohamed Azouazi, Abdelaziz Marzak","Computers and Society (cs.CY)","The continuous increase of data generated provides enormous possibilities of both public and private companies. The management of this mass of data or big data will play a crucial role in the society of the future, as it finds applications in different fields. There are so much potential and extremely useful insights hidden in the huge volume of data. The advanced analysis techniques available including predictive analytics, text mining, semantic analysis are needed to enable organizations to create a competitive advantage through data analyzed with different levels of sophistication, speed and accuracy previously unavailable. Therefore, is it still possible to have that level of sophistication with the ubiquitous numeric ocean that accompanies use every day via connected devices that invade our lives? However, development of big data requires a good understanding of the issues associated with it. And this is the purpose of this paper, which focuses on giving a close-up view of big data analysis, opportunities and challenges.","Wed, 14 Jan 2015 17:53:08 UTC (348 KB)"
"240","Improving Persian Document Classification Using Semantic Relations between Words","Saeed Parseh, Ahmad Baraani","Information Retrieval (cs.IR)","With the increase of information, document classification as one of the methods of text mining, plays vital role in many management and organizing information. Document classification is the process of assigning a document to one or more predefined category labels. Document classification includes different parts such as text processing, term selection, term weighting and final classification. The accuracy of document classification is very important. Thus improvement in each part of classification should lead to better results and higher precision. Term weighting has a great impact on the accuracy of the classification. Most of the existing weighting methods exploit the statistical information of terms in documents and do not consider semantic relations between words. In this paper, an automated document classification system is presented that uses a novel term weighting method based on semantic relations between terms. To evaluate the proposed method, three standard Persian corpuses are used. Experiment results show 2 to 4 percent improvement in classification accuracy compared with the best previous designed system for Persian documents.","Sun, 28 Dec 2014 10:56:53 UTC (404 KB)"
"241","Cauchy Principal Component Analysis","Pengtao Xie, Eric Xing","Machine Learning (cs.LG)","Principal Component Analysis (PCA) has wide applications in machine learning, text mining and computer vision. Classical PCA based on a Gaussian noise model is fragile to noise of large magnitude. Laplace noise assumption based PCA methods cannot deal with dense noise effectively. In this paper, we propose Cauchy Principal Component Analysis (Cauchy PCA), a very simple yet effective PCA method which is robust to various types of noise. We utilize Cauchy distribution to model noise and derive Cauchy PCA under the maximum likelihood estimation (MLE) framework with low rank constraint. Our method can robustly estimate the low rank matrix regardless of whether noise is large or small, dense or sparse. We analyze the robustness of Cauchy PCA from a robust statistics view and present an efficient singular value projection optimization method. Experimental results on both simulated data and real applications demonstrate the robustness of Cauchy PCA to various noise patterns.","Fri, 19 Dec 2014 20:06:02 UTC (1,100 KB)"
"242","Extraction of Pharmacokinetic Evidence of Drug-drug Interactions from the Literature","Artemy Kolchinsky, Analia Lourenco, Heng-Yi Wu, Lang Li, Luis M. Rocha","Machine Learning (stat.ML)","Drug-drug interaction (DDI) is a major cause of morbidity and mortality and a subject of intense scientific interest. Biomedical literature mining can aid DDI research by extracting evidence for large numbers of potential interactions from published literature and clinical databases. Though DDI is investigated in domains ranging in scale from intracellular biochemistry to human populations, literature mining has not been used to extract specific types of experimental evidence, which are reported differently for distinct experimental goals. We focus on pharmacokinetic evidence for DDI, essential for identifying causal mechanisms of putative interactions and as input for further pharmacological and pharmaco-epidemiology investigations. We used manually curated corpora of PubMed abstracts and annotated sentences to evaluate the efficacy of literature mining on two tasks: first, identifying PubMed abstracts containing pharmacokinetic evidence of DDIs; second, extracting sentences containing such evidence from abstracts. We implemented a text mining pipeline and evaluated it using several linear classifiers and a variety of feature transforms. The most important textual features in the abstract and sentence classification tasks were analyzed. We also investigated the performance benefits of using features derived from PubMed metadata fields, various publicly available named entity recognizers, and pharmacokinetic dictionaries. Several classifiers performed very well in distinguishing relevant and irrelevant abstracts (reaching F1~=0.93, MCC~=0.74, iAUC~=0.99) and sentences (F1~=0.76, MCC~=0.65, iAUC~=0.83). We found that word bigram features were important for achieving optimal classifier performance and that features derived from Medical Subject Headings (MeSH) terms significantly improved abstract classification. ...","Tue, 2 Dec 2014 00:01:39 UTC (967 KB)[v2] Mon, 18 May 2015 16:45:42 UTC (621 KB)"
"243","Analysis of the human diseasome reveals phenotype modules across common, genetic, and infectious diseases","Robert Hoehndorf, Paul N Schofield, Georgios V Gkoutos","Quantitative Methods (q-bio.QM)","Phenotypes are the observable characteristics of an organism arising from its response to the environment. Phenotypes associated with engineered and natural genetic variation are widely recorded using phenotype ontologies in model organisms, as are signs and symptoms of human Mendelian diseases in databases such as OMIM and Orphanet. Exploiting these resources, several computational methods have been developed for integration and analysis of phenotype data to identify the genetic etiology of diseases or suggest plausible interventions. A similar resource would be highly useful not only for rare and Mendelian diseases, but also for common, complex and infectious diseases. We apply a semantic text- mining approach to identify the phenotypes (signs and symptoms) associated with over 8,000 diseases. We demonstrate that our method generates phenotypes that correctly identify known disease-associated genes in mice and humans with high accuracy. Using a phenotypic similarity measure, we generate a human disease network in which diseases that share signs and symptoms cluster together, and we use this network to identify phenotypic disease modules.","Mon, 3 Nov 2014 12:15:39 UTC (8,430 KB)[v2] Wed, 26 Nov 2014 18:48:02 UTC (4,574 KB)"
"244","Term-Weighting Learning via Genetic Programming for Text Classification","Hugo Jair Escalante, Mauricio A. Garcia-Limon, Alicia Morales-Reyes, Mario Graff, Manuel Montes-y-Gomez, Eduardo F. Morales","Neural and Evolutionary Computing (cs.NE)","This paper describes a novel approach to learning term-weighting schemes (TWSs) in the context of text classification. In text mining a TWS determines the way in which documents will be represented in a vector space model, before applying a classifier. Whereas acceptable performance has been obtained with standard TWSs (e.g., Boolean and term-frequency schemes), the definition of TWSs has been traditionally an art. Further, it is still a difficult task to determine what is the best TWS for a particular problem and it is not clear yet, whether better schemes, than those currently available, can be generated by combining known TWS. We propose in this article a genetic program that aims at learning effective TWSs that can improve the performance of current schemes in text classification. The genetic program learns how to combine a set of basic units to give rise to discriminative TWSs. We report an extensive experimental study comprising data sets from thematic and non-thematic text classification as well as from image classification. Our study shows the validity of the proposed method; in fact, we show that TWSs learned with the genetic program outperform traditional schemes and other TWSs proposed in recent works. Further, we show that TWSs learned from a specific domain can be effectively used for other tasks.","Thu, 2 Oct 2014 18:38:11 UTC (161 KB)[v2] Fri, 3 Oct 2014 19:47:03 UTC (161 KB)[v3] Mon, 6 Oct 2014 20:48:29 UTC (161 KB)"
"245","Testing parametric models in linear-directional regression","Eduardo Garcia-Portugues, Ingrid Van Keilegom, Rosa M. Crujeiras, Wenceslao Gonzalez-Manteiga","Methodology (stat.ME)","This paper presents a goodness-of-fit test for parametric regression models with scalar response and directional predictor, that is, vectors in a sphere of arbitrary dimension. The testing procedure is based on the weighted squared distance between a smooth and a parametric regression estimator, where the smooth regression estimator is obtained by a projected local approach. Asymptotic behavior of the test statistic under the null hypothesis and local alternatives is provided, jointly with a consistent bootstrap algorithm for application in practice. A simulation study illustrates the performance of the test in finite samples. The procedure is also applied to a real data example from text mining.","Mon, 1 Sep 2014 19:01:12 UTC (3,457 KB)[v2] Sun, 28 Sep 2014 03:19:22 UTC (1,862 KB)[v3] Fri, 3 Apr 2015 15:08:59 UTC (1,863 KB)"
"246","A Case Study in Text Mining: Interpreting Twitter Data From World Cup Tweets","Daniel Godfrey, Caley Johns, Carl Meyer, Shaina Race, Carol Sadek","Machine Learning (stat.ML)","Cluster analysis is a field of data analysis that extracts underlying patterns in data. One application of cluster analysis is in text-mining, the analysis of large collections of text to find similarities between documents. We used a collection of about 30,000 tweets extracted from Twitter just before the World Cup started. A common problem with real world text data is the presence of linguistic noise. In our case it would be extraneous tweets that are unrelated to dominant themes. To combat this problem, we created an algorithm that combined the DBSCAN algorithm and a consensus matrix. This way we are left with the tweets that are related to those dominant themes. We then used cluster analysis to find those topics that the tweets describe. We clustered the tweets using k-means, a commonly used clustering algorithm, and Non-Negative Matrix Factorization (NMF) and compared the results. The two algorithms gave similar results, but NMF proved to be faster and provided more easily interpreted results. We explored our results using two visualization tools, Gephi and Wordle.","Thu, 21 Aug 2014 17:58:33 UTC (2,293 KB)"
"247","Convex Biclustering","Eric C. Chi, Genevera I. Allen, Richard G. Baraniuk","Methodology (stat.ME)","In the biclustering problem, we seek to simultaneously group observations and features. While biclustering has applications in a wide array of domains, ranging from text mining to collaborative filtering, the problem of identifying structure in high dimensional genomic data motivates this work. In this context, biclustering enables us to identify subsets of genes that are co-expressed only within a subset of experimental conditions. We present a convex formulation of the biclustering problem that possesses a unique global minimizer and an iterative algorithm, COBRA, that is guaranteed to identify it. Our approach generates an entire solution path of possible biclusters as a single tuning parameter is varied. We also show how to reduce the problem of selecting this tuning parameter to solving a trivial modification of the convex biclustering problem. The key contributions of our work are its simplicity, interpretability, and algorithmic guarantees - features that arguably are lacking in the current alternative algorithms. We demonstrate the advantages of our approach, which includes stably and reproducibly identifying biclusterings, on simulated and real microarray data.","Tue, 5 Aug 2014 03:27:17 UTC (258 KB)[v2] Thu, 25 Sep 2014 21:39:47 UTC (981 KB)[v3] Fri, 29 Jan 2016 00:09:49 UTC (983 KB)[v4] Fri, 15 Apr 2016 19:57:55 UTC (769 KB)"
"248","KNET: A General Framework for Learning Word Embedding using Morphological Knowledge","Qing Cui, Bin Gao, Jiang Bian, Siyu Qiu, Tie-Yan Liu","Computation and Language (cs.CL)","Neural network techniques are widely applied to obtain high-quality distributed representations of words, i.e., word embeddings, to address text mining, information retrieval, and natural language processing tasks. Recently, efficient methods have been proposed to learn word embeddings from context that captures both semantic and syntactic relationships between words. However, it is challenging to handle unseen words or rare words with insufficient context. In this paper, inspired by the study on word recognition process in cognitive psychology, we propose to take advantage of seemingly less obvious but essentially important morphological knowledge to address these challenges. In particular, we introduce a novel neural network architecture called KNET that leverages both contextual information and morphological word similarity built based on morphological knowledge to learn word embeddings. Meanwhile, the learning architecture is also able to refine the pre-defined morphological knowledge and obtain more accurate word similarity. Experiments on an analogical reasoning task and a word similarity task both demonstrate that the proposed KNET framework can greatly enhance the effectiveness of word embeddings.","Mon, 7 Jul 2014 12:45:10 UTC (220 KB)[v2] Mon, 1 Sep 2014 16:03:47 UTC (317 KB)[v3] Fri, 5 Sep 2014 15:58:35 UTC (317 KB)"
"249","Stock Market Prediction from WSJ: Text Mining via Sparse Matrix Factorization","Felix Ming Fai Wong, Zhenming Liu, Mung Chiang","Machine Learning (cs.LG)","We revisit the problem of predicting directional movements of stock prices based on news articles: here our algorithm uses daily articles from The Wall Street Journal to predict the closing stock prices on the same day. We propose a unified latent space model to characterize the ""co-movements"" between stock prices and news articles. Unlike many existing approaches, our new model is able to simultaneously leverage the correlations: (a) among stock prices, (b) among news articles, and (c) between stock prices and news articles. Thus, our model is able to make daily predictions on more than 500 stocks (most of which are not even mentioned in any news article) while having low complexity. We carry out extensive backtesting on trading strategies based on our algorithm. The result shows that our model has substantially better accuracy rate (55.7%) compared to many widely used algorithms. The return (56%) and Sharpe ratio due to a trading strategy based on our model are also much higher than baseline indices.","Fri, 27 Jun 2014 22:34:47 UTC (398 KB)"
"250","A Semantic VSM-Based Recommender System","Hadi Fanaee-T, Mehran Yazdi","Information Retrieval (cs.IR)","Online forums enable users to discuss together around various topics. One of the serious problems of these environments is high volume of discussions and thus information overload problem. Unfortunately without considering the users interests, traditional Information Retrieval (IR) techniques are not able to solve the problem. Therefore, employment of a Recommender System (RS) that could suggest favorite's topics of users according to their tastes could increases the dynamism of forum and prevent the users from duplicate posts. In addition, consideration of semantics can be useful for increasing the performance of IR based RS. Our goal is study of impact of ontology and data mining techniques on improving of content-based RS. For this purpose, at first, three type of ontologies will be constructed from the domain corpus with utilization of text mining, Natural Language Processing (NLP) and Wordnet and then they will be used as an input in two kind of RS: one, fully ontology-based and one with enriching the user profile vector with ontology in vector space model (VSM) (proposed method). Afterward the results will be compared with the simple VSM based RS. Given results show that the proposed RS presents the highest performance.","Thu, 12 Jun 2014 16:02:16 UTC (1,792 KB)"
"251","A Multiplicative Model for Learning Distributed Text-Based Attribute Representations","Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov","Machine Learning (cs.LG)","In this paper we propose a general framework for learning distributed representations of attributes: characteristics of text whose representations can be jointly learned with word embeddings. Attributes can correspond to document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditioned text generation.","Tue, 10 Jun 2014 20:29:10 UTC (350 KB)"
"252","Text Mining System for Non-Expert Miners","P. Ramya, S. Sasirekha","Information Retrieval (cs.IR)","Service oriented architecture integrated with text mining allows services to extract information in a well defined manner. In this paper, it is proposed to design a knowledge extracting system for the Ocean Information Data System. Deployed ARGO floating sensors of INCOIS (Indian National Council for Ocean Information Systems) organization reflects the characteristics of ocean. This is forwarded to the OIDS (Ocean Information Data System). For the data received from OIDS, pre-processing techniques are applied. Pre-processing involves the header retrieval and data separation. Header information is used to identify the region of sensor, whereas data is used in the analysis process of Ocean Information System. Analyzed data is segmented based on the region, by the header value. Mining technique and composition principle is applied on the segments for further analysis. Index Terms-- Service oriented architecture; Text Mining; ARGO floating sensor; INCOIS; OIDS; Pre-processing.","Sat, 7 Jun 2014 03:04:02 UTC (667 KB)"
"253","New Perspectives in Sinographic Language Processing Through the Use of Character Structure","Yannis Haralambous","Computation and Language (cs.CL)","Chinese characters have a complex and hierarchical graphical structure carrying both semantic and phonetic information. We use this structure to enhance the text model and obtain better results in standard NLP operations. First of all, to tackle the problem of graphical variation we define allographic classes of characters. Next, the relation of inclusion of a subcharacter in a characters, provides us with a directed graph of allographic classes. We provide this graph with two weights: semanticity (semantic relation between subcharacter and character) and phoneticity (phonetic relation) and calculate ""most semantic subcharacter paths"" for each character. Finally, adding the information contained in these paths to unigrams we claim to increase the efficiency of text mining methods. We evaluate our method on a text classification task on two corpora (Chinese and Japanese) of a total of 18 million characters and get an improvement of 3% on an already high baseline of 89.6% precision, obtained by a linear SVM classifier. Other possible applications and perspectives of the system are discussed.","Wed, 21 May 2014 16:49:50 UTC (2,054 KB)"
"254","Overview of Stemming Algorithms for Indian and Non-Indian Languages","Dalwadi Bijal, Suthar Sanket","Computation and Language (cs.CL)","Stemming is a pre-processing step in Text Mining applications as well as a very common requirement of Natural Language processing functions. Stemming is the process for reducing inflected words to their stem. The main purpose of stemming is to reduce different grammatical forms / word forms of a word like its noun, adjective, verb, adverb etc. to its root form. Stemming is widely uses in Information Retrieval system and reduces the size of index files. We can say that the goal of stemming is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. In this paper we have discussed different stemming algorithm for non-Indian and Indian language, methods of stemming, accuracy and errors.","Thu, 10 Apr 2014 17:16:01 UTC (264 KB)"
"255","Time Series Analysis on Stock Market for Text Mining Correlation of Economy News","Sadi Evren Seker, Cihan Mert, Khaled Al-Naami, Nuri Ozalp, Ugur Ayan","Computational Engineering, Finance, and Science (cs.CE)","This paper proposes an information retrieval method for the economy news. The effect of economy news, are researched in the word level and stock market values are considered as the ground proof. The correlation between stock market prices and economy news is an already addressed problem for most of the countries. The most well-known approach is applying the text mining approaches to the news and some time series analysis techniques over stock market closing values in order to apply classification or clustering algorithms over the features extracted. This study goes further and tries to ask the question what are the available time series analysis techniques for the stock market closing values and which one is the most suitable? In this study, the news and their dates are collected into a database and text mining is applied over the news, the text mining part has been kept simple with only term frequency-inverse document frequency method. For the time series analysis part, we have studied 10 different methods such as random walk, moving average, acceleration, Bollinger band, price rate of change, periodic average, difference, momentum or relative strength index and their variation. In this study we have also explained these techniques in a comparative way and we have applied the methods over Turkish Stock Market closing values for more than a 2 year period. On the other hand, we have applied the term frequency-inverse document frequency method on the economy news of one of the high-circulating newspapers in Turkey.","Sat, 8 Mar 2014 19:50:15 UTC (363 KB)"
"256","High-dimensional tests for spherical location and spiked covariance","Christophe Ley, Davy Paindaveine, Thomas Verdebout","Statistics Theory (math.ST)","Rotationally symmetric distributions on the p-dimensional unit hypersphere, extremely popular in directional statistics, involve a location parameter theta that indicates the direction of the symmetry axis. The most classical way of addressing the spherical location problem H_0:theta=theta_0, with theta_0 a fixed location, is the so-called Watson test, which is based on the sample mean of the observations. This test enjoys many desirable properties, but its implementation requires the sample size n to be large compared to the dimension p. This is a severe limitation, since more and more problems nowadays involve high-dimensional directional data (e.g., in genetics or text mining). In this work, we therefore introduce a modified Watson statistic that can cope with high-dimensionality. We derive its asymptotic null distribution as both n and p go to infinity. This is achieved in a universal asymptotic framework that allows p to go to infinity arbitrarily fast (or slowly) as a function of n. We further show that our results also provide high-dimensional tests for a problem that has recently attracted much attention, namely that of testing that the covariance matrix of a multinormal distribution has a ""theta_0-spiked"" structure. Finally, a Monte Carlo simulation study corroborates our asymptotic results.","Wed, 12 Feb 2014 13:56:55 UTC (546 KB)"
"257","Generating Extractive Summaries of Scientific Paradigms","Vahed Qazvinian, Dragomir R. Radev, Saif M. Mohammad, Bonnie Dorr, David Zajic, Michael Whidby, Taesun Moon","Information Retrieval (cs.IR)","Researchers and scientists increasingly find themselves in the position of having to quickly understand large amounts of technical material. Our goal is to effectively serve this need by using bibliometric text mining and summarization techniques to generate summaries of scientific literature. We show how we can use citations to produce automatically generated, readily consumable, technical extractive summaries. We first propose C-LexRank, a model for summarizing single scientific articles based on citations, which employs community detection and extracts salient information-rich sentences. Next, we further extend our experiments to summarize a set of papers, which cover the same scientific topic. We generate extractive summaries of a set of Question Answering (QA) and Dependency Parsing (DP) papers, their abstracts, and their citation sentences and show that citations have unique information amenable to creating a summary.","Tue, 4 Feb 2014 01:33:10 UTC (637 KB)"
"258","A Novel String Distance Function based on Most Frequent K Characters","Sadi Evren Seker, Oguz Altun, U<U+011F>ur Ayan, Cihan Mert","Data Structures and Algorithms (cs.DS)","This study aims to publish a novel similarity metric to increase the speed of comparison operations. Also the new metric is suitable for distance-based operations among strings. Most of the simple calculation methods, such as string length are fast to calculate but does not represent the string correctly. On the other hand the methods like keeping the histogram over all characters in the string are slower but good to represent the string characteristics in some areas, like natural language. We propose a new metric, easy to calculate and satisfactory for string comparison. Method is built on a hash function, which gets a string at any size and outputs the most frequent K characters with their frequencies. The outputs are open for comparison and our studies showed that the success rate is quite satisfactory for the text mining operations.","Sat, 25 Jan 2014 23:40:46 UTC (930 KB)"
"259","Data Mining Cultural Aspects of Social Media Marketing","Ronald Hochreiter, Christoph Waldhauser","Social and Information Networks (cs.SI)","For marketing to function in a globalized world it must respect a diverse set of local cultures. With marketing efforts extending to social media platforms, the crossing of cultural boundaries can happen in an instant. In this paper we examine how culture influences the popularity of marketing messages in social media platforms. Text mining, automated translation and sentiment analysis contribute largely to our research. From our analysis of 400 posts on the localized Google+ pages of German car brands in Germany and the US, we conclude that posting time and emotions are important predictors for reshare counts.","Wed, 22 Jan 2014 16:41:27 UTC (57 KB)[v2] Wed, 16 Apr 2014 16:05:10 UTC (603 KB)"
"260","The Why and How of Nonnegative Matrix Factorization","Nicolas Gillis","Machine Learning (stat.ML)","Nonnegative matrix factorization (NMF) has become a widely used tool for the analysis of high-dimensional data as it automatically extracts sparse and meaningful features from a set of nonnegative data vectors. We first illustrate this property of NMF on three applications, in image processing, text mining and hyperspectral imaging --this is the why. Then we address the problem of solving NMF, which is NP-hard in general. We review some standard NMF algorithms, and also present a recent subclass of NMF problems, referred to as near-separable NMF, that can be solved efficiently (that is, in polynomial time), even in the presence of noise --this is the how. Finally, we briefly describe some problems in mathematics and computer science closely related to NMF via the nonnegative rank.","Tue, 21 Jan 2014 09:03:12 UTC (2,058 KB)[v2] Fri, 7 Mar 2014 10:32:43 UTC (2,064 KB)"
"261","A survey of methods to ease the development of highly multilingual text mining applications","Ralf Steinberger","Computation and Language (cs.CL)","Multilingual text processing is useful because the information content found in different languages is complementary, both regarding facts and opinions. While Information Extraction and other text mining software can, in principle, be developed for many languages, most text analysis tools have only been applied to small sets of languages because the development effort per language is large. Self-training tools obviously alleviate the problem, but even the effort of providing training data and of manually tuning the results is usually considerable. In this paper, we gather insights by various multilingual system developers on how to minimise the effort of developing natural language processing applications for many languages. We also explain the main guidelines underlying our own effort to develop complex text mining software for tens of languages. While these guidelines - most of all: extreme simplicity - can be very restrictive and limiting, we believe to have shown the feasibility of the approach through the development of the Europe Media Monitor (EMM) family of applications (this http URL). EMM is a set of complex media monitoring tools that process and analyse up to 100,000 online news articles per day in between twenty and fifty languages. We will also touch upon the kind of language resources that would make it easier for all to develop highly multilingual text mining applications. We will argue that - to achieve this - the most needed resources would be freely available, simple, parallel and uniform multilingual dictionaries, corpora and software tools.","Mon, 13 Jan 2014 18:05:28 UTC (753 KB)"
"262","Statistical Analysis based Hypothesis Testing Method in Biological Knowledge Discovery","Md. Naseef-Ur-Rahman Chowdhury, Suvankar Paul, Kazi Zakia Sultana","Information Retrieval (cs.IR)","The correlation and interactions among different biological entities comprise the biological system. Although already revealed interactions contribute to the understanding of different existing systems, researchers face many questions everyday regarding inter-relationships among entities. Their queries have potential role in exploring new relations which may open up a new area of investigation. In this paper, we introduce a text mining based method for answering the biological queries in terms of statistical computation such that researchers can come up with new knowledge discovery. It facilitates user to submit their query in natural linguistic form which can be treated as hypothesis. Our proposed approach analyzes the hypothesis and measures the p-value of the hypothesis with respect to the existing literature. Based on the measured value, the system either accepts or rejects the hypothesis from statistical point of view. Moreover, even it does not find any direct relationship among the entities of the hypothesis, it presents a network to give an integral overview of all the entities through which the entities might be related. This is also congenial for the researchers to widen their view and thus think of new hypothesis for further investigation. It assists researcher to get a quantitative evaluation of their assumptions such that they can reach a logical conclusion and thus aids in relevant re-searches of biological knowledge discovery. The system also provides the researchers a graphical interactive interface to submit their hypothesis for assessment in a more convenient way.","Thu, 9 Jan 2014 18:05:15 UTC (264 KB)"
"263","Novel text categorization by amalgamation of augmented k-nearest neighborhood classification and k-medoids clustering","RamachandraRao Kurada, Dr. K Karteeka Pavan","Information Retrieval (cs.IR)","Machine learning for text classification is the underpinning of document cataloging, news filtering, document steering and exemplification. In text mining realm, effective feature selection is significant to make the learning task more accurate and competent. One of the traditional lazy text classifier k-Nearest Neighborhood (kNN) has a major pitfall in calculating the similarity between all the objects in training and testing sets, there by leads to exaggeration of both computational complexity of the algorithm and massive consumption of main memory. To diminish these shortcomings in viewpoint of a data-mining practitioner an amalgamative technique is proposed in this paper using a novel restructured version of kNN called AugmentedkNN(AkNN) and k-Medoids(kMdd) clustering.The proposed work comprises preprocesses on the initial training set by imposing attribute feature selection for reduction of high dimensionality, also it detects and excludes the high-fliers samples in the initial training set and restructures a constrictedtraining set. The kMdd clustering algorithm generates the cluster centers (as interior objects) for each category and restructures the constricted training set with centroids. This technique is amalgamated with AkNNclassifier that was prearranged with text mining similarity measures. Eventually, significantweights and ranks were assigned to each object in the new training set based upon their accessory towards the object in testing set. Experiments conducted on Reuters-21578 a UCI benchmark text mining data set, and comparisons with traditional kNNclassifier designates the referredmethod yieldspreeminentrecitalin both clustering and classification.","Mon, 9 Dec 2013 10:36:22 UTC (605 KB)"
"264","A Hybrid Web Recommendation System based on the Improved Association Rule Mining Algorithm","Ujwala Wanaskar, Sheetal Vij, Debajyoti Mukhopadhyay","Information Retrieval (cs.IR)","As the growing interest of web recommendation systems those are applied to deliver customized data for their users, we started working on this system. Generally the recommendation systems are divided into two major categories such as collaborative recommendation system and content based recommendation system. In case of collaborative recommen-dation systems, these try to seek out users who share same tastes that of given user as well as recommends the websites according to the liking given user. Whereas the content based recommendation systems tries to recommend web sites similar to those web sites the user has liked. In the recent research we found that the efficient technique based on asso-ciation rule mining algorithm is proposed in order to solve the problem of web page recommendation. Major problem of the same is that the web pages are given equal importance. Here the importance of pages changes according to the fre-quency of visiting the web page as well as amount of time user spends on that page. Also recommendation of newly added web pages or the pages those are not yet visited by users are not included in the recommendation set. To over-come this problem, we have used the web usage log in the adaptive association rule based web mining where the asso-ciation rules were applied to personalization. This algorithm was purely based on the Apriori data mining algorithm in order to generate the association rules. However this method also suffers from some unavoidable drawbacks. In this paper we are presenting and investigating the new approach based on weighted Association Rule Mining Algorithm and text mining. This is improved algorithm which adds semantic knowledge to the results, has more efficiency and hence gives better quality and performances as compared to existing approaches.","Thu, 28 Nov 2013 04:22:55 UTC (451 KB)"
"265","Using Robust PCA to estimate regional characteristics of language use from geo-tagged Twitter messages","Daniel Kondor, Istvan Csabai, Laszlo Dobos, Janos Szule, Norbert Barankai, Tamas Hanyecz, Tamas Seb<U+0151>k, Zsofia Kallus, Gabor Vattay","Computation and Language (cs.CL)","Principal component analysis (PCA) and related techniques have been successfully employed in natural language processing. Text mining applications in the age of the online social media (OSM) face new challenges due to properties specific to these use cases (e.g. spelling issues specific to texts posted by users, the presence of spammers and bots, service announcements, etc.). In this paper, we employ a Robust PCA technique to separate typical outliers and highly localized topics from the low-dimensional structure present in language use in online social networks. Our focus is on identifying geospatial features among the messages posted by the users of the Twitter microblogging service. Using a dataset which consists of over 200 million geolocated tweets collected over the course of a year, we investigate whether the information present in word usage frequencies can be used to identify regional features of language use and topics of interest. Using the PCA pursuit method, we are able to identify important low-dimensional features, which constitute smoothly varying functions of the geographic location.","Tue, 5 Nov 2013 19:31:33 UTC (1,581 KB)"
"266","Towards Applying Text Mining Techniques on Software Quality Standards and Models","Zador Daniel Kelemen, Rob Kusters, Jos Trienekens, Katalin Balla","Software Engineering (cs.SE)","Many of quality approaches are described in hundreds of textual pages. Manual processing of information consumes plenty of resources. In this report we present a text mining approach applied on CMMI, one well known and widely known quality approach. The text mining analysis can provide a quick overview on the scope of a quality approaches. The result of the analysis could accelerate the understanding and the selection of quality approaches.","Tue, 22 Oct 2013 17:14:27 UTC (600 KB)[v2] Mon, 11 Nov 2013 18:38:57 UTC (0 KB)"
"267","Reading Stockholm Riots 2013 in social media by text-mining","Andrzej Jarynowski, Amir Rostami","Social and Information Networks (cs.SI)","The riots in Stockholm in May 2013 were an event that reverberated in the world media for its dimension of violence that had spread through the Swedish capital. In this study we have investigated the role of social media in creating media phenomena via text mining and natural language processing. We have focused on two channels of communication for our analysis: Twitter and this http URL (Forum of Polish community in Sweden). Our preliminary results show some hot topics driving discussion related mostly to Swedish Police and Swedish Politics by counting word usage. Typical features for media intervention are presented. We have built networks of most popular phrases, clustered by categories (geography, media institution, etc.). Sentiment analysis shows negative connotation with Police. The aim of this preliminary exploratory quantitative study was to generate questions and hypotheses, which we could carefully follow by deeper more qualitative methods.","Fri, 4 Oct 2013 13:04:45 UTC (702 KB)"
"268","Incorporating Text Analysis into Evolution of Social Groups in Blogosphere","Bogdan Gliwa, Anna Zygmunt, Stanisaw Podgorski","Social and Information Networks (cs.SI)","Data reflecting social and business relations has often form of network of connections between entities (called social network). In such network important and influential users can be identified as well as groups of strongly connected users. Finding such groups and observing their evolution becomes an increasingly important research problem. One of the significant problems is to develop method incorporating not only information about connections between entities but also information obtained from text written by the users. Method presented in this paper combine social network analysis and text mining in order to understand groups evolution.","Thu, 22 Aug 2013 21:41:18 UTC (550 KB)"
"269","Clinical Relationships Extraction Techniques from Patient Narratives","Wafaa Tawfik Abdel-moneim, Mohamed Hashem Abdel-Aziz, Mohamed Monier Hassan","Information Retrieval (cs.IR)","The Clinical E-Science Framework (CLEF) project was used to extract important information from medical texts by building a system for the purpose of clinical research, evidence-based healthcare and genotype-meets-phenotype informatics. The system is divided into two parts, one part concerns with the identification of relationships between clinically important entities in the text. The full parses and domain-specific grammars had been used to apply many approaches to extract the relationship. In the second part of the system, statistical machine learning (ML) approaches are applied to extract relationship. A corpus of oncology narratives that hand annotated with clinical relationships can be used to train and test a system that has been designed and implemented by supervised machine learning (ML) approaches. Many features can be extracted from these texts that are used to build a model by the classifier. Multiple supervised machine learning algorithms can be applied for relationship extraction. Effects of adding the features, changing the size of the corpus, and changing the type of the algorithm on relationship extraction are examined. Keywords: Text mining; information extraction; NLP; entities; and relations.","Fri, 21 Jun 2013 15:30:09 UTC (613 KB)"
"270","A Fuzzy Based Approach to Text Mining and Document Clustering","Sumit Goswami, Mayank Singh Shishodia","Machine Learning (cs.LG)","Fuzzy logic deals with degrees of truth. In this paper, we have shown how to apply fuzzy logic in text mining in order to perform document clustering. We took an example of document clustering where the documents had to be clustered into two categories. The method involved cleaning up the text and stemming of words. Then, we chose m number of features which differ significantly in their word frequencies (WF), normalized by document length, between documents belonging to these two clusters. The documents to be clustered were represented as a collection of m normalized WF values. Fuzzy c-means (FCM) algorithm was used to cluster these documents into two clusters. After the FCM execution finished, the documents in the two clusters were analysed for the values of their respective m features. It was known that documents belonging to a document type, say X, tend to have higher WF values for some particular features. If the documents belonging to a cluster had higher WF values for those same features, then that cluster was said to represent X. By fuzzy logic, we not only get the cluster name, but also the degree to which a document belongs to a cluster.","Thu, 6 Jun 2013 07:35:23 UTC (359 KB)"
"271","Proceedings Fourth International Workshop on Computational Models for Cell Processes","Ion Petre (Abo Akademi University)","Computational Engineering, Finance, and Science (cs.CE)","The fourth international workshop on Computational Models for Cell Processes (CompMod 2013) took place on June 11, 2013 at the Abo Akademi University, Turku, Finland, in conjunction with iFM 2013. The first edition of the workshop (2008) took place in Turku, Finland, in conjunction with Formal Methods 2008, the second edition (2009) took place in Eindhoven, the Netherlands, as well in conjunction with Formal Methods 2009, and the third one took place in Aachen, Germany, in conjunction with CONCUR 2013. This volume contains the final versions of all contributions accepted for presentation at the workshop. The goal of the CompMod workshop series is to bring together researchers in Computer Science and Mathematics (both discrete and continuous), interested in the opportunities and the challenges of Systems Biology. The Program Committee of CompMod 2013 selected 3 papers for presentation at the workshop. In addition, we had two invited talks and five informal presentations. The scientific program of the workshop spans an interesting mix of approaches to systems and even synthetic biology, encompassing several different modeling approaches, ranging from quantitative to qualitative techniques, from continuous to discrete mathematics, and from deterministic to stochastic methods. We thank our invited speakers Daniela Besozzi (Universita degli Studi di Milano, Milano, Italy) and Juho Rousu (Aalto University, Finland) for accepting our invitation and for presenting some of their recent results at CompMod 2013. The technical contributions address the mathematical modeling of the PDGF signalling pathway, the canonical labelling of site graphs, rule-based modeling of polymerization reactions, rule-based modeling as a platform for the analysis of synthetic self-assembled nano-systems, robustness analysis of stochastic systems, an algebraic approach to gene assembly in ciliates, and large-scale text mining of biomedical literature.","Sun, 9 Jun 2013 13:42:56 UTC (267 KB)"
"272","Somoclu: An Efficient Parallel Library for Self-Organizing Maps","Peter Wittek, Shi Chao Gao, Ik Soo Lim, Li Zhao","Distributed, Parallel, and Cluster Computing (cs.DC)","Somoclu is a massively parallel tool for training self-organizing maps on large data sets written in C++. It builds on OpenMP for multicore execution, and on MPI for distributing the workload across the nodes in a cluster. It is also able to boost training by using CUDA if graphics processing units are available. A sparse kernel is included, which is useful for high-dimensional but sparse data, such as the vector spaces common in text mining workflows. Python, R and MATLAB interfaces facilitate interactive use. Apart from fast execution, memory use is highly optimized, enabling training large emergent maps even on a single computer.","Tue, 7 May 2013 06:43:26 UTC (15 KB)[v2] Wed, 28 Jan 2015 12:40:08 UTC (721 KB)[v3] Mon, 11 Jan 2016 10:48:52 UTC (1,712 KB)[v4] Fri, 9 Jun 2017 14:03:01 UTC (1,590 KB)"
"273","Data, text and web mining for business intelligence: a survey","Abdul-Aziz Rashid Al-Azmi","Information Retrieval (cs.IR)","The Information and Communication Technologies revolution brought a digital world with huge amounts of data available. Enterprises use mining technologies to search vast amounts of data for vital insight and knowledge. Mining tools such as data mining, text mining, and web mining are used to find hidden knowledge in large databases or the Internet.","Fri, 12 Apr 2013 08:04:31 UTC (321 KB)"
"274","An improved semantic similarity measure for document clustering based on topic maps","Muhammad Rafi, Mohammad Shahid Shaikh","Information Retrieval (cs.IR)","A major computational burden, while performing document clustering, is the calculation of similarity measure between a pair of documents. Similarity measure is a function that assigns a real number between 0 and 1 to a pair of documents, depending upon the degree of similarity between them. A value of zero means that the documents are completely dissimilar whereas a value of one indicates that the documents are practically identical. Traditionally, vector-based models have been used for computing the document similarity. The vector-based models represent several features present in documents. These approaches to similarity measures, in general, cannot account for the semantics of the document. Documents written in human languages contain contexts and the words used to describe these contexts are generally semantically related. Motivated by this fact, many researchers have proposed seman-tic-based similarity measures by utilizing text annotation through external thesauruses like WordNet (a lexical database). In this paper, we define a semantic similarity measure based on documents represented in topic maps. Topic maps are rapidly becoming an industrial standard for knowledge representation with a focus for later search and extraction. The documents are transformed into a topic map based coded knowledge and the similarity between a pair of documents is represented as a correlation between the common patterns (sub-trees). The experimental studies on the text mining datasets reveal that this new similarity measure is more effective as compared to commonly used similarity measures in text clustering.","Sun, 17 Mar 2013 18:28:02 UTC (250 KB)"
"275","A Hybrid Approach to Extract Keyphrases from Medical Documents","Kamal Sarkar","Information Retrieval (cs.IR)","Keyphrases are the phrases, consisting of one or more words, representing the important concepts in the articles. Keyphrases are useful for a variety of tasks such as text summarization, automatic indexing, clustering/classification, text mining etc. This paper presents a hybrid approach to keyphrase extraction from medical documents. The keyphrase extraction approach presented in this paper is an amalgamation of two methods: the first one assigns weights to candidate keyphrases based on an effective combination of features such as position, term frequency, inverse document frequency and the second one assign weights to candidate keyphrases using some knowledge about their similarities to the structure and characteristics of keyphrases available in the memory (stored list of keyphrases). An efficient candidate keyphrase identification method as the first component of the proposed keyphrase extraction system has also been introduced in this paper. The experimental results show that the proposed hybrid approach performs better than some state-of-the art keyphrase extraction approaches.","Wed, 6 Mar 2013 20:09:05 UTC (303 KB)[v2] Sat, 25 Jan 2014 16:34:35 UTC (243 KB)"
"276","Alternating proximal gradient method for sparse nonnegative Tucker decomposition","Yangyang Xu","Optimization and Control (math.OC)","Multi-way data arises in many applications such as electroencephalography (EEG) classification, face recognition, text mining and hyperspectral data analysis. Tensor decomposition has been commonly used to find the hidden factors and elicit the intrinsic structures of the multi-way data. This paper considers sparse nonnegative Tucker decomposition (NTD), which is to decompose a given tensor into the product of a core tensor and several factor matrices with sparsity and nonnegativity constraints. An alternating proximal gradient method (APG) is applied to solve the problem. The algorithm is then modified to sparse NTD with missing values. Per-iteration cost of the algorithm is estimated scalable about the data size, and global convergence is established under fairly loose conditions. Numerical experiments on both synthetic and real world data demonstrate its superiority over a few state-of-the-art methods for (sparse) NTD from partial and/or full observations. The MATLAB code along with demos are accessible from the author's homepage.","Mon, 11 Feb 2013 18:22:33 UTC (191 KB)[v2] Tue, 6 May 2014 16:30:25 UTC (176 KB)"
"277","An alternative text representation to TF-IDF and Bag-of-Words","Zhixiang (Eddie)Xu, Minmin Chen, Kilian Q. Weinberger, Fei Sha","Information Retrieval (cs.IR)","In text mining, information retrieval, and machine learning, text documents are commonly represented through variants of sparse Bag of Words (sBoW) vectors (e.g. TF-IDF). Although simple and intuitive, sBoW style representations suffer from their inherent over-sparsity and fail to capture word-level synonymy and polysemy. Especially when labeled data is limited (e.g. in document classification), or the text documents are short (e.g. emails or abstracts), many features are rarely observed within the training corpus. This leads to overfitting and reduced generalization accuracy. In this paper we propose Dense Cohort of Terms (dCoT), an unsupervised algorithm to learn improved sBoW document features. dCoT explicitly models absent words by removing and reconstructing random sub-sets of words in the unlabeled corpus. With this approach, dCoT learns to reconstruct frequent words from co-occurring infrequent words and maps the high dimensional sparse sBoW vectors into a low-dimensional dense representation. We show that the feature removal can be marginalized out and that the reconstruction can be solved for in closed-form. We demonstrate empirically, on several benchmark datasets, that dCoT features significantly improve the classification accuracy across several document classification tasks.","Mon, 28 Jan 2013 21:04:45 UTC (633 KB)"
"278","The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization","Hugo Van hamme","Numerical Analysis (math.NA)","Non-negative matrix factorization (NMF) has become a popular machine learning approach to many problems in text mining, speech and image processing, bio-informatics and seismic data analysis to name a few. In NMF, a matrix of non-negative data is approximated by the low-rank product of two matrices with non-negative entries. In this paper, the approximation quality is measured by the Kullback-Leibler divergence between the data and its low-rank reconstruction. The existence of the simple multiplicative update (MU) algorithm for computing the matrix factors has contributed to the success of NMF. Despite the availability of algorithms showing faster convergence, MU remains popular due to its simplicity. In this paper, a diagonalized Newton algorithm (DNA) is proposed showing faster convergence while the implementation remains simple and suitable for high-rank problems. The DNA algorithm is applied to various publicly available data sets, showing a substantial speed-up on modern hardware.","Tue, 15 Jan 2013 15:59:46 UTC (249 KB)[v2] Mon, 18 Mar 2013 09:15:29 UTC (251 KB)"
"279","Matrix Approximation under Local Low-Rank Assumption","Joonseok Lee, Seungyeon Kim, Guy Lebanon, Yoram Singer","Machine Learning (cs.LG)","Matrix approximation is a common tool in machine learning for building accurate prediction models for recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is only locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy in recommendation tasks.","Tue, 15 Jan 2013 00:54:38 UTC (313 KB)"
"280","A comparative study of root-based and stem-based approaches for measuring the similarity between arabic words for arabic text mining applications","Hanane Froud, Abdelmonaim Lachkar, Said Alaoui Ouatik","Computation and Language (cs.CL)","Representation of semantic information contained in the words is needed for any Arabic Text Mining applications. More precisely, the purpose is to better take into account the semantic dependencies between words expressed by the co-occurrence frequencies of these words. There have been many proposals to compute similarities between words based on their distributions in contexts. In this paper, we compare and contrast the effect of two preprocessing techniques applied to Arabic corpus: Rootbased (Stemming), and Stem-based (Light Stemming) approaches for measuring the similarity between Arabic words with the well known abstractive model -Latent Semantic Analysis (LSA)- with a wide variety of distance functions and similarity measures, such as the Euclidean Distance, Cosine Similarity, Jaccard Coefficient, and the Pearson Correlation Coefficient. The obtained results show that, on the one hand, the variety of the corpus produces more accurate results; on the other hand, the Stem-based approach outperformed the Root-based one because this latter affects the words meanings.","Fri, 14 Dec 2012 23:34:07 UTC (250 KB)"
"281","Finding influential users of an online health community: a new metric based on sentiment influence","Kang Zhao, Greta Greer, Baojun Qiu, Prasenjit Mitra, Kenneth Portier, John Yen","Social and Information Networks (cs.SI)","What characterizes influential users in online health communities (OHCs)? We hypothesize that (1) the emotional support received by OHC members can be assessed from their sentiment ex-pressed in online interactions, and (2) such assessments can help to identify influential OHC members. Through text mining and sentiment analysis of users' online interactions, we propose a novel metric that directly measures a user's ability to affect the sentiment of others. Using dataset from an OHC, we demonstrate that this metric is highly effective in identifying influential users. In addition, combining the metric with other traditional measures further improves the identification of influential users. This study can facilitate online community management and advance our understanding of social influence in OHCs.","Mon, 26 Nov 2012 20:37:00 UTC (562 KB)[v2] Wed, 5 Dec 2012 23:12:05 UTC (563 KB)"
"282","Inference of the Russian drug community from one of the largest social networks in the Russian Federation","L. J. Dijkstra, A. V. Yakushev, P. A. C. Duijn, A. V. Boukhanovsky, P. M. A. Sloot","Social and Information Networks (cs.SI)","The criminal nature of narcotics complicates the direct assessment of a drug community, while having a good understanding of the type of people drawn or currently using drugs is vital for finding effective intervening strategies. Especially for the Russian Federation this is of immediate concern given the dramatic increase it has seen in drug abuse since the fall of the Soviet Union in the early nineties. Using unique data from the Russian social network 'LiveJournal' with over 39 million registered users worldwide, we were able for the first time to identify the on-line drug community by context sensitive text mining of the users' blogs using a dictionary of known drug-related official and 'slang' terminology. By comparing the interests of the users that most actively spread information on narcotics over the network with the interests of the individuals outside the on-line drug community, we found that the 'average' drug user in the Russian Federation is generally mostly interested in topics such as Russian rock, non-traditional medicine, UFOs, Buddhism, yoga and the occult. We identify three distinct scale-free sub-networks of users which can be uniquely classified as being either 'infectious', 'susceptible' or 'immune'.","Tue, 20 Nov 2012 15:46:35 UTC (1,765 KB)[v2] Mon, 10 Dec 2012 09:39:30 UTC (1,766 KB)[v3] Mon, 13 May 2013 16:02:14 UTC (1,766 KB)"
"283","A New Similarity Measure for Taxonomy Based on Edge Counting","Manjula Shenoy.K, K.C.Shet, U.Dinesh Acharya","Artificial Intelligence (cs.AI)","This paper introduces a new similarity measure based on edge counting in a taxonomy like WorldNet or Ontology. Measurement of similarity between text segments or concepts is very useful for many applications like information retrieval, ontology matching, text mining, and question answering and so on. Several measures have been developed for measuring similarity between two concepts: out of these we see that the measure given by Wu and Palmer [1] is simple, and gives good performance. Our measure is based on their measure but strengthens it. Wu and Palmer [1] measure has a disadvantage that it does not consider how far the concepts are semantically. In our measure we include the shortest path between the concepts and the depth of whole taxonomy together with the distances used in Wu and Palmer [1]. Also the measure has following disadvantage i.e. in some situations, the similarity of two elements of an IS-A ontology contained in the neighborhood exceeds the similarity value of two elements contained in the same hierarchy. Our measure introduces a penalization factor for this case based upon shortest length between the concepts and depth of whole taxonomy.","Tue, 20 Nov 2012 10:53:22 UTC (154 KB)"
"284","Automating Legal Research through Data Mining","Mohamed Firdhous","Information Retrieval (cs.IR)","The term legal research generally refers to the process of identifying and retrieving appropriate information necessary to support legal decision making from past case records. At present, the process is mostly manual, but some traditional technologies such as keyword searching are commonly used to speed the process up. But a keyword search is not a comprehensive search to cater to the requirements of legal research as the search result includes too many false hits in terms of irrelevant case records. Hence the present generic tools cannot be used to automate legal research. This paper presents a framework which was developed by combining several Text Mining techniques to automate the process overcoming the difficulties in the existing methods. Further, the research also identifies the possible enhancements that could be done to enhance the effectiveness of the framework.","Thu, 8 Nov 2012 14:19:49 UTC (424 KB)"
"285","Towards Large-scale and Ultrahigh Dimensional Feature Selection via Feature Generation","Mingkui Tan, Ivor W. Tsang, Li Wang","Machine Learning (cs.LG)","In many real-world applications such as text mining, it is desirable to select the most relevant features or variables to improve the generalization ability, or to provide a better interpretation of the prediction models. {In this paper, a novel adaptive feature scaling (AFS) scheme is proposed by introducing a feature scaling {vector $\d \in [0, 1]^m$} to alleviate the bias problem brought by the scaling bias of the diverse features.} By reformulating the resultant AFS model to semi-infinite programming problem, a novel feature generating method is presented to identify the most relevant features for classification problems. In contrast to the traditional feature selection methods, the new formulation has the advantage of solving extremely high-dimensional and large-scale problems. With an exact solution to the worst-case analysis in the identification of relevant features, the proposed feature generating scheme converges globally. More importantly, the proposed scheme facilitates the group selection with or without special structures. Comprehensive experiments on a wide range of synthetic and real-world datasets demonstrate that the proposed method {achieves} better or competitive performance compared with the existing methods on (group) feature selection in terms of generalization performance and training efficiency. The C++ and MATLAB implementations of our algorithm can be available at \emph{this http URL}.","Mon, 24 Sep 2012 13:23:39 UTC (342 KB)"
"286","Content-based Text Categorization using Wikitology","Muhammad Rafi, Sundus Hassan, Mohammad Shahid Shaikh","Information Retrieval (cs.IR)","A major computational burden, while performing document clustering, is the calculation of similarity measure between a pair of documents. Similarity measure is a function that assign a real number between 0 and 1 to a pair of documents, depending upon the degree of similarity between them. A value of zero means that the documents are completely dissimilar whereas a value of one indicates that the documents are practically identical. Traditionally, vector-based models have been used for computing the document similarity. The vector-based models represent several features present in documents. These approaches to similarity measures, in general, cannot account for the semantics of the document. Documents written in human languages contain contexts and the words used to describe these contexts are generally semantically related. Motivated by this fact, many researchers have proposed semantic-based similarity measures by utilizing text annotation through external thesauruses like WordNet (a lexical database). In this paper, we define a semantic similarity measure based on documents represented in topic maps. Topic maps are rapidly becoming an industrial standard for knowledge representation with a focus for later search and extraction. The documents are transformed into a topic map based coded knowledge and the similarity between a pair of documents is represented as a correlation between the common patterns. The experimental studies on the text mining datasets reveal that this new similarity measure is more effective as compared to commonly used similarity measures in text clustering.","Fri, 17 Aug 2012 15:49:38 UTC (438 KB)"
"287","Tracing scientist's research trends realtimely","Xianwen Wang, Zhi Wang, Shenmeng Xu","Digital Libraries (cs.DL)","In this research, we propose a method to trace scientists' research trends realtimely. By monitoring the downloads of scientific articles in the journal of Scientometrics for 744 hours, namely one month, we investigate the download statistics. Then we aggregate the keywords in these downloaded research papers, and analyze the trends of article downloading and keyword downloading. Furthermore, taking both the download of keywords and articles into consideration, we design a method to detect the emerging research trends. We find that in scientometrics field, social media, new indices to quantify scientific productivity (g-index), webometrics, semantic, text mining, open access are emerging fields that scientometrics researchers are focusing on.","Tue, 7 Aug 2012 07:08:21 UTC (778 KB)[v2] Thu, 13 Dec 2012 05:36:34 UTC (916 KB)"
"288","INSTRUCT: Space-Efficient Structure for Indexing and Complete Query Management of String Databases","Sourav Dutta, Arnab Bhattacharya","Databases (cs.DB)","The tremendous expanse of search engines, dictionary and thesaurus storage, and other text mining applications, combined with the popularity of readily available scanning devices and optical character recognition tools, has necessitated efficient storage, retrieval and management of massive text databases for various modern applications. For such applications, we propose a novel data structure, INSTRUCT, for efficient storage and management of sequence databases. Our structure uses bit vectors for reusing the storage space for common triplets, and hence, has a very low memory requirement. INSTRUCT efficiently handles prefix and suffix search queries in addition to the exact string search operation by iteratively checking the presence of triplets. We also propose an extension of the structure to handle substring search efficiently, albeit with an increase in the space requirements. This extension is important in the context of trie-based solutions which are unable to handle such queries efficiently. We perform several experiments portraying that INSTRUCT outperforms the existing structures by nearly a factor of two in terms of space requirements, while the query times are better. The ability to handle insertion and deletion of strings in addition to supporting all kinds of queries including exact search, prefix/suffix search and substring search makes INSTRUCT a complete data structure.","Mon, 2 Jul 2012 12:38:47 UTC (67 KB)[v2] Tue, 3 Jul 2012 04:54:37 UTC (67 KB)"
"289","Memory-Efficient Topic Modeling","Jia Zeng, Zhi-Qiang Liu, Xiao-Qin Cao","Machine Learning (cs.LG)","As one of the simplest probabilistic topic modeling techniques, latent Dirichlet allocation (LDA) has found many important applications in text mining, computer vision and computational biology. Recent training algorithms for LDA can be interpreted within a unified message passing framework. However, message passing requires storing previous messages with a large amount of memory space, increasing linearly with the number of documents or the number of topics. Therefore, the high memory usage is often a major problem for topic modeling of massive corpora containing a large number of topics. To reduce the space complexity, we propose a novel algorithm without storing previous messages for training LDA: tiny belief propagation (TBP). The basic idea of TBP relates the message passing algorithms with the non-negative matrix factorization (NMF) algorithms, which absorb the message updating into the message passing process, and thus avoid storing previous messages. Experimental results on four large data sets confirm that TBP performs comparably well or even better than current state-of-the-art training algorithms for LDA but with a much less memory consumption. TBP can do topic modeling when massive corpora cannot fit in the computer memory, for example, extracting thematic topics from 7 GB PUBMED corpora on a common desktop computer with 2GB memory.","Wed, 6 Jun 2012 08:34:43 UTC (2,235 KB)[v2] Fri, 8 Jun 2012 14:07:26 UTC (2,239 KB)"
"290","Effective Listings of Function Stop words for Twitter","Murphy Choy","Information Retrieval (cs.IR)","Many words in documents recur very frequently but are essentially meaningless as they are used to join words together in a sentence. It is commonly understood that stop words do not contribute to the context or content of textual documents. Due to their high frequency of occurrence, their presence in text mining presents an obstacle to the understanding of the content in the documents. To eliminate the bias effects, most text mining software or approaches make use of stop words list to identify and remove those words. However, the development of such top words list is difficult and inconsistent between textual sources. This problem is further aggravated by sources such as Twitter which are highly repetitive or similar in nature. In this paper, we will be examining the original work using term frequency, inverse document frequency and term adjacency for developing a stop words list for the Twitter data source. We propose a new technique using combinatorial values as an alternative measure to effectively list out stop words.","Tue, 29 May 2012 15:37:46 UTC (234 KB)"
"291","Event based classification of Web 2.0 text streams","Andreas Bauer, Christian Wolff","Information Retrieval (cs.IR)","Web 2.0 applications like Twitter or Facebook create a continuous stream of information. This demands new ways of analysis in order to offer insight into this stream right at the moment of the creation of the information, because lots of this data is only relevant within a short period of time. To address this problem real time search engines have recently received increased attention. They take into account the continuous flow of information differently than traditional web search by incorporating temporal and social features, that describe the context of the information during its creation. Standard approaches where data first get stored and then is processed from a peristent storage suffer from latency. We want to address the fluent and rapid nature of text stream by providing an event based approach that analyses directly the stream of information. In a first step we want to define the difference between real time search and traditional search to clarify the demands in modern text filtering. In a second step we want to show how event based features can be used to support the tasks of real time search engines. Using the example of Twitter we present in this paper a way how to combine an event based approach with text mining and information filtering concepts in order to classify incoming information based on stream features. We calculate stream dependant features and feed them into a neural network in order to classify the text streams. We show the separative capabilities of event based features as the foundation for a real time search engine.","Mon, 16 Apr 2012 04:50:13 UTC (188 KB)[v2] Fri, 28 Jun 2013 12:31:37 UTC (187 KB)"
"292","Semi-Automatically Extracting FAQs to Improve Accessibility of Software Development Knowledge","Stefan Hen, Martin Monperrus (INRIA Lille - Nord Europe), Mira Mezini","Software Engineering (cs.SE)","Frequently asked questions (FAQs) are a popular way to document software development knowledge. As creating such documents is expensive, this paper presents an approach for automatically extracting FAQs from sources of software development discussion, such as mailing lists and Internet forums, by combining techniques of text mining and natural language processing. We apply the approach to popular mailing lists and carry out a survey among software developers to show that it is able to extract high-quality FAQs that may be further improved by experts.","Fri, 23 Mar 2012 07:13:06 UTC (184 KB)"
"293","Literature-based knowledge discovery: the state of the art","Xiaoyong Liu, Hui Fu","Digital Libraries (cs.DL)","Literature-based knowledge discovery method was introduced by Dr. Swanson in 1986. He hypothesized a connection between Raynaud's phenomenon and dietary fish oil, the field of literature-based discovery (LBD) was born from then on. During the subsequent two decades, LBD's research attracts some scientists including information science, computer science, and biomedical science, etc.. It has been a part of knowledge discovery and text mining. This paper summarizes the development of recent years about LBD and presents two parts, methodology research and applied research. Lastly, some problems are pointed as future research directions.","Fri, 16 Mar 2012 03:33:06 UTC (132 KB)"
"294","Concept Relation Discovery and Innovation Enabling Technology (CORDIET)","Jonas Poelmans, Paul Elzinga, Alexey Neznanov, Stijn Viaene, Sergei O. Kuznetsov, Dmitry Ignatov, Guido Dedene","Artificial Intelligence (cs.AI)","Concept Relation Discovery and Innovation Enabling Technology (CORDIET), is a toolbox for gaining new knowledge from unstructured text data. At the core of CORDIET is the C-K theory which captures the essential elements of innovation. The tool uses Formal Concept Analysis (FCA), Emergent Self Organizing Maps (ESOM) and Hidden Markov Models (HMM) as main artifacts in the analysis process. The user can define temporal, text mining and compound attributes. The text mining attributes are used to analyze the unstructured text in documents, the temporal attributes use these document's timestamps for analysis. The compound attributes are XML rules based on text mining and temporal attributes. The user can cluster objects with object-cluster rules and can chop the data in pieces with segmentation rules. The artifacts are optimized for efficient data analysis; object labels in the FCA lattice and ESOM map contain an URL on which the user can click to open the selected document.","Mon, 13 Feb 2012 23:19:51 UTC (415 KB)"
"295","Pbm: A new dataset for blog mining","Mehwish Aziz, Muhammad Rafi","Artificial Intelligence (cs.AI)","Text mining is becoming vital as Web 2.0 offers collaborative content creation and sharing. Now Researchers have growing interest in text mining methods for discovering knowledge. Text mining researchers come from variety of areas like: Natural Language Processing, Computational Linguistic, Machine Learning, and Statistics. A typical text mining application involves preprocessing of text, stemming and lemmatization, tagging and annotation, deriving knowledge patterns, evaluating and interpreting the results. There are numerous approaches for performing text mining tasks, like: clustering, categorization, sentimental analysis, and summarization. There is a growing need to standardize the evaluation of these tasks. One major component of establishing standardization is to provide standard datasets for these tasks. Although there are various standard datasets available for traditional text mining tasks, but there are very few and expensive datasets for blog-mining task. Blogs, a new genre in web 2.0 is a digital diary of web user, which has chronological entries and contains a lot of useful knowledge, thus offers a lot of challenges and opportunities for text mining. In this paper, we report a new indigenous dataset for Pakistani Political Blogosphere. The paper describes the process of data collection, organization, and standardization. We have used this dataset for carrying out various text mining tasks for blogosphere, like: blog-search, political sentiments analysis and tracking, identification of influential blogger, and clustering of the blog-posts. We wish to offer this dataset free for others who aspire to pursue further in this domain.","Tue, 10 Jan 2012 15:18:38 UTC (91 KB)"
"296","A Topic Modeling Toolbox Using Belief Propagation","Jia Zeng","Machine Learning (cs.LG)","Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper introduces a topic modeling toolbox (TMBP) based on the belief propagation (BP) algorithms. TMBP toolbox is implemented by MEX C++/Matlab/Octave for either Windows 7 or Linux. Compared with existing topic modeling packages, the novelty of this toolbox lies in the BP algorithms for learning LDA-based topic models. The current version includes BP algorithms for latent Dirichlet allocation (LDA), author-topic models (ATM), relational topic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project and more BP-based algorithms for various topic models will be added in the near future. Interested users may also extend BP algorithms for learning more complicated topic models. The source codes are freely available under the GNU General Public Licence, Version 1.0 at this https URL.","Wed, 4 Jan 2012 07:07:06 UTC (15 KB)[v2] Thu, 5 Apr 2012 06:48:35 UTC (9 KB)"
"297","Alternating proximal gradient method for nonnegative matrix factorization","Yangyang Xu","Information Theory (cs.IT)","Nonnegative matrix factorization has been widely applied in face recognition, text mining, as well as spectral analysis. This paper proposes an alternating proximal gradient method for solving this problem. With a uniformly positive lower bound assumption on the iterates, any limit point can be proved to satisfy the first-order optimality conditions. A Nesterov-type extrapolation technique is then applied to accelerate the algorithm. Though this technique is at first used for convex program, it turns out to work very well for the non-convex nonnegative matrix factorization problem. Extensive numerical experiments illustrate the efficiency of the alternating proximal gradient method and the accleration technique. Especially for real data tests, the accelerated method reveals high superiority to state-of-the-art algorithms in speed with comparable solution qualities.","Thu, 22 Dec 2011 18:22:59 UTC (362 KB)[v2] Sat, 29 Dec 2012 17:43:05 UTC (0 KB)[v3] Mon, 11 Feb 2013 17:54:57 UTC (0 KB)"
"298","Dynamical Classes of Collective Attention in Twitter","Janette Lehmann, Bruno Goncalves, Jose J. Ramasco, Ciro Cattuto","Social and Information Networks (cs.SI)","Micro-blogging systems such as Twitter expose digital traces of social discourse with an unprecedented degree of resolution of individual behaviors. They offer an opportunity to investigate how a large-scale social system responds to exogenous or endogenous stimuli, and to disentangle the temporal, spatial and topical aspects of users' activity. Here we focus on spikes of collective attention in Twitter, and specifically on peaks in the popularity of hashtags. Users employ hashtags as a form of social annotation, to define a shared context for a specific event, topic, or meme. We analyze a large-scale record of Twitter activity and find that the evolution of hastag popularity over time defines discrete classes of hashtags. We link these dynamical classes to the events the hashtags represent and use text mining techniques to provide a semantic characterization of the hastag classes. Moreover, we track the propagation of hashtags in the Twitter social network and find that epidemic spreading plays a minor role in hastag popularity, which is mostly driven by exogenous factors.","Tue, 8 Nov 2011 13:06:22 UTC (1,388 KB)[v2] Thu, 1 Mar 2012 17:45:52 UTC (1,379 KB)"
"299","Syndromic classification of Twitter messages","Nigel Collier, Son Doan","Computation and Language (cs.CL)","Recent studies have shown strong correlation between social networking data and national influenza rates. We expanded upon this success to develop an automated text mining system that classifies Twitter messages in real time into six syndromic categories based on key terms from a public health ontology. 10-fold cross validation tests were used to compare Naive Bayes (NB) and Support Vector Machine (SVM) models on a corpus of 7431 Twitter messages. SVM performed better than NB on 4 out of 6 syndromes. The best performing classifiers showed moderately strong F1 scores: respiratory = 86.2 (NB); gastrointestinal = 85.4 (SVM polynomial kernel degree 2); neurological = 88.6 (SVM polynomial kernel degree 1); rash = 86.0 (SVM polynomial kernel degree 1); constitutional = 89.3 (SVM polynomial kernel degree 1); hemorrhagic = 89.9 (NB). The resulting classifiers were deployed together with an EARS C2 aberration detection algorithm in an experimental online system.","Thu, 13 Oct 2011 23:42:32 UTC (108 KB)"
"300","What's unusual in online disease outbreak news?","Nigel Collier","Computation and Language (cs.CL)","Background: Accurate and timely detection of public health events of international concern is necessary to help support risk assessment and response and save lives. Novel event-based methods that use the World Wide Web as a signal source offer potential to extend health surveillance into areas where traditional indicator networks are lacking. In this paper we address the issue of systematically evaluating online health news to support automatic alerting using daily disease-country counts text mined from real world data using BioCaster. For 18 data sets produced by BioCaster, we compare 5 aberration detection algorithms (EARS C2, C3, W2, F-statistic and EWMA) for performance against expert moderated ProMED-mail postings. Results: We report sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), mean alerts/100 days and F1, at 95% confidence interval (CI) for 287 ProMED-mail postings on 18 outbreaks across 14 countries over a 366 day period. Results indicate that W2 had the best F1 with a slight benefit for day of week effect over C2. In drill down analysis we indicate issues arising from the granular choice of country-level modeling, sudden drops in reporting due to day of week effects and reporting bias. Automatic alerting has been implemented in BioCaster available from this http URL. Conclusions: Online health news alerts have the potential to enhance manual analytical methods by increasing throughput, timeliness and detection rates. Systematic evaluation of health news aberrations is necessary to push forward our understanding of the complex relationship between news report volumes and case numbers and to select the best performing features and algorithms.","Thu, 13 Oct 2011 23:22:43 UTC (1,759 KB)"
"301","Towards cross-lingual alerting for bursty epidemic events","Nigel Collier","Computation and Language (cs.CL)","Background: Online news reports are increasingly becoming a source for event based early warning systems that detect natural disasters. Harnessing the massive volume of information available from multilingual newswire presents as many challenges as opportunities due to the patterns of reporting complex spatiotemporal events. Results: In this article we study the problem of utilising correlated event reports across languages. We track the evolution of 16 disease outbreaks using 5 temporal aberration detection algorithms on text-mined events classified according to disease and outbreak country. Using ProMED reports as a silver standard, comparative analysis of news data for 13 languages over a 129 day trial period showed improved sensitivity, F1 and timeliness across most models using cross-lingual events. We report a detailed case study analysis for Cholera in Angola 2010 which highlights the challenges faced in correlating news events with the silver standard. Conclusions: The results show that automated health surveillance using multilingual text mining has the potential to turn low value news into high value alerts if informed choices are used to govern the selection of models and data sources. An implementation of the C2 alerting algorithm using multilingual news is available at the BioCaster portal this http URL.","Thu, 13 Oct 2011 23:05:48 UTC (944 KB)"
"302","VOGCLUSTERS: an example of DAME web application","Marco Castellani, Massimo Brescia, Ettore Mancini, Luca Pellecchia, Giuseppe Longo","Instrumentation and Methods for Astrophysics (astro-ph.IM)","We present the alpha release of the VOGCLUSTERS web application, specialized for data and text mining on globular clusters. It is one of the web2.0 technology based services of Data Mining & Exploration (DAME) Program, devoted to mine and explore heterogeneous information related to globular clusters data.","Mon, 19 Sep 2011 17:33:39 UTC (232 KB)[v2] Thu, 22 Sep 2011 10:57:57 UTC (232 KB)"
"303","Learning Topic Models by Belief Propagation","Jia Zeng, William K. Cheung, Jiming Liu","Machine Learning (cs.LG)","Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper represents LDA as a factor graph within the Markov random field (MRF) framework, which enables the classic loopy belief propagation (BP) algorithm for approximate inference and parameter estimation. Although two commonly-used approximate inference methods, such as variational Bayes (VB) and collapsed Gibbs sampling (GS), have gained great successes in learning LDA, the proposed BP is competitive in both speed and accuracy as validated by encouraging experimental results on four large-scale document data sets. Furthermore, the BP algorithm has the potential to become a generic learning scheme for variants of LDA-based topic models. To this end, we show how to learn two typical variants of LDA-based topic models, such as author-topic models (ATM) and relational topic models (RTM), using BP based on the factor graph representation.","Thu, 15 Sep 2011 19:20:48 UTC (5,383 KB)[v2] Sun, 25 Sep 2011 21:17:41 UTC (5,383 KB)[v3] Mon, 3 Oct 2011 03:17:44 UTC (5,978 KB)[v4] Sat, 24 Mar 2012 12:47:02 UTC (5,419 KB)"
"304","Text mining and visualization using VOSviewer","Nees Jan van Eck, Ludo Waltman","Digital Libraries (cs.DL)","VOSviewer is a computer program for creating, visualizing, and exploring bibliometric maps of science. In this report, the new text mining functionality of VOSviewer is presented. A number of examples are given of applications in which VOSviewer is used for analyzing large amounts of text data.","Fri, 9 Sep 2011 16:16:22 UTC (920 KB)"
"305","Decision Support for e-Governance: A Text Mining Approach","G.Koteswara Rao, Shubhamoy Dey","Databases (cs.DB)","Information and communication technology has the capability to improve the process by which governments involve citizens in formulating public policy and public projects. Even though much of government regulations may now be in digital form (and often available online), due to their complexity and diversity, identifying the ones relevant to a particular context is a non-trivial task. Similarly, with the advent of a number of electronic online forums, social networking sites and blogs, the opportunity of gathering citizens' petitions and stakeholders' views on government policy and proposals has increased greatly, but the volume and the complexity of analyzing unstructured data makes this difficult. On the other hand, text mining has come a long way from simple keyword search, and matured into a discipline capable of dealing with much more complex tasks. In this paper we discuss how text-mining techniques can help in retrieval of information and relationships from textual data sources, thereby assisting policy makers in discovering associations between policies and citizens' opinions expressed in electronic public forums and blogs etc. We also present here, an integrated text mining based architecture for e-governance decision support along with a discussion on the Indian scenario.","Wed, 31 Aug 2011 11:48:57 UTC (529 KB)"
"306","Typesafe Modeling in Text Mining","Fabian Steeg","Programming Languages (cs.PL)","Based on the concept of annotation-based agents, this report introduces tools and a formal notation for defining and running text mining experiments using a statically typed domain-specific language embedded in Scala. Using machine learning for classification as an example, the framework is used to develop and document text mining experiments, and to show how the concept of generic, typesafe annotation corresponds to a general information model that goes beyond text processing.","Thu, 28 Jul 2011 17:46:20 UTC (419 KB)"
"307","Accelerated Multiplicative Updates and Hierarchical ALS Algorithms for Nonnegative Matrix Factorization","Nicolas Gillis, Francois Glineur","Optimization and Control (math.OC)","Nonnegative matrix factorization (NMF) is a data analysis technique used in a great variety of applications such as text mining, image processing, hyperspectral data analysis, computational biology, and clustering. In this paper, we consider two well-known algorithms designed to solve NMF problems, namely the multiplicative updates of Lee and Seung and the hierarchical alternating least squares of Cichocki et al. We propose a simple way to significantly accelerate these schemes, based on a careful analysis of the computational cost needed at each iteration, while preserving their convergence properties. This acceleration technique can also be applied to other algorithms, which we illustrate on the projected gradient method of Lin. The efficiency of the accelerated algorithms is empirically demonstrated on image and text datasets, and compares favorably with a state-of-the-art alternating nonnegative least squares algorithm.","Tue, 26 Jul 2011 12:26:07 UTC (306 KB)[v2] Thu, 6 Oct 2011 13:16:20 UTC (787 KB)"
"308","Fence - An Efficient Parser with Ambiguity Support for Model-Driven Language Specification","Luis Quesada, Fernando Berzal, Francisco J. Cortijo","Computation and Language (cs.CL)","Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguities. In this paper, we propose Fence, an efficient bottom-up parsing algorithm with lexical and syntactic ambiguity support that enables the use of model-based language specification in practice.","Sat, 23 Jul 2011 12:56:02 UTC (308 KB)[v2] Fri, 7 Oct 2011 09:50:12 UTC (298 KB)"
"309","Tuffy: Scaling up Statistical Inference in Markov Logic Networks using an RDBMS","Feng Niu (University of Wisconsin-Madison), Christopher Re (University of Wisconsin-Madison), AnHai Doan (University of Wisconsin-Madison), Jude Shavlik (University of Wisconsin-Madison)","Databases (cs.DB)","Markov Logic Networks (MLNs) have emerged as a powerful framework that combines statistical and logical reasoning; they have been applied to many data intensive problems including information extraction, entity resolution, and text mining. Current implementations of MLNs do not scale to large real-world data sets, which is preventing their wide-spread adoption. We present Tuffy that achieves scalability via three novel contributions: (1) a bottom-up approach to grounding that allows us to leverage the full power of the relational optimizer, (2) a novel hybrid architecture that allows us to perform AI-style local search efficiently using an RDBMS, and (3) a theoretical insight that shows when one can (exponentially) improve the efficiency of stochastic local search. We leverage (3) to build novel partitioning, loading, and parallel algorithms. We show that our approach outperforms state-of-the-art implementations in both quality and speed on several publicly available datasets.","Sat, 16 Apr 2011 08:52:25 UTC (1,437 KB)"
"310","A Concept Annotation System for Clinical Records","Ning Kang, Rogier Barendse, Zubair Afzal, Bharat Singh, Martijn J. Schuemie, Erik M. van Mulligen, Jan A. Kors","Information Retrieval (cs.IR)","Unstructured information comprises a valuable source of data in clinical records. For text mining in clinical records, concept extraction is the first step in finding assertions and relationships. This study presents a system developed for the annotation of medical concepts, including medical problems, tests, and treatments, mentioned in clinical records. The system combines six publicly available named entity recognition system into one framework, and uses a simple voting scheme that allows to tune precision and recall of the system to specific needs. The system provides both a web service interface and a UIMA interface which can be easily used by other systems. The system was tested in the fourth i2b2 challenge and achieved an F-score of 82.1% for the concept exact match task, a score which is among the top-ranking systems. To our knowledge, this is the first publicly available clinical record concept annotation system.","Wed, 8 Dec 2010 00:56:45 UTC (297 KB)"
"311","The CALBC RDF Triple Store: retrieval over large literature content","Samuel Croset, Christoph Grabmuller, Chen Li, Silvestras Kavaliauskas, Dietrich Rebholz-Schuhmann","Digital Libraries (cs.DL)","Integration of the scientific literature into a biomedical research infrastructure requires the processing of the literature, identification of the contained named entities (NEs) and concepts, and to represent the content in a standardised way. The CALBC project partners (PPs) have produced a large-scale annotated biomedical corpus with four different semantic groups through the harmonisation of annotations from automatic text mining solutions (Silver Standard Corpus, SSC). The four semantic groups were chemical entities and drugs (CHED), genes and proteins (PRGE), diseases and disorders (DISO) and species (SPE). The content of the SSC has been fully integrated into RDF Triple Store (4,568,678 triples) and has been aligned with content from the GeneAtlas (182,840 triples), UniProtKb (12,552,239 triples for human) and the lexical resource LexEBI (BioLexicon). RDF Triple Store enables querying the scientific literature and bioinformatics resources at the same time for evidence of genetic causes, such as drug targets and disease involvement.","Wed, 8 Dec 2010 00:19:06 UTC (585 KB)"
"312","A study on the relation between linguistics-oriented and domain-specific semantics","He Tan","Artificial Intelligence (cs.AI)","In this paper we dealt with the comparison and linking between lexical resources with domain knowledge provided by ontologies. It is one of the issues for the combination of the Semantic Web Ontologies and Text Mining. We investigated the relations between the linguistics oriented and domain-specific semantics, by associating the GO biological process concepts to the FrameNet semantic frames. The result shows the gaps between the linguistics-oriented and domain-specific semantics on the classification of events and the grouping of target words. The result provides valuable information for the improvement of domain ontologies supporting for text mining systems. And also, it will result in benefits to language understanding technology.","Tue, 7 Dec 2010 23:03:42 UTC (30 KB)"
"313","A Multilevel Approach For Nonnegative Matrix Factorization","Nicolas Gillis, Francois Glineur","Optimization and Control (math.OC)","Nonnegative Matrix Factorization (NMF) is the problem of approximating a nonnegative matrix with the product of two low-rank nonnegative matrices and has been shown to be particularly useful in many applications, e.g., in text mining, image processing, computational biology, etc. In this paper, we explain how algorithms for NMF can be embedded into the framework of multilevel methods in order to accelerate their convergence. This technique can be applied in situations where data admit a good approximate representation in a lower dimensional space through linear transformations preserving nonnegativity. A simple multilevel strategy is described and is experimentally shown to speed up significantly three popular NMF algorithms (alternating nonnegative least squares, multiplicative updates and hierarchical alternating least squares) on several standard image datasets.","Sat, 4 Sep 2010 22:55:34 UTC (575 KB)[v2] Sun, 12 Sep 2010 16:47:01 UTC (554 KB)[v3] Tue, 4 Oct 2011 00:02:21 UTC (569 KB)"
"314","Clustering Unstructured Data (Flat Files) - An Implementation in Text Mining Tool","Yasir Safeer (1), Atika Mustafa (1), Anis Noor Ali (1) ((1) FAST - National University of Computer and Emerging Sciences)","Information Retrieval (cs.IR)","With the advancement of technology and reduced storage costs, individuals and organizations are tending towards the usage of electronic media for storing textual information and documents. It is time consuming for readers to retrieve relevant information from unstructured document collection. It is easier and less time consuming to find documents from a large collection when the collection is ordered or classified by group or category. The problem of finding best such grouping is still there. This paper discusses the implementation of k-Means clustering algorithm for clustering unstructured text documents that we implemented, beginning with the representation of unstructured text and reaching the resulting set of clusters. Based on the analysis of resulting clusters for a sample set of documents, we have also proposed a technique to represent documents that can further improve the clustering result.","Sun, 25 Jul 2010 14:38:32 UTC (1,045 KB)"
"315","A New Approach to Keyphrase Extraction Using Neural Networks","Kamal Sarkar, Mita Nasipuri, Suranjan Ghose","Information Retrieval (cs.IR)","Keyphrases provide a simple way of describing a document, giving the reader some clues about its contents. Keyphrases can be useful in a various applications such as retrieval engines, browsing interfaces, thesaurus construction, text mining etc.. There are also other tasks for which keyphrases are useful, as we discuss in this paper. This paper describes a neural network based approach to keyphrase extraction from scientific articles. Our results show that the proposed method performs better than some state-of-the art keyphrase extraction approaches.","Mon, 19 Apr 2010 18:24:02 UTC (110 KB)"
"316","On Generation of Firewall Log Status Reporter (SRr) Using Perl","Sugam Sharma (1), Hari Cohly (2), Tzusheng Pei (2), ((1)Iowa State University, USA, (2) Jackson State University, USA)","Software Engineering (cs.SE)","Computer System Administration and Network Administration are few such areas where Practical Extraction Reporting Language (Perl) has robust utilization these days apart from Bioinformatics. The key role of a System/Network Administrator is to monitor log files. Log file are updated every day. To scan the summary of large log files and to quickly determine if there is anything wrong with the server or network we develop a Firewall Log Status Reporter (SRr). SRr helps to generate the reports based on the parameters of interest. SRr provides the facility to admin to generate the individual firewall report or all reports in one go. By scrutinizing the results of the reports admin can trace how many times a particular request has been made from which source to which destination and can track the errors easily. Perl scripts can be seen as the UNIX script replacement in future arena and SRr is one development with the same hope that we can believe in. SRr is a generalized and customizable utility completely written in Perl and may be used for text mining and data mining application in Bioinformatics research and development too.","Mon, 5 Apr 2010 09:52:05 UTC (292 KB)"
"317","Towards Effective Sentence Simplification for Automatic Processing of Biomedical Text","Siddhartha Jonnalagadda, Luis Tari, Jorg Hakenberg, Chitta Baral, Graciela Gonzalez","Computation and Language (cs.CL)","The complexity of sentences characteristic to biomedical articles poses a challenge to natural language parsers, which are typically trained on large-scale corpora of non-technical text. We propose a text simplification process, bioSimplify, that seeks to reduce the complexity of sentences in biomedical abstracts in order to improve the performance of syntactic parsers on the processed sentences. Syntactic parsing is typically one of the first steps in a text mining pipeline. Thus, any improvement in performance would have a ripple effect over all processing steps. We evaluated our method using a corpus of biomedical sentences annotated with syntactic links. Our empirical results show an improvement of 2.90% for the Charniak-McClosky parser and of 4.23% for the Link Grammar parser when processing simplified sentences rather than the original sentences in the corpus.","Sun, 24 Jan 2010 20:35:29 UTC (58 KB)"
"318","On Utilization and Importance of Perl Status Reporter (SRr) in Text Mining","Sugam Sharma, Tzusheng Pei, Hari Cohly","Information Retrieval (cs.IR)","In Bioinformatics, text mining and text data mining sometimes interchangeably used is a process to derive high-quality information from text. Perl Status Reporter (SRr) is a data fetching tool from a flat text file and in this research paper we illustrate the use of SRr in text or data mining. SRr needs a flat text input file where the mining process to be performed. SRr reads input file and derives the high quality information from it. Typically text mining tasks are text categorization, text clustering, concept and entity extraction, and document summarization. SRr can be utilized for any of these tasks with little or none customizing efforts. In our implementation we perform text categorization mining operation on input file. The input file has two parameters of interest (firstKey and secondKey). The composition of these two parameters describes the uniqueness of entries in that file in the similar manner as done by composite key in database. SRr reads the input file line by line and extracts the parameters of interest and form a composite key by joining them together. It subsequently generates an output file consisting of the name as firstKey secondKey. SRr reads the input file and tracks the composite key. It further stores all that data lines, having the same composite key, in output file generated by SRr based on that composite key.","Tue, 19 Jan 2010 12:16:55 UTC (756 KB)[v2] Thu, 21 Jan 2010 21:21:22 UTC (749 KB)"
"319","Metric and Kernel Learning using a Linear Transformation","Prateek Jain, Brian Kulis, Jason V. Davis, Inderjit S. Dhillon","Machine Learning (cs.LG)","Metric and kernel learning are important in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study metric learning as a problem of learning a linear transformation of the input data. We show that for high-dimensional data, a particular framework for learning a linear transformation of the data based on the LogDet divergence can be efficiently kernelized to learn a metric (or equivalently, a kernel function) over an arbitrarily high dimensional space. We further demonstrate that a wide class of convex loss functions for learning linear transformations can similarly be kernelized, thereby considerably expanding the potential applications of metric learning. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision and text mining.","Fri, 30 Oct 2009 18:19:03 UTC (75 KB)"
"320","Using Underapproximations for Sparse Nonnegative Matrix Factorization","Nicolas Gillis, Francois Glineur","Optimization and Control (math.OC)","Nonnegative Matrix Factorization consists in (approximately) factorizing a nonnegative data matrix by the product of two low-rank nonnegative matrices. It has been successfully applied as a data analysis technique in numerous domains, e.g., text mining, image processing, microarray data analysis, collaborative filtering, etc. We introduce a novel approach to solve NMF problems, based on the use of an underapproximation technique, and show its effectiveness to obtain sparse solutions. This approach, based on Lagrangian relaxation, allows the resolution of NMF problems in a recursive fashion. We also prove that the underapproximation problem is NP-hard for any fixed factorization rank, using a reduction of the maximum edge biclique problem in bipartite graphs. We test two variants of our underapproximation approach on several standard image datasets and show that they provide sparse part-based representations with low reconstruction error. Our results are comparable and sometimes superior to those obtained by two standard Sparse Nonnegative Matrix Factorization techniques.","Fri, 6 Feb 2009 11:05:23 UTC (457 KB)[v2] Sat, 24 Oct 2009 17:02:58 UTC (511 KB)"
"321","Adaptive Spam Detection Inspired by a Cross-Regulation Model of Immune Dynamics: A Study of Concept Drift","Alaa Abi-Haidar, Luis M. Rocha","Artificial Intelligence (cs.AI)","This paper proposes a novel solution to spam detection inspired by a model of the adaptive immune system known as the crossregulation model. We report on the testing of a preliminary algorithm on six e-mail corpora. We also compare our results statically and dynamically with those obtained by the Naive Bayes classifier and another binary classification method we developed previously for biomedical text-mining applications. We show that the cross-regulation model is competitive against those and thus promising as a bio-inspired algorithm for spam detection in particular, and binary classification in general.","Thu, 4 Dec 2008 20:40:32 UTC (298 KB)"
"322","Reconstruction of Protein-Protein Interaction Pathways by Mining Subject-Verb-Objects Intermediates","Maurice HT Ling, Christophe Lefevre, Kevin R. Nicholas, Feng Lin","Information Retrieval (cs.IR)","The exponential increase in publication rate of new articles is limiting access of researchers to relevant literature. This has prompted the use of text mining tools to extract key biological information. Previous studies have reported extensive modification of existing generic text processors to process biological text. However, this requirement for modification had not been examined. In this study, we have constructed Muscorian, using MontyLingua, a generic text processor. It uses a two-layered generalization-specialization paradigm previously proposed where text was generically processed to a suitable intermediate format before domain-specific data extraction techniques are applied at the specialization layer. Evaluation using a corpus and experts indicated 86-90% precision and approximately 30% recall in extracting protein-protein interactions, which was comparable to previous studies using either specialized biological text processing tools or modified existing tools. Our study had also demonstrated the flexibility of the two-layered generalization-specialization paradigm by using the same generalization layer for two specialized information extraction tasks.","Mon, 6 Aug 2007 01:22:46 UTC (295 KB)"
"323","A Methodological Framework for Socio-Cognitive Analyses of Collaborative Design of Open Source Software","Warren Sack, Francoise Detienne (INRIA Rocquencourt), Nicholas Ducheneaut, Jean-Marie Burkhardt (LEI), Dilan Mahendran, Flore Barcellini (INRIA Rocquencourt, EIFFEL)","Human-Computer Interaction (cs.HC)","Open Source Software (OSS) development challenges traditional software engineering practices. In particular, OSS projects are managed by a large number of volunteers, working freely on the tasks they choose to undertake. OSS projects also rarely rely on explicit system-level design, or on project plans or schedules. Moreover, OSS developers work in arbitrary locations and collaborate almost exclusively over the Internet, using simple tools such as email and software code tracking databases (e.g. CVS). All the characteristics above make OSS development akin to weaving a tapestry of heterogeneous components. The OSS design process relies on various types of actors: people with prescribed roles, but also elements coming from a variety of information spaces (such as email and software code). The objective of our research is to understand the specific hybrid weaving accomplished by the actors of this distributed, collective design process. This, in turn, challenges traditional methodologies used to understand distributed software engineering: OSS development is simply too ""fibrous"" to lend itself well to analysis under a single methodological lens. In this paper, we describe the methodological framework we articulated to analyze collaborative design in the Open Source world. Our framework focuses on the links between the heterogeneous components of a project's hybrid network. We combine ethnography, text mining, and socio-technical network analysis and visualization to understand OSS development in its totality. This way, we are able to simultaneously consider the social, technical, and cognitive aspects of OSS development. We describe our methodology in detail, and discuss its implications for future research on distributed collective practices.","Fri, 2 Mar 2007 13:51:29 UTC (410 KB)"
"324","Acronym-Meaning Extraction from Corpora Using Multi-Tape Weighted Finite-State Machines","Andre Kempe","Computation and Language (cs.CL)","The automatic extraction of acronyms and their meaning from corpora is an important sub-task of text mining. It can be seen as a special case of string alignment, where a text chunk is aligned with an acronym. Alternative alignments have different cost, and ideally the least costly one should give the correct meaning of the acronym. We show how this approach can be implemented by means of a 3-tape weighted finite-state machine (3-WFSM) which reads a text chunk on tape 1 and an acronym on tape 2, and generates all alternative alignments on tape 3. The 3-WFSM can be automatically generated from a simple regular expression. No additional algorithms are required at any stage. Our 3-WFSM has a size of 27 states and 64 transitions, and finds the best analysis of an acronym in a few milliseconds.","Wed, 6 Dec 2006 10:13:12 UTC (8 KB)"
"325","Exploring term-document matrices from matrix models in text mining","Ioannis Antonellis, Efstratios Gallopoulos","Information Retrieval (cs.IR)","We explore a matrix-space model, that is a natural extension to the vector space model for Information Retrieval. Each document can be represented by a matrix that is based on document extracts (e.g. sentences, paragraphs, sections). We focus on the performance of this model for the specific case in which documents are originally represented as term-by-sentence matrices. We use the singular value decomposition to approximate the term-by-sentence matrices and assemble these results to form the pseudo-``term-document'' matrix that forms the basis of a text mining method alternative to traditional VSM and LSI. We investigate the singular values of this matrix and provide experimental evidence suggesting that the method can be particularly effective in terms of accuracy for text collections with multi-topic documents, such as web pages with news.","Tue, 21 Feb 2006 16:14:16 UTC (193 KB)"
"326","Preference Learning in Terminology Extraction: A ROC-based approach","Jerome Aze (LRI), Mathieu Roche (LRI), Yves Kodratoff (LRI), Michele Sebag (LRI)","Machine Learning (cs.LG)","A key data preparation step in Text Mining, Term Extraction selects the terms, or collocation of words, attached to specific concepts. In this paper, the task of extracting relevant collocations is achieved through a supervised learning algorithm, exploiting a few collocations manually labelled as relevant/irrelevant. The candidate terms are described along 13 standard statistical criteria measures. From these examples, an evolutionary learning algorithm termed Roger, based on the optimization of the Area under the ROC curve criterion, extracts an order on the candidate terms. The robustness of the approach is demonstrated on two real-world domain applications, considering different domains (biology and human resources) and different languages (English and French).","Tue, 13 Dec 2005 13:25:57 UTC (34 KB)"
"327","Transitive Text Mining for Information Extraction and Hypothesis Generation","Johannes Stegmann, Guenter Grohmann (Charite, Berlin)","Information Retrieval (cs.IR)","Transitive text mining - also named Swanson Linking (SL) after its primary and principal researcher - tries to establish meaningful links between literature sets which are virtually disjoint in the sense that each does not mention the main concept of the other. If successful, SL may give rise to the development of new hypotheses. In this communication we describe our approach to transitive text mining which employs co-occurrence analysis of the medical subject headings (MeSH), the descriptors assigned to papers indexed in PubMed. In addition, we will outline the current state of our web-based information system which will enable our users to perform literature-driven hypothesis building on their own.","Wed, 7 Sep 2005 12:16:22 UTC (15 KB)"
"328","Learning from Web: Review of Approaches","Vitaly Schetinin","Neural and Evolutionary Computing (cs.NE)","Knowledge discovery is defined as non-trivial extraction of implicit, previously unknown and potentially useful information from given data. Knowledge extraction from web documents deals with unstructured, free-format documents whose number is enormous and rapidly growing. The artificial neural networks are well suitable to solve a problem of knowledge discovery from web documents because trained networks are able more accurately and easily to classify the learning and testing examples those represent the text mining domain. However, the neural networks that consist of large number of weighted connections and activation units often generate the incomprehensible and hard-to-understand models of text classification. This problem may be also addressed to most powerful recurrent neural networks that employ the feedback links from hidden or output units to their input units. Due to feedback links, recurrent neural networks are able take into account of a context in document. To be useful for data mining, self-organizing neural network techniques of knowledge extraction have been explored and developed. Self-organization principles were used to create an adequate neural-network structure and reduce a dimensionality of features used to describe text documents. The use of these principles seems interesting because ones are able to reduce a neural-network redundancy and considerably facilitate the knowledge representation.","Wed, 13 Apr 2005 13:40:38 UTC (64 KB)"
"329","State of the Art, Evaluation and Recommendations regarding ""Document Processing and Visualization Techniques""","Martin Rajman, Martin Vesely, Pierre Andrews","Computation and Language (cs.CL)","Several Networks of Excellence have been set up in the framework of the European FP5 research program. Among these Networks of Excellence, the NEMIS project focuses on the field of Text Mining. Within this field, document processing and visualization was identified as one of the key topics and the WG1 working group was created in the NEMIS project, to carry out a detailed survey of techniques associated with the text mining process and to identify the relevant research topics in related research areas. In this document we present the results of this comprehensive survey. The report includes a description of the current state-of-the-art and practice, a roadmap for follow-up research in the identified areas, and recommendations for anticipated technological development in the domain of text mining.","Wed, 29 Dec 2004 15:19:03 UTC (666 KB)"
"330","Evaluation of text data mining for database curation: lessons learned from the KDD Challenge Cup","Alexander S. Yeh, Lynette Hirschman, Alexander A. Morgan","Computation and Language (cs.CL)","MOTIVATION: The biological literature is a major repository of knowledge. Many biological databases draw much of their content from a careful curation of this literature. However, as the volume of literature increases, the burden of curation increases. Text mining may provide useful tools to assist in the curation process. To date, the lack of standards has made it impossible to determine whether text mining techniques are sufficiently mature to be useful. RESULTS: We report on a Challenge Evaluation task that we created for the Knowledge Discovery and Data Mining (KDD) Challenge Cup. We provided a training corpus of 862 articles consisting of journal articles curated in FlyBase, along with the associated lists of genes and gene products, as well as the relevant data fields from FlyBase. For the test, we provided a corpus of 213 new (`blind') articles; the 18 participating groups provided systems that flagged articles for curation, based on whether the article contained experimental evidence for gene expression products. We report on the the evaluation results and describe the techniques used by the top performing groups. CONTACT: asy@mitre.org KEYWORDS: text mining, evaluation, curation, genomics, data management","Wed, 20 Aug 2003 18:40:39 UTC (57 KB)"
"331","Phenomenological approach to profile impact of scientific research: Citation Mining","J.A del Rio, R.N. Kostoff, E.O. Garcia, A.M. Ramirez, J. A. Humenik","Physics and Society (physics.soc-ph)","In this paper we present a phenomenological approach to describe a complex system: scientific research impact through Citation Mining. The novel concept of Citation Mining, a combination of citation bibliometrics and text mining, is used for the phenomenological description. Citation Mining starts with a group of core papers whose impact is to be examined, retrieves the papers that cite these core papers, and then analyzes the technical infrastructure (authors, jorunals, institutions) of the citing papers as well as their thematic characteristics.","Mon, 17 Dec 2001 05:46:54 UTC (264 KB)"
"332","Using compression to identify acronyms in text","Stuart Yeates, David Bainbridge, Ian H. Witten","Digital Libraries (cs.DL)","Text mining is about looking for patterns in natural language text, and may be defined as the process of analyzing text to extract information from it for particular purposes. In previous work, we claimed that compression is a key technology for text mining, and backed this up with a study that showed how particular kinds of lexical tokens---names, dates, locations, etc.---can be identified and located in running text, using compression models to provide the leverage necessary to distinguish different token types (Witten et al., 1999)","Tue, 4 Jul 2000 02:02:01 UTC (34 KB)"
